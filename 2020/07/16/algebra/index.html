<!DOCTYPE html><html xmlns:v-bind="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="Hexo 3.9.0"><title>Essence of Linear Algebra - Avalon</title><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="John Reese"><meta name="description" content="What are exactly vectors and matrices?"><meta name="keywords" content><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous"><link rel="stylesheet" href="/css/journal.css?74236570"><script src="/js/loadCSS.js"></script><script>loadCSS("https://fonts.googleapis.com/css?family=Lora|Montserrat|Fira+Mono|Material+Icons"),function(e){var t,a={kitId:"dwg1tuc",scriptTimeout:3e3,async:!0},o=e.documentElement,s=setTimeout(function(){o.className=o.className.replace(/\bwf-loading\b/g,"")+" wf-inactive"},a.scriptTimeout),c=e.createElement("script"),i=!1,n=e.getElementsByTagName("script")[0];o.className+=" wf-loading",c.src="https://use.typekit.net/"+a.kitId+".js",c.async=!0,c.onload=c.onreadystatechange=function(){if(t=this.readyState,!(i||t&&"complete"!=t&&"loaded"!=t)){i=!0,clearTimeout(s);try{Typekit.load(a)}catch(e){}}},n.parentNode.insertBefore(c,n)}(document)</script><noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora|Montserrat|Anonymous+Pro:400|Material+Icons"></noscript><link rel="stylesheet" href="/css/prism.css" type="text/css"></head><body><div id="top"></div><div id="app"><div class="single-column-drawer-container" ref="drawer" v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }"><div class="drawer-content"><div class="drawer-menu"><a class="a-block drawer-menu-item false" href="http://yoursite.com">Home </a><a class="a-block drawer-menu-item false" href="/archives">記事一覽 </a><a class="a-block drawer-menu-item false" href="/about/index.html">關於我 </a><a class="a-block drawer-menu-item false" href="/categories/index.html">分類 </a><a class="a-block drawer-menu-item false" href="/tags/index.html">標簽</a></div></div></div><transition name="fade"><div v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if="isDrawerOpen" v-on:click="toggleDrawer"></div></transition><nav ref="navBar" class="navbar sticky-top navbar-light single-column-nav-container"><div ref="navBackground" class="nav-background"></div><div class="container container-narrow nav-content"><button id="nav_dropdown_btn" class="nav-dropdown-toggle" type="button" v-on:click="toggleDrawer"><i class="material-icons">menu</i></button> <a ref="navTitle" class="navbar-brand" href="/">Avalon</a></div></nav><div class="single-column-header-container" ref="pageHead" v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }"><a href="/"><div class="single-column-header-title">Avalon</div><div class="single-column-header-subtitle">Welcome!!</div></a></div><div ref="sideContainer" class="side-container"><a class="a-block nav-head false" href="/"><div class="nav-title">远い理想郷</div><div class="nav-subtitle">お帰りなさい</div></a><div class="nav-link-list"><a class="a-block no-tint nav-link-item false" href="/archives">記事一覽 </a><a class="a-block nav-link-item false" href="/about/index.html">關於我 </a><a class="a-block nav-link-item false" href="/categories/index.html">分類 </a><a class="a-block nav-link-item false" href="/tags/index.html">標簽</a></div><div class="nav-footer">Proudly published with Hexo<br>Theme <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> by <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a><br>&copy; 2020 <a href="http://yoursite.com">Avalon</a></div></div><div ref="extraContainer" class="extra-container"><div class="pagination"><a id="globalBackToTop" class="pagination-action animated-visibility" href="#top" :class="{ invisible: scrollY == 0 }"><i class="material-icons pagination-action-icon">keyboard_arrow_up</i></a></div></div><div ref="streamContainer" class="stream-container"><div class="post-list-container post-list-container-shadow"><div class="post"><div class="post-head-wrapper" style="background-image:url(/2020/07/16/algebra/Matrix.png)"><div class="post-title">Essence of Linear Algebra<div class="post-meta"><time datetime="2020-07-16T13:54:17.000Z" itemprop="datePublished">2020-07-16 09:54 </time>&nbsp; <i class="material-icons">folder</i> <a href="/categories/Math/">Math</a>, <a href="/categories/Math/Engineering/">Engineering</a> <i class="material-icons">label</i> <a href="/tags/Math/">Math</a>, <a href="/tags/Linear-Algebra/">Linear Algebra</a><script type="text/x-mathjax-config">MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></div></div></div><div class="post-body-wrapper"><div class="post-body"><h3 id="announcements">Announcements</h3><p><strong>Contents below are basically notes of <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" target="_blank" rel="noopener">Essence of linear algebra</a> from <a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" target="_blank" rel="noopener">3Blue1Brown</a>.</strong></p><h3 id="what-is-exactly-a-vector">What is exactly a vector?</h3><p>There are mainly 3 perspectives as below</p><ul><li>Physics student - Vectors are arrows with specific directions and lengths.</li><li>CS student - Vectors are ordered lists of numbers.</li><li>Mathematician - Vectors are arrows and at the same time ordered lists of numbers.</li></ul><p>If there is a coordinate system say <span class="math inline">\(x-y\)</span> plane, we can easily imagine an arrow with its tail sitting at the origin. No matter which direction it points to and how long it is, it is a 2-dimensional vector which can be easily visualized. In the meantime, the coordinates of this vector is a pair of numbers that tell you how to get from the tail of the vector, at the origin, to its tip. For example, the first number of vector <span class="math inline">\(\vec{v} = [2 \space 3]^T\)</span> tells you how far to walk along the <span class="math inline">\(x\)</span>-axis and the second number tells you how far to walk along the <span class="math inline">\(y\)</span>-axis after that. The same goes for n-dimensional space.</p><h4 id="vector-operations">Vector Operations</h4><p>With the coordinates definition, it is straightforward to define the vector addition. Imagine there are 2 vectors <span class="math inline">\(\vec{u}, \vec{v}\)</span> and move the second one so that its tail sits at the tip of the first one. Then draw a new vector from the tail of the first one to the tip of the second one and that new vector is the sum of these 2 vectors. <span class="math display">\[ \vec{u} + \vec{v} = \begin{bmatrix} u_{1} \\ u_{2} \end{bmatrix} + \begin{bmatrix} v_{1} \\ v_{2} \end{bmatrix} = \begin{bmatrix} u_{1} + v_{1} \\ u_{2} + v_{2} \end{bmatrix} \]</span> And another vector operation is multiplication by a number. <span class="math inline">\(2\vec{v}\)</span> simply means stretching the original vector so that it's twice the original length. <span class="math inline">\(\frac{1}{3}\vec{v}\)</span> means squishing <span class="math inline">\(\vec{v}\)</span> so that it's <span class="math inline">\(\frac{1}{3}\)</span> of the original length. This process is called <strong>scaling</strong>. And these numbers to scale these vectors are <strong>scalars</strong>. <span class="math display">\[ a \vec{v} = a \begin{bmatrix} v_{1} \\ v_{2} \end{bmatrix} = \begin{bmatrix} a v_{1} \\ av_{2} \end{bmatrix} \]</span></p><h3 id="linear-combinations-span-and-basis-vectors">Linear combinations, span, and basis vectors</h3><p>Let us look at a vector <span class="math inline">\(\vec{v} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}\)</span>. If we use the above vector operations to express this vector using 2 special vectors <span class="math inline">\(\vec{i} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span> and <span class="math inline">\(\vec{j} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}\)</span> which are 2 unit vectors in x-direction and y-direction (Also, <span class="math inline">\(\vec{i}\)</span> and <span class="math inline">\(\vec{j}\)</span> are the typical basis vectors of the x-y coordinate system), we can get a linear combination <span class="math inline">\(\vec{v} = 2 \vec{i} + 3\vec{j}\)</span>. It is natural to think of <span class="math inline">\(\vec{v}\)</span> as adding <span class="math inline">\(\vec{i}\)</span> scaled by 2 and <span class="math inline">\(\vec{j}\)</span> scaled by 3. This &quot;adding scaled vectors&quot; process is using linear combinations of basis vectors to express any 2-D vector. And this is a key point if we are to discuss concepts below.</p><h4 id="span">Span</h4><p>The span of <span class="math inline">\(\vec{u}\)</span> and <span class="math inline">\(\vec{v}\)</span> is the set of all their linear combinations - <span class="math inline">\(a \vec{u} + b \vec{v}\)</span> with <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> varying over all real numbers. In other words, the span of these 2 vectors is also defining what are all the possible vectors you can reach using these 2 vectors and 2 fundamental operations - vector addition and scalar multiplication. If <span class="math inline">\(\vec{u}\)</span> and <span class="math inline">\(\vec{v}\)</span> line up, their span is just a line. If <span class="math inline">\(\vec{u}\)</span> and <span class="math inline">\(\vec{v}\)</span> are both zero vectors, their span is just a point. In most cases, their span is the entire infinite sheet of 2-D space.</p><p>From this perspective, linearly dependent vectors arise when one vector can be removed from a set of multiple vectors without reducing the span. And this vector can be expressed as a linear combination of the others because it's already in the span of the others. In other words, if each vector does add another dimension to the span, they are linearly independent.</p><h4 id="basis">Basis</h4><p>The basis of a vector space is a set of linearly independent vectors that span the full space.</p><h3 id="matrices-and-linear-transformations">Matrices and Linear Transformations</h3><p>Transformation is a fancy word for function. In the context of linear algebra, we would like to think about transformations that take in some input vector and spit out another vector. And the word transformation suggests how an input vector is converted to the output vector. It may experience spinning, stretching or reversing.</p><p>A transformation is linear if 1) all lines must remain lines without getting curved and 2) the origin must remain fixed in place. In general, linear transformations can be seen as <strong>keeping grid lines parallel and evenly spaced</strong>. A straightforward example is a rotation about the origin.</p><p>Now the question now becomes <strong>how should we describe any linear transformation numerically?</strong> The answer is super simple - we can show the <strong>transformed basis vector matrix</strong> to represent this process. An intuitive explanation is that given the basis vectors and every vector in their span is a certain linear combination of the basis. And because of the property of <strong>keeping grid lines parallel and evenly spaced</strong> in linear transformations, a vector starting off a certain linear combination of basis vectors still ends up the same linear combination of the transformed basis vectors. Mathematically, we can express the process <span class="math display">\[ i = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \rightarrow \begin{bmatrix} 1 \\ -2 \end{bmatrix}, \space j = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \rightarrow \begin{bmatrix} 3 \\ 0 \end{bmatrix} \\ \]</span> as the basis vector transformation, and the process <span class="math display">\[ \begin{bmatrix} x \\ y \end{bmatrix} = x\begin{bmatrix} 1 \\ 0 \end{bmatrix} + y \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 0\\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} \rightarrow \begin{bmatrix} x \\ y \end{bmatrix} = x\begin{bmatrix} 1 \\ -2 \end{bmatrix} + y \begin{bmatrix} 3 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 &amp; -2\\ 3 &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} \]</span> as linear transformation of any vector. This is exactly &quot;adding scaled vectors&quot; stated above in <strong>Linear Combination</strong> chapter. If we omit the vector <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span> to simplify the process and we can get <span class="math display">\[ \begin{bmatrix} 1 &amp; 0\\ 0 &amp; 1 \end{bmatrix} \rightarrow \begin{bmatrix} 1 &amp; -2\\ 3 &amp; 0 \end{bmatrix} \]</span> As we can see, the column vector tells what the original basis vector <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span> become after transformed. Obviously, the basis vector matrix transforms to another one representing a certain type of linear transformation and all vectors in the original span follow that.</p><p>If I am given a matrix <span class="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}\)</span>, I will say it indicates a <span class="math inline">\(90^\circ\)</span> counterclockwise rotation in 2-D space. Correspondingly, <span class="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}\)</span> means rotating vector <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span> <span class="math inline">\(90^\circ\)</span> counterclockwise about the origin. The important thing of introducing linear transformation is <strong>seeing any matrix as a certain linear transformation</strong> because it will be easier to understand concepts like matrix multiplication, determinant, eigenvectors and others.</p><h3 id="matrix-multiplication-as-composition">Matrix Multiplication as Composition</h3><p>Now consider 3 matrices <span class="math display">\[ R = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}, \space S = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}, RS = \begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 0 \end{bmatrix} \]</span> where <span class="math inline">\(R\)</span> represents a <span class="math inline">\(90^\circ\)</span> clockwise rotation, <span class="math inline">\(S\)</span> represents a shear and <span class="math inline">\(RS\)</span> means a rotation and shear. Note that <span class="math inline">\(RS\)</span> describes an overall effect of a rotation then a shear. It is equivalent to carrying out 2 successive actions to a vector like below. <span class="math display">\[ \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix} \left( \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} \right) = \begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} \]</span> where left hand side shows first rotating and shearing vector <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span> and right hand side shows the composite transformation. And this is because of the geometric meaning of matrix multiplication which <strong>applying one transformation then another</strong>.</p><p>Now we take a loot at how matrix multiplication is done mathematically <span class="math display">\[ \begin{bmatrix} e &amp; f \\ g &amp; h \end{bmatrix} \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} = \begin{bmatrix} a \begin{bmatrix} e \\ g \end{bmatrix} + c \begin{bmatrix} f \\ h \end{bmatrix} &amp;&amp; b \begin{bmatrix} e \\ g \end{bmatrix} + d \begin{bmatrix} f \\ h \end{bmatrix} \end{bmatrix} \]</span> we can think of matrix multiplication as transforming basis vectors under different rules. Naturally, the first column vector is the vector <span class="math inline">\(\begin{bmatrix} a \\ c \end{bmatrix}\)</span> after transformed and second column follows the similar procedure.</p><p>Here are some matrix multiplication properties which can be easily proved by this thought.</p><ul><li><p>Associativity</p><p><span class="math inline">\((AB)C = A(BC)\)</span> can be seen as first applying transformation represented by <span class="math inline">\(C\)</span> and then <span class="math inline">\(AB\)</span> and also can be seen as first applying composite transformation represented by <span class="math inline">\(BC\)</span> and then <span class="math inline">\(A\)</span>. They are equivalent in transforming as the composite transformation <span class="math inline">\(ABC\)</span>.</p></li><li><p>Commutativity</p><p><span class="math inline">\(AB \neq BA\)</span> can be proved if A is a rotation and B is a shear. <span class="math inline">\(AB\)</span> is a shear-then-rotate transformation that will make basis vectors point close together while <span class="math inline">\(BA\)</span> is a rotate-then-shear transformation giving basis vectors pointing far part.</p></li></ul><h3 id="determinant">Determinant</h3><p>We have known any matrix represents a certain type of linear transformation and we describe the scaling size of a linear transformation represented by a matrix using <strong>determinant</strong>. If the determinant of a transformation is <span class="math inline">\(3\)</span> then this transformation increases the area of a region by a factor of <span class="math inline">\(3\)</span>. Let's look at the below matrix <span class="math display">\[ \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 2 \end{bmatrix} \]</span> whose linear transformation means stretching all vectors in <span class="math inline">\(y\)</span> direction by the factor of <span class="math inline">\(2\)</span>. This turns out to increase all areas by the factor of <span class="math inline">\(2\)</span>. And this scaling factor is exactly described by determinant of the matrix.</p><p>Sometimes, the determinant of a matrix can be <strong>negative</strong>. In this case, the absolute value of the determinant still indicates the scaling factor while the negative sign means the orientation determined by basis vectors is now different from the original. To make it more clearly, we can think the original 2-D space as a sheet and the sheet is now flipped after the linear transformation is done.</p><p>Mathematically, the determinant of a 2-D matrix is computed as <span class="math display">\[ det\left( \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} \right) = ab - cd \]</span> and <span class="math inline">\(ab - cd\)</span> is exactly the area of parallelogram whose adjacent edges are <span class="math inline">\(\begin{bmatrix} a \\ c \end{bmatrix}\)</span> and <span class="math inline">\(\begin{bmatrix} b \\ d \end{bmatrix}\)</span>. And if we extend the determinant computation to 3-D, we will see the determinant is exactly the volume of a parallelepiped spanned by the matrix's column vectors.</p><p>Also, determinant of matrices satisfy the following rules <span class="math display">\[ det(M_1 M_2) = det(M_1) det(M_2) \]</span> because the transformation represented by <span class="math inline">\(M_1 M_2\)</span> is equivalent to carrying out <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> successively and therefore the overall scaling factor is the multiplication of the separate scaling factors.</p><h3 id="inverse-matrices-column-space-and-null-space">Inverse Matrices, Column Space and Null Space</h3><p>From the perspective of linear transformations, these concepts will look different if we understand these concepts in the usual computation way. Let's first look at a system of equations <span class="math display">\[ \begin{matrix} 2x + 5y + 3z = -3 \\ 4x + 0y + 8z = 0 \\ 1x + 3y + 0z = 2 \end{matrix} \]</span> which is super familiar when we were at primary school. But if we present this linear system using matrix multiplication, we can get <span class="math display">\[ \begin{bmatrix} 2 &amp; 5 &amp; 3 \\ 4 &amp; 0 &amp; 8 \\ 1 &amp; 3 &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} -3 \\ 0 \\ 2 \end{bmatrix} \]</span> which is also very intuitive if we express the above system as <span class="math inline">\(A \vec{x} = \vec{b}\)</span> recall the geometric meaning of matrix multiplication. Obviously, <span class="math inline">\(A\)</span> indicates a linear transformation and solving <span class="math inline">\(A \vec{x} = \vec{b}\)</span> means we are looking for an <span class="math inline">\(\vec{x}\)</span> which lands on <span class="math inline">\(\vec{b}\)</span> after transformed. In short, we can think of a certain vector is stretched and rotated to become <span class="math inline">\(\begin{bmatrix} -3 \\ 0 \\ 2 \end{bmatrix}\)</span> and this is exactly what we are looking for. How? Let's first consider the situation where <span class="math inline">\(det(A) \neq 0\)</span> meaning the transformation doesn't shrink the space dimension. And we can use <span class="math inline">\(A\)</span>'s inverse to get the solution <span class="math display">\[ A^{-1} A \vec{x} = \vec{x} = A^{-1} \vec{b} \]</span> Note that <span class="math inline">\(\vec{x}\)</span> is transformed to <span class="math inline">\(\vec{b}\)</span> under <span class="math inline">\(A\)</span> and <span class="math inline">\(\vec{b}\)</span> is transformed back to <span class="math inline">\(\vec{x}\)</span> under <span class="math inline">\(A^{-1}\)</span>. This is pretty similar to the concept of <strong>functions and inverse functions</strong> <span class="math display">\[ f(x) = y \\ f^{-1}(y) = x \\ f^{-1}(f(x)) = f^{-1}(y) = x \]</span> we can see any value becomes itself if mapped by a function then mapped by the corresponding inverse function. And the same idea goes for linear transformations. <span class="math display">\[ A^{-1} A \vec{x} = \vec{x} \]</span> In general, <span class="math inline">\(A^{-1}\)</span> is a unique transformation that we will end up back where we started if we apply <span class="math inline">\(A\)</span> then apply <span class="math inline">\(A^{-1}\)</span>. <span class="math inline">\(A^{-1} A\)</span> comes to a transformation that does nothing and this is also called identity transformation. Geometrically, <span class="math inline">\(A^{-1}\)</span> transforms every vector back to what they are before they are transformed by <span class="math inline">\(A\)</span>. For example, if <span class="math inline">\(A\)</span> is a counterclockwise rotation by <span class="math inline">\(90^{\circ}\)</span> and then <span class="math inline">\(A^{-1}\)</span> is a clockwise rotation by <span class="math inline">\(90^{\circ}\)</span>.</p><p>However, if <span class="math inline">\(det(A) = 0\)</span>, it means <span class="math inline">\(A\)</span> squishes a high-dimension space into a low-dimension space like squishing a plane into a line. At this time, there is no inverse matrix <span class="math inline">\(A^{-1}\)</span> because we cannot &quot;unsquish&quot; a line into a plane. At least, that's not something a function can do since that would require an individual vector to convert to a multiple vectors while function is always a 1-to-1 mapping.</p><p>We have a new terminology <strong>rank</strong> to describe these situations where <span class="math inline">\(det(A) = 0\)</span>. For a 3-d matrix <span class="math inline">\(A\)</span>, if the output of the transformation is a line meaning it's one-dimensional, we say the transformation <span class="math inline">\(A\)</span> has a rank of <span class="math inline">\(1\)</span>. Similarly, if the output is a plane meaning it's two-dimensional, we say the transformation <span class="math inline">\(A\)</span> has a rank of <span class="math inline">\(2\)</span>. So <strong>rank</strong> means the number of dimensions in the output of a transformation.</p><p>To sum up, the set of all possible outputs <span class="math inline">\(A \vec{x}\)</span> is called the column space of the matrix <span class="math inline">\(A\)</span>. This is pretty natural because the column vectors of <span class="math inline">\(A\)</span> tells us where the basis vectors land after transformation. If the rank of a matrix is as high as it can be, it means it equals the number of columns and we call the matrix <strong>full rank</strong>. If all basis vectors land on a line for a 3-D matrix, then the column space is a line and <span class="math inline">\(rank(A) = 1\)</span>. Now solving the equation has become the question that if the target vector <span class="math inline">\(\vec{b}\)</span> is within the span of columns of <span class="math inline">\(A\)</span>. Let's continue with the above example. If <span class="math inline">\(\vec{b}\)</span> happens to be on that line where all basis vectors land after transformation, then there are infinite solutions to that equation. However, if <span class="math inline">\(\vec{b}\)</span> happens to be out of scope of the span, there is no solution.</p><p>Let's also consider a special vector which is always in the column space whatever the transformation, and this vector is called zero vector. And the set of all possible vectors that land on the origin after transformation is called <strong>null space</strong> or <strong>kernel</strong> of your matrix. It's the space of all vectors that become null. And when we try to solve an equation like <span class="math inline">\(A \vec{x} = \vec{0}\)</span>, the null space gives us all possible solutions to this equation. Also, we call this kind of equations with the name of <strong><a href="https://en.wikipedia.org/wiki/System_of_linear_equations#Homogeneous_systems" target="_blank" rel="noopener">Homogeneous Linear Equations</a></strong>.</p><h3 id="non-square-matrices-as-transformations-between-dimensions">Non-square Matrices as Transformations between Dimensions</h3><p>For a non-square matrix, we also use linear transformation perspective to interpret the geometric meaning it stands for. For example, <span class="math display">\[ \begin{bmatrix} 3 &amp; 1 \\ 4 &amp; 1 \\ 5 &amp; 9 \end{bmatrix} \]</span> which is a <span class="math inline">\(3 \times 2\)</span> matrix and its column vectors still indicate where the original 2-D space basis vectors land after transformation. As we can see, 2 columns indicate that the input space has 2 basis vectors and 3 rows indicate that the landing spots for each of these basis vectors is described with three separate coordinates. This matrix reveals a transformation that maps 2 dimensions to 3 dimensions. However, this matrix is still full rank because the number of column equals the rank of the column space. Note, the rank is still 2 even the matrix represents a mapping from 2-D to 3-D. This is because the set of all possible outputs after transformation still span a plane in 3-D space instead of a 3-D space.</p><p>Similarly, the below <span class="math inline">\(2 \times 3\)</span> matrix <span class="math display">\[ \begin{bmatrix} 3 &amp; 1 &amp; 5\\ 4 &amp; 1 &amp; 5\\\end{bmatrix} \]</span> represents a mapping from 3-D to 2-D because the 3 columns indicate the input space has 3 basis vectors while the 2 rows indicate the landing spots for each of these basis vectors is described with only 2 coordinates. We can think of this process as squishing and projecting the 3 orthogonal basis onto a 2-D plane.</p><p>Now, let's look at a <span class="math inline">\(1 \times 2\)</span> matrix <span class="math display">\[ \begin{bmatrix} 3 &amp; 2 \end{bmatrix} \]</span> which represents the process of smashing a plane into a line while keeping evenly spaced dots remain evenly spaced after mapped.</p><p>To sum up, for non-square matrices, the number of columns and the number of rows represent the dimensions of input space and the dimensions of output space respectively. And there is no determinant for non-square matrices. This is because the determinant of a matrix indicates the scaling size of transformation in the same space and within the same dimension. However, we cannot measure how the size of space change over dimensions.</p><h3 id="dot-products-and-duality">Dot Products and Duality</h3><p>A fuller understanding of the role the dot products play in math can only be found in the light of linear transformations.</p><p>Let's first review the standard introduction of dot products and its geometric meaning. <span class="math display">\[ \vec{v} \cdot \vec{w} = \begin{bmatrix} a \\ b \end{bmatrix} \cdot \begin{bmatrix} c \\ d \end{bmatrix} = ac + bd\\ \vec{v} \cdot \vec{w} = (Length \space of \space projected \space \vec{w}) (Length \space of \space \vec{v}) \\ \vec{v} \cdot \vec{w} = (Length \space of \space projected \space \vec{v}) (Length \space of \space \vec{w}) \]</span> One surprisingly amazing property of dot products is that the order of this projection and multiplication process doesn't matter. We can project <span class="math inline">\(\vec{v}\)</span> onto <span class="math inline">\(\vec{w}\)</span> and multiply the projected length of <span class="math inline">\(\vec{v}\)</span> by the length of <span class="math inline">\(\vec{w}\)</span> and we can also project <span class="math inline">\(\vec{w}\)</span> onto <span class="math inline">\(\vec{v}\)</span> and multiply the projected length of <span class="math inline">\(\vec{w}\)</span> by the length of <span class="math inline">\(\vec{v}\)</span>. And this actually can be proved by building similar triangles or as follows <span class="math display">\[ \vec{v} \cdot \vec{w} = |\vec{w}| cos&lt;\vec{v}, \vec{w}&gt; |\vec{v}| = |\vec{w}| |\vec{v}| cos&lt;\vec{v}, \vec{w}&gt; \]</span> And another tricky point is how is the perspective of projection and multiplication associated with the perspective of multiplying coordinates of pairs and adding them together?</p><p>To answer this, let's recall the geometric meaning of <span class="math inline">\(1 \times 2\)</span> matrix covered in last chapter, say a matrix like below <span class="math display">\[ \begin{bmatrix} 3 &amp; -2 \end{bmatrix} \]</span> which means a transformation where 2 basis vectors in 2-D space have now landed on 3 and -2 on a 1-D number line. And if we apply this transformation to a certain 2-D vector, we can get <span class="math display">\[ \begin{bmatrix} 3 &amp; -2 \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = 3a -2b \]</span> where <span class="math inline">\(3a-2b\)</span> is exactly where the original vector <span class="math inline">\(\begin{bmatrix} a \\ b \end{bmatrix}\)</span> land on the number line after transformed. And this matrix multiplication operation is numerically equivalent to the dot products between <span class="math inline">\(\begin{bmatrix} a \\ b \end{bmatrix}\)</span> and <span class="math inline">\(\begin{bmatrix} 3 \\ -2 \end{bmatrix}\)</span>. So it's natural to declare there is a nice association between <span class="math inline">\(1 \times 2\)</span> matrix and a 2-D vector. Let's look at the image below</p><figure><img src="dot.JPG" alt="Dot"><figcaption>Dot</figcaption></figure><p>In the 2-D coordinate system, we have a vector <span class="math inline">\(\vec{u}\)</span> and we also have basis vectors sitting on <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axis. Also, we draw a number line through <span class="math inline">\(\vec{u}\)</span> to show where these 2-D vectors will land. From the image, using a line of symmetry, we can easily tell the basis vector <span class="math inline">\(\hat{i}\)</span> sitting on <span class="math inline">\(x\)</span> axis is converted to a number exactly the same as the <span class="math inline">\(x\)</span> coordinate of <span class="math inline">\(\vec{u}\)</span>. And the same goes for the other basis vector <span class="math inline">\(\hat{j}\)</span>. Till now, we have found a 2-D to 1-D linear projection transformation restricted by <span class="math inline">\(\vec{u}\)</span> and the entries of corresponding <span class="math inline">\(1 \times 2\)</span> matrix describing the transformation are exactly the coordinates of <span class="math inline">\(\vec{u}\)</span>. So this just explains why taking a dot product among vectors can be interpreted as projecting a vector onto the span of the other one and taking the length. <span class="math display">\[ \begin{bmatrix} u_x &amp; u_y \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = a \cdot u_x + b \cdot u_y \\ \begin{bmatrix} u_x \\ u_y \end{bmatrix} \cdot \begin{bmatrix} a \\ b \end{bmatrix} = a \cdot u_x + b \cdot u_y \\ \]</span> Let us think about the process again! We had a linear transformation from 2-D space to the number line which was not defined by numerical dot products. It was just defined by projecting space onto a copy of the number line decided by the vector <span class="math inline">\(\vec{u}\)</span>. Because the transformation is linear, it was necessarily described by some <span class="math inline">\(1 \times 2\)</span> matrix whose entries are the coordinates <span class="math inline">\(\vec{u}\)</span>. And since multiplying this matrix by another 2-D vector <span class="math inline">\(\vec{v}\)</span> is the same as taking a product between <span class="math inline">\(\vec{u}\)</span> and <span class="math inline">\(\vec{v}\)</span>. This transformation and the vector is inescapably related to each other. The punch line here is, for any linear transformation whose output space is the number line, there is going to be a unique vector corresponding to that transformation. In this sense, applying the transformation is the same thing as taking a product. This is an example duality: the dual of a linear transformation from <span class="math inline">\(n\)</span> dimension to <span class="math inline">\(1\)</span> dimension is a vector in that <span class="math inline">\(n\)</span> dimension.</p><p>A takeaway here is that a vector sometimes can be interpreted as a linear transformation instead of an arrow in space.</p><h3 id="cross-product">Cross Product</h3><p>For 2-D vectors, the cross product of 2 2-D vectors is a new vector. And the length of the vector will be the area of a parallelogram spanned by these 2 vectors. The direction of the new vector is going to be perpendicular to that parallelogram and can be told with right hand rules. Specifically, the cross product is defined as <span class="math display">\[ \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} \times \begin{bmatrix} w_1 \\ w_2 \\ w_3 \end{bmatrix} = det \left( \begin{bmatrix} \hat{i} &amp; v_1 &amp; w_1 \\ \hat{j} &amp; v_2 &amp; w_2 \\ \hat{k} &amp; v_3 &amp; w_3 \end{bmatrix} \right) \]</span> Recall that for a <span class="math inline">\(2 \times 1\)</span> matrix, there is always a 2-D vector (which is the dual vector of that transformation) that corresponds to it. And performing the transformation is the same as taking a product with that vector. This is called duality. While this does not only apply to <span class="math inline">\(2 \times 1\)</span> matrix, it also applies to any matrix if the corresponding linear transformation's output space is <span class="math inline">\(1\)</span> dimension. And the cross product also embodies the idea of duality.</p><p>To explain how duality is applied in cross product, let's plan to</p><ol type="1"><li><p>Define a 3d-to-1d linear transformation in terms of <span class="math inline">\(\hat{v}\)</span> and <span class="math inline">\(\hat{w}\)</span>,</p></li><li>Find its dual vector <span class="math inline">\(\hat{p}\)</span> in 3-D space,</li><li><p>Show that this dual vector <span class="math inline">\(\hat{p} = \hat{v} \times \hat{w}\)</span>.</p></li></ol><p>And this is all because this transformation displays the connection between the computation and the geometry of the cross product.</p><p>Recall that in 2D space, the cross product of <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span> is simply the determinant of the matrix whose column vectors are <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span>. This is also the are of the parallelogram spanned by these 2 vectors. And we will naturally think of the volume of some parallelepiped as the cross product among 3D vectors. But the question is how the parallelepiped looks like? Now we consider a function <span class="math display">\[ L\left( \begin{bmatrix} x \\ y \\ z \end{bmatrix} \right) = det \left( \begin{bmatrix} x &amp; v_1 &amp; w_1 \\ y &amp; v_2 &amp; w_2 \\ z &amp; v_3 &amp; w_3 \end{bmatrix} \right) \]</span> which describes a parallelepiped spanned by <span class="math inline">\(\vec{v}\)</span>, <span class="math inline">\(\vec{w}\)</span> and an unknown 3D vector. And an important feature about this function is its linearity. Based on that, we can bring the idea of duality, which means we can introduce a <span class="math inline">\(1 \times 3\)</span> matrix to describe the 3D-to-1D transformation, <span class="math display">\[ \begin{bmatrix} v_2 w_3 - v_3 w_2 &amp; v_3 w_1 - v_1 w_3 &amp; v_1 w_2 - v_2 w_1 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = det \left( \begin{bmatrix} x &amp; v_1 &amp; w_1 \\ y &amp; v_2 &amp; w_2 \\ z &amp; v_3 &amp; w_3 \end{bmatrix} \right) \]</span> and there is a corresponding vector and taking dot product of it is equivalent as performing that transformation. <span class="math display">\[ \begin{bmatrix} v_2 w_3 - v_3 w_2 \\ v_3 w_1 - v_1 w_3 \\ v_1 w_2 - v_2 w_1 \end{bmatrix} \cdot \begin{bmatrix} x \\ y \\ z \end{bmatrix} = det \left( \begin{bmatrix} x &amp; v_1 &amp; w_1 \\ y &amp; v_2 &amp; w_2 \\ z &amp; v_3 &amp; w_3 \end{bmatrix} \right) \]</span> So the function above is built to find such a vector <span class="math inline">\(\vec{p}\)</span> that taking a dot product between <span class="math inline">\(\vec{p}\)</span> and <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span> is equivalent to the determinant of the matrix whose column vectors are <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span>, <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span>.</p><p>And this also gives the geometric meaning of <span class="math inline">\(\vec{p}\)</span>. Because <span class="math inline">\(\vec{p}\)</span> is such a 3D vector that taking a dot product between <span class="math inline">\(\vec{p}\)</span> and <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span> is equivalent to the signed volume of the parallelepiped whose spanned by <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span>, <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span>. To state the geometric property more clearly, let's decompose the volume of the above parallelepiped as</p><p><span class="math display">\[ (Area \space of \space parallelogram \space spanned \space by \space \vec{v}, \vec{w}) \times (Component \space of \begin{bmatrix} x \\ y \\ z \end{bmatrix} perpendicular \space to \space \vec{v}, \vec{w}) \]</span> From this perspective, the function above is projecting the vector <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span> onto a line perpendicular to <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span>, then multiplying the length of the projection by the area of the parallelogram spanned by <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span>. Also, this is the same as taking a product between <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span> and a vector perpendicular to <span class="math inline">\(\vec{v}\)</span>, <span class="math inline">\(\vec{w}\)</span> with a length <span class="math inline">\(=\)</span> the area of that parallelogram. So this is the geometric meaning of <span class="math inline">\(\vec{p}\)</span>.</p><p>To integrate the geometry and computation perspective, <span class="math inline">\(\vec{p}\)</span> and <span class="math inline">\(\vec{v} \times \vec{w}\)</span> are 2 dual vectors of the same linear transformation, so they must be the same. So we have presented how the cross product of two 3D vectors is computed and its geometric meaning.</p><h3 id="change-of-basis">Change of Basis</h3><p>Till now, we have always used a coordinate system to translate between vectors and a set of numbers. And there are 2 special vectors <span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span> called basis vectors of the standard coordinate system. Each vector in the coordinate system is a linear combination of these basis vectors. Now let's think about what will happen if we change the set of basis vectors into a different set.</p><p>Let's consider another set of basis vectors <span class="math inline">\(\hat{b_1} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span> and <span class="math inline">\(\hat{b_2} = \begin{bmatrix} -1 \\ 1 \end{bmatrix}\)</span>, this is just like <span class="math inline">\(\hat{i} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span> and <span class="math inline">\(\hat{b_2} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}\)</span> in our system. Now it's natural to know how to translate vectors between different coordinate systems. For example, if a vector is expressed as <span class="math inline">\(\begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span> in the <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> system, what does it look like in our system? Likewise, if a vector is expressed as <span class="math inline">\(\begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span> in our system, what does it look like in the <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> system?</p><p>Now let's look at the matrix whose columns are <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> as below <span class="math display">\[ \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1\end{bmatrix} \]</span> which transforms the basis vectors into a new set of basis vectors, and also transforms the original coordinate system into a new coordinate system. But numerically, this new basis vectors are still expressed using our original language. Therefore, if a vector is expressed as <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span> in our language, what it will look like in the <span class="math inline">\(\hat{b_1}\)</span>. <span class="math inline">\(\hat{b_2}\)</span> system is <span class="math display">\[ \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1\end{bmatrix} \begin{bmatrix} x \\ y\end{bmatrix} \]</span> and this is because a vector is always the same linear combination of basis vectors whatever the transformation is.</p><p>In the opposite, if a vector is expressed as <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span> in the <span class="math inline">\(\hat{b_1}\)</span>. <span class="math inline">\(\hat{b_2}\)</span> system, what it is in our original system? <span class="math display">\[ \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1\end{bmatrix}^{-1} \begin{bmatrix} x \\ y\end{bmatrix} \]</span> and this is because of the same reason as above.</p><p>Since vectors are not the only thing expressed with coordinates, the question now becomes how we translate matrices/ linear transformations between different coordinate systems? For example, what a <span class="math inline">\(90^{\circ}\)</span> clockwise rotation looks like in the <span class="math inline">\(\hat{b_1}\)</span>. <span class="math inline">\(\hat{b_2}\)</span> system? <span class="math display">\[ \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1\end{bmatrix}^{-1} \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1\end{bmatrix} \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1\end{bmatrix} \]</span> this process is like first performing the transformation to express basis vectors in our language and then performing rotation. This intermediate matrix is the rotated <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> basis vectors in our language. The remaining step is to translate the intermediate one into a final one in the language of <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> system. The composition of these 3 matrices gives us the <span class="math inline">\(90^{\circ}\)</span> clockwise rotation in the language of <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> system.</p><p>Whenever we see an expression like <span class="math inline">\(A^{-1} M A\)</span>, it suggests a translation or an empathy in a mathematical way, where <span class="math inline">\(M\)</span> represents an intuitive linear transformation and the other 2 matrices represent the empathy, the translator or the shift in perspective. It still indicates the same transformation but from other perspective.</p><h3 id="eigenvectors-and-eigenvalues">Eigenvectors and Eigenvalues</h3><p>Let's think about a matrix like <span class="math inline">\(\begin{bmatrix} 3 &amp; 1 \\ 0 &amp; 2\end{bmatrix}\)</span> and a random vector <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span>. If we apply the linear transformation represented by that matrix to that vector, the vector is most likely to get knocked off the vector's span (the line passing through the origin and its tip) during the transformation. But there are some special vectors that do remain on their own span, meaning the effect that the matrix has on such a vector is just stretching or squishing like a scalar. For this specific example, <span class="math inline">\(\begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span> is such a special vector which is stretched to 3 times itself and still lands on <span class="math inline">\(x\)</span> axis. And due to linearity, any other vector on the <span class="math inline">\(x\)</span> axis (the vector's span) will also be stretched out by a factor of 3 during the transformation. In summary, these vectors are so-called eigenvectors of the transformation, and there are corresponding eigenvalues measuring the factor it stretches or squishes during that transformation. If we translate what we plan to do above into mathematical terms, we can get <span class="math display">\[ A \vec{v} = \lambda \vec{v} \]</span> which is equivalent to solving <span class="math inline">\((A - \lambda I)\vec{v} = \vec{0}\)</span>. If we want a non-zero solution of <span class="math inline">\(\vec{v}\)</span>, then the transformation <span class="math inline">\(A - \lambda I\)</span> must be a dimension reducing one, meaning a zero determinant. We can think of <span class="math inline">\(\lambda\)</span> as a disturbance term and changes the linear transformation <span class="math inline">\(A\)</span> in a way that the new transformation squishes space into a lower dimension or equivalently the new column vectors are colinear.</p><p>And why are these useful things to think about? Let's consider some 3D rotation. If we can find an eigenvector for that rotation, then we find the axis of that rotation. It's much easier to think about a 3D rotation in terms of some axis of rotation with some angle compared to a <span class="math inline">\(3 \times 3\)</span> matrix.</p><p>There are some takeaways about solving eigenvalues and eigenvectors</p><ul><li>It doesn't matter if there is a negative eigenvalue as long as the eigenvector stays on the line it spans out without getting knocked off.</li><li>A transformation doesn't have to have eigenvectors. For example, there are no eigenvectors for a <span class="math inline">\(90^{\circ}\)</span> rotation since any non-zero vector is rotated and moves its own span.</li><li>There may be only 1 eigenvalue but more than 1 eigenvectors for a transformation. Say a transformation that stretches everything by 2. It only has the eigenvalue of 2 but every vector in the plane is the eigenvector with that eigenvalue.</li></ul><p>What if all basis vectors are eigenvectors? If we write the above eigenvector equation in matrix format, it's $A V = V $ where columns of <span class="math inline">\(V\)</span> are eigenvectors of <span class="math inline">\(A\)</span> and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix whose diagonal elements are the corresponding eigenvalues. Then the linear transformation <span class="math inline">\(A\)</span> is express as <span class="math inline">\(V^{-1} A V = \Lambda\)</span> in the language of eigen-basis system, and we can then get <span class="math inline">\(A^n = V^{-1} \Lambda^{n} V\)</span> consequently. Because <span class="math inline">\(\Lambda\)</span> is a diagonal matrix, it's much easier to compute <span class="math inline">\(\Lambda^n\)</span> compared to compute the <span class="math inline">\(n_{th}\)</span> power of a non-diagonal matrix.</p><p>What if the basis vectors are not eigenvectors? Because of the great properties above, we would like to perform change of basis so that these eigenvectors become our basis vectors. But that can only happen when there are enough eigenvectors to span a full space. Mathematically, we can get basis vectors as follows <span class="math display">\[ V^{-1} A V = \Lambda \]</span> and this is also called diagonalization.</p><h3 id="abstract-vector-spaces">Abstract Vector Spaces</h3><p>Let's go back to the original question and ask what are vectors? Are they lists of numbers or ordered arrows? We won't call vectors as list of numbers considering determinant and eigenvectors don't care about the coordinate system. In summary, vectors can be specified as ordered arrows or lists of numbers but they are technically not the full definition of vectors. In mathematical terms, they are called vector spaces. And there are 8 axioms any vector space must satisfy so that those vector operations, dot products and eigen-things are valid.</p><h3 id="other-topics">Other Topics</h3><p><strong>Orthogonal Matrix</strong></p><p>An <strong>orthogonal matrix</strong> is a square matrix whose columns and rows are orthogonal unit vectors (orthonormal vectors). Mathematically speaking, for the <span class="math inline">\(i_{th}\)</span> and <span class="math inline">\(j_{th}\)</span> column <span class="math inline">\(C_i\)</span>, <span class="math inline">\(C_j\)</span> of the orthogonal matrix <span class="math inline">\(A\)</span>, we have <span class="math display">\[ \langle C_i, C_j \rangle = \delta_{ij} \]</span> where <span class="math inline">\(\delta_{ij} = 1\)</span> for <span class="math inline">\(i=j\)</span> while <span class="math inline">\(\delta_{ij} = 0\)</span> for <span class="math inline">\(i \neq j\)</span>. Then we can derive <span class="math display">\[ A^T A = \begin{bmatrix} C_1^T \\ ... \\C_n^T \end{bmatrix} \begin{bmatrix} C_1 &amp; ... &amp; C_n \end{bmatrix} = [\langle C_i, C_j \rangle]_{1\leq i,j \leq n}=I_n \]</span></p><p>which is also <span class="math inline">\(A^T = A^{-1}\)</span>. Besides, <span class="math inline">\(A A^T = I_n\)</span> holds true and <span class="math inline">\(A^T\)</span> is also an orthogonal matrix.</p><p>Note, if a matrix has pairwise orthogonal column vectors but not pairwise orthogonal row vectors, then it is not an orthogonal matrix. For example, <span class="math inline">\(\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\1 &amp; 0 &amp; 0\end{bmatrix}\)</span> is such a matrix.</p><p>Geometrically, an orthogonal matrix represents an <strong>orthogonal transformation that preserves a symmetric inner product</strong>. To put it simply, an orthogonal transformation is either a rigid rotation or a rotation followed by a flip. Either of them does not stretch or squish the original space and preserves lengths of vectors and angles between vectors after transformation. Mathematically, these properties for orthogonal matrices can be expressed as</p><ul><li><p><span class="math inline">\(\langle \vec{v}, \vec{w} \rangle = \langle A\vec{v}, A\vec{w} \rangle\)</span></p></li><li><p><span class="math inline">\(||\vec{v}|| = ||A \vec{v}||\)</span></p></li><li><p><span class="math inline">\(det(A) = 1 \space or -1\)</span></p></li></ul><p>Besides, the product of orthogonal matrices is also orthogonal. Orthogonal matrices are the real analogue of <a href="https://en.wikipedia.org/wiki/Unitary_matrix" target="_blank" rel="noopener">unitary matrices</a> (which require complex square matrices). And a number of <a href="https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra" target="_blank" rel="noopener">decompositions</a> involve unitary/ orthogonal matrices including <span class="math inline">\(QR\)</span> decomposition, Singular value decomposition and Eigenvalue decomposition of a symmetric matrix.</p><h3 id="transpose-of-matrices">Transpose of Matrices</h3><p>The transpose of a matrix is flipping a matrix over its diagonal and switching its rows and columns in a way that <span class="math inline">\(A_{ij} = A^T_{ji}\)</span>. But from the perspective of linear transformation, how do we interpret the transpose a matrix geometrically? There is a <a href="https://math.stackexchange.com/questions/2192992/truly-intuitive-geometric-interpretation-for-the-transpose-of-a-square-matrix" target="_blank" rel="noopener">discussion</a> and giving 3 perspectives of the transpose of matrices. What I find the most intuitive is the one based on Singular Value Decomposition. Let's first find out how we interpret SVD geometrically as below</p><figure><img src="SVD.png" alt="SVD"><figcaption>SVD</figcaption></figure><p>which is the geometric representation of SVD <span class="math inline">\(A = U \Sigma V^T\)</span> where <span class="math inline">\(V\)</span> and <span class="math inline">\(U\)</span> are orthogonal matrices while <span class="math inline">\(\Sigma\)</span> is a diagonal matrix. And therefore the linear transformation represented by <span class="math inline">\(A\)</span> can be seen as the successive actions of first rotating (<span class="math inline">\(V^T\)</span>), then scaling (<span class="math inline">\(\Sigma\)</span>) and finally rotating (<span class="math inline">\(U\)</span>). This is what we are getting when we look at the image above from the left to the right.</p><p>The transpose of a matrix <span class="math inline">\(A^T = V \Sigma U^T\)</span> can then be derived. At the same time, <span class="math inline">\(U^T = U^{-1}\)</span> and <span class="math inline">\(V = {(V^T)}^{-1}\)</span> hold true because they are orthogonal matrices and we can rewrite the transpose of <span class="math inline">\(A\)</span> as <span class="math inline">\(A^T = {(V^T)}^{-1} \Sigma U^{-1}\)</span>. Since <span class="math inline">\(A^{-1}\)</span> can be seen as such a linear transformation that transforms whatever is transformed by <span class="math inline">\(A\)</span> back to its original state, then <span class="math inline">\(V\)</span> is rotating whatever is rotated by <span class="math inline">\(V^T\)</span> to the original and <span class="math inline">\(U^T\)</span> is rotating whatever is rotated by <span class="math inline">\(U\)</span> to the original state. And therefore we can also interpret <span class="math inline">\(A^T\)</span> as the successive actions of first rotating, then scaling and finally rotating. What is different is that <span class="math inline">\(A^T\)</span> is doing exactly the opposite actions of <span class="math inline">\(A\)</span> in terms of rotations.</p><p>With this interpretation, we can easily prove <span class="math inline">\(det(A) = det(A^T)\)</span> and <a href="https://proofwiki.org/wiki/Determinant_of_Transpose" target="_blank" rel="noopener">here</a> is another brief proof.</p><h3 id="less-intuitive-concepts-and-conclusions">Less Intuitive Concepts and Conclusions</h3><ul><li><p>Interpret the trace of matrices geometrically. <span class="math inline">\(Trace(AB) = Trace(BA)\)</span>.</p></li><li><p><span class="math inline">\(Rank(column \space space \space of \space A) = Rank(row \space space \space of \space A)\)</span></p></li><li><p><a href="https://en.wikipedia.org/wiki/Kernel_(linear_algebra)" target="_blank" rel="noopener">Kernel</a>, <a href="https://en.wikipedia.org/wiki/Image_(mathematics)" target="_blank" rel="noopener">Image</a> and <a href="https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem" target="_blank" rel="noopener">Rank Nullity Theorem</a></p><p>We have covered the concept of kernel in Null Spaces Chapter. If there is a <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> representing a linear transformation <span class="math inline">\(T: V \rightarrow W\)</span>, then the kernel of <span class="math inline">\(T\)</span> is defined as <span class="math display">\[ Kernel(T) = \{ \vec{v} \in V | T(\vec{v}) = \vec{0}\} \]</span> where <span class="math inline">\(\vec{0}\)</span> is the zero vector in <span class="math inline">\(W\)</span>.</p><p>While the image of <span class="math inline">\(T\)</span> or the range of <span class="math inline">\(T\)</span> is defined as <span class="math display">\[ Image(T) = \{T(\vec{v}) | \vec{v} \in V\} \]</span> Note, the image of <span class="math inline">\(T\)</span> is a subspace of the output space <span class="math inline">\(W\)</span> while the kernel of <span class="math inline">\(T\)</span> is a subspace of the input space <span class="math inline">\(V\)</span>. Here is a <a href="https://www.youtube.com/watch?v=vyYrvhbDhW4" target="_blank" rel="noopener">video</a> illustrating this.</p><p>And the <strong>Rank Nullity Theorem</strong> is stated as <span class="math display">\[ Rank(A) + dim(Kernel(T)) = n \]</span> Intuitively, we can think of <span class="math inline">\(n\)</span> as the number of dimensions in input space while <strong>rank</strong> means the number of dimensions in output space and <span class="math inline">\(dim(Kernel(T))\)</span> naturally is the dimension lost in performing the transformation <span class="math inline">\(T\)</span>. For example, for a <span class="math inline">\(1 \times 2\)</span> matrix <span class="math inline">\(A\)</span> that squishes a space into a number line, we know that there is one dimension of information missing in the 2D-to-1D transformation.</p></li><li><p>Solutions to Linear Systems</p><p>For a linear system <span class="math inline">\(Ax = b\)</span>, we can use the concept of rank and linear transformation to analyze if there is any solution to this system. From the perspective of linear transformation, <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(A\)</span> represents a transformation and <span class="math inline">\(x\)</span> is a vector in the input vector space and <span class="math inline">\(b\)</span> is another vector in the output vector space. The equation can be interpreted as the question <strong>if vector <span class="math inline">\(b\)</span> is within the output space of <span class="math inline">\(A\)</span></strong>. Mathematically speaking, that is <span class="math display">\[ rank([A \space b]) == rank(A) \]</span> where <span class="math inline">\([A \space b]\)</span> is an augmented matrix. If <span class="math inline">\(rank([A \space b]) &gt; rank(A)\)</span>, it means <span class="math inline">\(b\)</span> is out of space and there is no solution to that equation. If <span class="math inline">\(rank([A \space b]) = rank(A)\)</span>, it means <span class="math inline">\(b\)</span> is within space and there is one or many solution to that equation. As for it's a scenario with one solution or many solutions, it depends on <span class="math inline">\(rank(A) == n\)</span>. If <span class="math inline">\(rank(A)==n\)</span>, then there is only one solution. Otherwise, there are infinite solutions.</p></li><li><p>Characteristic Polynomial of Square Matrices</p><p>The characteristic polynomial of a square matrix is a polynomial which is invariant under matrix similarity and has the eigenvalues as roots. <span class="math display">\[ f_A(\lambda) = |\lambda I - A| \]</span> For <span class="math inline">\(B = Q^{-1}A Q\)</span>, the characteristic polynomial is <span class="math display">\[ f_B(\lambda) = |\lambda I - Q^{-1}A Q| = |Q^{-1}(\lambda I - A)Q| = |\lambda I - A| \]</span> which is the same as <span class="math inline">\(A\)</span>'s.</p></li></ul><h4 id="reference">Reference</h4><ol type="1"><li><a href="https://www.quora.com/What-is-the-geometric-interpretation-of-the-transpose-of-a-matrix" target="_blank" rel="noopener">What is the geometric interpretation of the transpose of a matrix?</a></li><li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" target="_blank" rel="noopener">Essence of Linear Algebra</a></li></ol></div></div><nav class="post-pagination"><a class="newer-posts" href="/2020/07/27/namespaces/">Previous post<br>Namespaces and Scope in Python </a><span class="page-number"></span> <a class="older-posts" href="/2020/05/02/SVD/">Next post<br>Singular Value Decomposition</a></nav></div></div><div class="single-column-footer">Proudly published with Hexo<br>Theme <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> by <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a><br>&copy; 2020 <a href="http://yoursite.com">Avalon</a></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.4/dist/umd/popper.min.js" integrity="sha256-EGs9T1xMHdvM1geM8jPpoo8EZ1V1VRsmcJz8OByENLA=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.5.17/dist/vue.min.js" integrity="sha256-FtWfRI+thWlNz2sB3SJbwKx5PgMyKIVgwHCTwa3biXc=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/smooth-scroll@14.2.1/dist/smooth-scroll.polyfills.min.js" integrity="sha256-CI4Gq5E0io1Pv0xM3qPM+NUIOhbIBvC3GiN1Y4KhXpw=" crossorigin="anonymous"></script><script src="/js/journal.js?24814520"></script></body></html>