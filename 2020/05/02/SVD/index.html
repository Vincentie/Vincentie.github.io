<!DOCTYPE html><html xmlns:v-bind="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="Hexo 3.9.0"><title>Singular Value Decomposition - Avalon</title><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="John Reese"><meta name="description" content="A Useful Decomposition in Linear Algebra"><meta name="keywords" content><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous"><link rel="stylesheet" href="/css/journal.css?64239607"><script src="/js/loadCSS.js"></script><script>loadCSS("https://fonts.googleapis.com/css?family=Lora|Montserrat|Fira+Mono|Material+Icons"),function(e){var t,a={kitId:"dwg1tuc",scriptTimeout:3e3,async:!0},o=e.documentElement,s=setTimeout(function(){o.className=o.className.replace(/\bwf-loading\b/g,"")+" wf-inactive"},a.scriptTimeout),c=e.createElement("script"),i=!1,n=e.getElementsByTagName("script")[0];o.className+=" wf-loading",c.src="https://use.typekit.net/"+a.kitId+".js",c.async=!0,c.onload=c.onreadystatechange=function(){if(t=this.readyState,!(i||t&&"complete"!=t&&"loaded"!=t)){i=!0,clearTimeout(s);try{Typekit.load(a)}catch(e){}}},n.parentNode.insertBefore(c,n)}(document)</script><noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora|Montserrat|Anonymous+Pro:400|Material+Icons"></noscript><link rel="stylesheet" href="/css/prism.css" type="text/css"></head><body><div id="top"></div><div id="app"><div class="single-column-drawer-container" ref="drawer" v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }"><div class="drawer-content"><div class="drawer-menu"><a class="a-block drawer-menu-item false" href="http://yoursite.com">Home </a><a class="a-block drawer-menu-item false" href="/archives">記事一覽 </a><a class="a-block drawer-menu-item false" href="/about/index.html">關於我 </a><a class="a-block drawer-menu-item false" href="/categories/index.html">分類 </a><a class="a-block drawer-menu-item false" href="/tags/index.html">標簽</a></div></div></div><transition name="fade"><div v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if="isDrawerOpen" v-on:click="toggleDrawer"></div></transition><nav ref="navBar" class="navbar sticky-top navbar-light single-column-nav-container"><div ref="navBackground" class="nav-background"></div><div class="container container-narrow nav-content"><button id="nav_dropdown_btn" class="nav-dropdown-toggle" type="button" v-on:click="toggleDrawer"><i class="material-icons">menu</i></button> <a ref="navTitle" class="navbar-brand" href="/">Avalon</a></div></nav><div class="single-column-header-container" ref="pageHead" v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }"><a href="/"><div class="single-column-header-title">Avalon</div><div class="single-column-header-subtitle">Welcome!!</div></a></div><div ref="sideContainer" class="side-container"><a class="a-block nav-head false" href="/"><div class="nav-title">远い理想郷</div><div class="nav-subtitle">お帰りなさい</div></a><div class="nav-link-list"><a class="a-block no-tint nav-link-item false" href="/archives">記事一覽 </a><a class="a-block nav-link-item false" href="/about/index.html">關於我 </a><a class="a-block nav-link-item false" href="/categories/index.html">分類 </a><a class="a-block nav-link-item false" href="/tags/index.html">標簽</a></div><div class="nav-footer">Proudly published with Hexo<br>Theme <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> by <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a><br>&copy; 2020 <a href="http://yoursite.com">Avalon</a></div></div><div ref="extraContainer" class="extra-container"><div class="pagination"><a id="globalBackToTop" class="pagination-action animated-visibility" href="#top" :class="{ invisible: scrollY == 0 }"><i class="material-icons pagination-action-icon">keyboard_arrow_up</i></a></div></div><div ref="streamContainer" class="stream-container"><div class="post-list-container post-list-container-shadow"><div class="post"><div class="post-head-wrapper" style="background-image:url(/2020/05/02/SVD/SVD.png)"><div class="post-title">Singular Value Decomposition<div class="post-meta"><time datetime="2020-05-03T00:48:35.000Z" itemprop="datePublished">2020-05-02 20:48 </time>&nbsp; <i class="material-icons">folder</i> <a href="/categories/Math/">Math</a> <i class="material-icons">label</i> <a href="/tags/Math/">Math</a>, <a href="/tags/Linear-Algebra/">Linear Algebra</a>, <a href="/tags/Data-Engineering/">Data Engineering</a><script type="text/x-mathjax-config">MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></div></div></div><div class="post-body-wrapper"><div class="post-body"><h2 id="singular-value-decomposition">Singular Value Decomposition</h2><p>Singular Value Decomposition (SVD) is widely used in many applications such as Data Reduction , Data-Driven Generation of Fourier transform (FFT), Recommendation Algorithms, Facial Recognition and also can derive other algorithms like PCA.</p><h3 id="derivation">Derivation</h3><p>We give a data matrix consisting of a bunch of column vectors <span class="math inline">\({x_k} \in \mathbb{R}^n\)</span> for <span class="math inline">\(k=\{1, 2, .., m\}\)</span> as below, where <span class="math inline">\(x_k\)</span> can be a vector summarizing the facial characteristics of a person <span class="math inline">\(k\)</span> or a vector describing what a snapshot looks like at time <span class="math inline">\(k\)</span>. <span class="math display">\[ X = \begin{bmatrix} | &amp; | &amp; ... &amp; | \\ x_1 &amp; x_2 &amp; ... &amp; x_m \\ | &amp; | &amp; ... &amp; | \\ \end{bmatrix} \]</span> And what SVD does is decompose the above data matrix into a product of three other matrices <span class="math display">\[ X = U \Sigma V^T \]</span> where left singular vectors <span class="math inline">\(U\)</span> and right singular vectors <span class="math inline">\(V\)</span> are unitary matrices, and <span class="math inline">\(\Sigma\)</span> is a diagonal matrix. Specifically, we have <span class="math display">\[ \begin{align} X &amp;= \begin{bmatrix} | &amp; | &amp; ... &amp; | \\ u_1 &amp; u_2 &amp; ... &amp; u_n \\ | &amp; | &amp; ... &amp; | \\ \end{bmatrix} \begin{bmatrix} \sigma_1 &amp; &amp; &amp; \\ &amp; \sigma_2 &amp; &amp; \\ &amp; &amp; ... &amp; \\ &amp; &amp; &amp; \sigma_m \\ &amp; &amp; \hline\huge 0 &amp; \\ &amp; &amp; &amp; \\ \end{bmatrix} \begin{bmatrix} - &amp; v_1^T &amp; - \\ - &amp; v_2^T &amp; - \\ &amp; ... &amp; \\ - &amp; v_m^T &amp; - \\ \end{bmatrix} \\ \end{align} \]</span> where unitary matrices means <span class="math inline">\(U U^T = U^T U = I_{n \times n}\)</span> and <span class="math inline">\(V V^T = V^T V = I_{m \times m}\)</span>.</p><p>The elements <span class="math inline">\(\sigma_k\)</span> in <span class="math inline">\(\Sigma\)</span> are so-called singular values, and they are ordered like <span class="math inline">\(\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_m\)</span> by importance. At the same time, the importance is also reflected in <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> where <span class="math inline">\(u_1\)</span> is more important than <span class="math inline">\(u_2\)</span> in representing <span class="math inline">\(n\)</span> rows of <span class="math inline">\(X\)</span> because of their corresponding singular values <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span>.</p><p>Intuitively, if <span class="math inline">\(x_1\)</span> represents one person's face, we can interpret <span class="math inline">\(U\)</span> as &quot;eigen&quot; faces which will be reshaped into those original faces, while the column vectors of <span class="math inline">\(V^T\)</span> along with singular values <span class="math inline">\(\sigma\)</span> serve as the reshaping factor for all <span class="math inline">\(u_1, u_2, ..., u_m\)</span> to make <span class="math inline">\(x_1, x_2, ..., x_m\)</span>.</p><p>And SVD can be done to any matrices and <span class="math inline">\(U\)</span>, <span class="math inline">\(\Sigma\)</span>, <span class="math inline">\(V\)</span> are unique and guaranteed to exist.</p><h3 id="matrix-approximation">Matrix Approximation</h3><p>Let's do introduce another form of SVD assuming <span class="math inline">\(n &gt;&gt; m\)</span> when there are a lot more measurements/observations $$ <span class="math display">\[\begin{align} X &amp;= \begin{bmatrix} | &amp; | &amp; ... &amp; | \\ u_1 &amp; u_2 &amp; ... &amp; u_n \\ | &amp; | &amp; ... &amp; | \\ \end{bmatrix} \begin{bmatrix} \sigma_1 &amp; &amp; &amp; \\ &amp; \sigma_2 &amp; &amp; \\ &amp; &amp; ... &amp; \\ &amp; &amp; &amp; \sigma_m \\ &amp; &amp; \hline\huge 0 &amp; \\ &amp; &amp; &amp; \\ \end{bmatrix} \begin{bmatrix} - &amp; v_1^T &amp; - \\ - &amp; v_2^T &amp; - \\ &amp; ... &amp; \\ - &amp; v_m^T &amp; - \\ \end{bmatrix} \\ &amp;= \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + ... + \sigma_m u_m v_m^T \\ &amp;= \hat U \hat \Sigma {\hat V} ^T\\ \end{align}\]</span> $$ This is usually called economy SVD.</p><p>Recall that SVD is expressed as decomposing <span class="math inline">\(X\)</span> into 2 sets of orthogonal bases (<span class="math inline">\(u_1, u_2,..., u_m\)</span> and <span class="math inline">\(v_1, v_2,..., v_m\)</span>) and a singular values diagonal matrix. With the economy SVD formula, we can tell SVD can also be expressed as a sum of a series of rank-1 matrices (rank-1 matrices means <span class="math inline">\(\sigma_k u_k v_k^T\)</span>). This is called the <a href="https://www2.math.uconn.edu/~leykekhman/courses/MATH3795/Lectures/Lecture_9_Linear_least_squares_SVD.pdf#page=7" target="_blank" rel="noopener">dyadic decomposition</a> of <span class="math inline">\(A\)</span>, which decomposes the matrix <span class="math inline">\(A\)</span> of rank <span class="math inline">\(r\)</span> into sum of <span class="math inline">\(r\)</span> matrices of rank 1.</p><p>Naturally, as more rank-1 matrices are summed up, the summation increasingly improves the approximation of <span class="math inline">\(X\)</span>. Hence, if we truncate <span class="math inline">\(\hat U, \hat \Sigma, \hat V\)</span> at rank <span class="math inline">\(r\)</span> and chop the summation off to only keep the first <span class="math inline">\(r\)</span> columns of <span class="math inline">\(\hat U\)</span>, a <span class="math inline">\(r \times r\)</span> submatrix of <span class="math inline">\(\hat \Sigma\)</span> and the first <span class="math inline">\(r\)</span> rows of <span class="math inline">\(\hat V^T\)</span>, then we have an rank-r matrix <span class="math display">\[ X \approx \tilde U \tilde \Sigma \tilde V^T \]</span> We can conclude <span class="math inline">\(\tilde U \tilde \Sigma \tilde V^T\)</span> is the best estimator of <span class="math inline">\(X\)</span> with rank <span class="math inline">\(r\)</span>. Specifically, this conclusion is given by <a href="https://en.wikipedia.org/wiki/Low-rank_approximation" target="_blank" rel="noopener">Eckart-Young Theorem</a> <span class="math display">\[ \underset{\tilde x \space s.t. \space rank(\tilde x) = r}{\operatorname{argmin}} || X - \tilde X||_F = \tilde U \tilde \Sigma \tilde V^T \]</span> where <span class="math inline">\(F\)</span> represents the Frobenius Norm for matrices (<span class="math inline">\(||A||_F = \sqrt{\sum_{i, j}(A_{i,j})^2}\)</span>). It guarantees the best possible matrix approximation to <span class="math inline">\(X\)</span> at rank <span class="math inline">\(r\)</span> is given by the first <span class="math inline">\(r\)</span> truncated SVD. <strong>Note: </strong>Since truncated <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are not square matrices anymore, <span class="math inline">\(\tilde U \tilde U^T = I\)</span> and <span class="math inline">\(\tilde V \tilde V^T = I\)</span> does not hold true anymore.</p><h3 id="svd-in-dominant-correlations">SVD in Dominant Correlations</h3><p>If we compute the covariance matrix of <span class="math inline">\(X\)</span>, we can get <span class="math display">\[ X^T X = V \hat \Sigma \hat U^T \hat U \hat \Sigma V^T = V \hat \Sigma^2 V^T \]</span> which follows the eigen-decomposition form of <span class="math inline">\(X^T X V = V \Sigma^2\)</span>. Therefore, the right singular vectors <span class="math inline">\(V\)</span> become the eigenvectors of the covariance matrix of column-wise matrix <span class="math inline">\(X\)</span>. And the squared singular values make up the eigenvalues matrix <span class="math inline">\(\Sigma^2\)</span> .</p><p>If we do the same thing to another covariance matrix, we have <span class="math display">\[ X X^T = \hat U \hat \Sigma V^T V \hat \Sigma \hat U^T = \hat U \hat \Sigma^2 \hat U^T \]</span> Similarly, the left singular vectors <span class="math inline">\(U\)</span> become the eigenvectors of the covariance matrix of row-wise matrix <span class="math inline">\(X^T\)</span>. This covariance matrix calculates covariance among observations. And the eigenvalues matrix is the same as above.</p><p>Something about <span class="math inline">\(\hat U\)</span> and <span class="math inline">\(\hat V\)</span></p><p>Sometimes when <span class="math inline">\(X\)</span> represents a huge dataset, one issue here is that <span class="math inline">\(\hat U\)</span> is hard to compute because <span class="math inline">\(X X^T\)</span> is a really huge matrix. But we can use the property that <span class="math inline">\(X X^T\)</span> and <span class="math inline">\(X X^T\)</span> shared the same eigenvalues matrix <span class="math inline">\(\Sigma\)</span> to compute <span class="math inline">\(\hat U = X V \hat \Sigma^{-1}\)</span>.</p><p>Unitary Transformations</p><p>Derived from <span class="math inline">\(X = \hat U \hat \Sigma V^T\)</span>, we have <span class="math inline">\(X V = \hat U \hat \Sigma\)</span> or <span class="math inline">\(\hat U^T X =\hat \Sigma V^T\)</span> which explains the unitary transformations between two eigen bases. <span class="math inline">\(m\)</span> dimensional eigen bases <span class="math inline">\(V\)</span> multiplying by <span class="math inline">\(X\)</span> are mapped into <span class="math inline">\(n\)</span> dimensional eigen bases <span class="math inline">\(\hat U\)</span>. And <span class="math inline">\(\hat U\)</span> can also be mapped into <span class="math inline">\(V\)</span>.</p><h3 id="svd-in-solving-linear-systems-of-equations">SVD in Solving Linear Systems of Equations</h3><p>Below is a super normal linear system <span class="math display">\[ A x = b \]</span> where <span class="math inline">\(A\)</span> is a known <span class="math inline">\(n \times m\)</span> matrix, <span class="math inline">\(b\)</span> is a known <span class="math inline">\(n \times 1\)</span> vector and <span class="math inline">\(x\)</span> is an unknown <span class="math inline">\(m \times 1\)</span> vector.</p><p>When <span class="math inline">\(n &lt; m\)</span>, the linear system is underdetermined as below <span class="math display">\[ \begin{bmatrix} | &amp; | &amp; ... &amp; | \\ A_1 &amp; A_2 &amp; ... &amp; A_m \\ | &amp; | &amp; ... &amp; | \\ \end{bmatrix} \begin{bmatrix} x_1\\ x_2 \\ x_3\\ x_4 \\ ... \\ x_m\\ \end{bmatrix} = \begin{bmatrix} b_1\\ b_2 \\ ... \\ b_n\\ \end{bmatrix} \]</span> and there are many solutions <span class="math inline">\(x\)</span> given <span class="math inline">\(b\)</span> because there is much more freedom among <span class="math inline">\(x\)</span> than among <span class="math inline">\(b\)</span> to uniquely determine the values of <span class="math inline">\(x\)</span>.</p><p>When <span class="math inline">\(n &gt; m\)</span>, the linear system is overdetermined as below <span class="math display">\[ \begin{bmatrix} | &amp; | &amp; ... &amp; | \\ | &amp; | &amp; ... &amp; | \\ A_1 &amp; A_2 &amp; ... &amp; A_m \\ | &amp; | &amp; ... &amp; | \\ | &amp; | &amp; ... &amp; | \\ \end{bmatrix} \begin{bmatrix} x_1\\ ... \\x_m\\\end{bmatrix} = \begin{bmatrix} b_1\\ b_2 \\ b_3 \\ ... \\b_n\\\end{bmatrix} \]</span> and there are generally zero solutions <span class="math inline">\(x\)</span> given <span class="math inline">\(b\)</span> because there is not enough freedom for <span class="math inline">\(x\)</span> to satisfy all these <span class="math inline">\(b\)</span> constrictions.</p><p>What SVD allows us to do is approximately inverts <span class="math inline">\(A\)</span> to compute so-called <strong>pseudo inverse matrix</strong> <span class="math inline">\(A^{\dagger}\)</span> to give a best <span class="math inline">\(x\)</span> as the solution to the above linear system when <span class="math inline">\(n \neq m\)</span>. Below is the solution <span class="math display">\[ x = A^{\dagger} b \]</span> where <span class="math inline">\(A^{\dagger} = V \hat \Sigma^{-1} \hat U^T\)</span> is referred to as <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" target="_blank" rel="noopener">Moore–Penrose</a> (left) pseudo inverse derived from <span class="math inline">\(A = \hat U \hat \Sigma V^T\)</span>. And this pseudo matrix <span class="math inline">\(A^{\dagger}\)</span> has these optimal properties under different circumstances as below</p><ul><li><span class="math inline">\(A^{\dagger}\)</span> has the minimum 2-norm <span class="math inline">\(min ||x||_2, s.t. Ax = b\)</span> among all possible solutions when the linear system is underdetermined.</li><li><span class="math inline">\(A^{\dagger}\)</span> minimizes the error <span class="math inline">\(min||Ax - b||_2, s.t.Ax=b\)</span> when the linear system is overdetermined, which is also the most common OLS.</li></ul><h3 id="least-squares-regression-and-svd">Least Squares Regression and SVD</h3><p>Recall that if there exists a solution to the linear system <span class="math inline">\(Ax = b\)</span>, it basically means <span class="math inline">\(b\)</span> is within the span of the columns of <span class="math inline">\(A\)</span>, considering <span class="math inline">\(Ax\)</span> simply means the linear combinations of column vectors of <span class="math inline">\(A\)</span>. Now let's look at new linear combinations obtained by SVD as below <span class="math display">\[ A \tilde x = A A^{\dagger} b = \hat U \hat \Sigma V^T V \hat \Sigma^{-1} \hat U^T b = \hat U \hat U^T b \]</span> Even though the estimate <span class="math inline">\(\tilde x\)</span> does not equal to <span class="math inline">\(b\)</span> (because truncated <span class="math inline">\(\hat U\)</span> makes <span class="math inline">\(\hat U \hat U^T \neq I\)</span>), it is still the best least square solution projecting <span class="math inline">\(b\)</span> onto the span of the columns of <span class="math inline">\(U\)</span> which is also the span of the columns of <span class="math inline">\(A\)</span>.</p><p>Now consider a one-dimensional example <span class="math inline">\(ax = b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are <span class="math inline">\(n \times 1\)</span> vector. If there is a unique solution to this equation, then <span class="math inline">\((a, b)\)</span> constitutes a straight line and <span class="math inline">\(x\)</span> is then the slope of this line. But the often case is that these <span class="math inline">\((a, b)\)</span> dots do not lie on the same line, thus making SVD useful to find a best possible line to fit these <span class="math inline">\((a, b)\)</span> dots.</p><figure><img src="SVD_LS.JPG" alt="SVD_LS"><figcaption>SVD_LS</figcaption></figure><p>As the above graph shows, the true line represents <span class="math inline">\(ax = b\)</span> while in reality there are observation errors in measuring <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and SVD finds the regression/fitted line based on theses noisy <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p><h3 id="principal-component-analysis-pca-and-svd">Principal Component Analysis (PCA) and SVD</h3><p>PCA is a <strong>projection method</strong> that uses an orthogonal transformation to convert a set of observations of <strong>possibly correlated variables</strong> into a set of values of <strong>linearly uncorrelated variables</strong> called principal components (PCs).</p><p>Intuitively, PCA converts observations of variables into a set of PCs to independently represent the original data with different importance. So we can use some of PCs to reduce data dimension while keeping the information of data as much as possible.</p><p>(The PC searching process is sequential, in each step, PCA identifies the axis that accounts for the largest amount of variance once the residuals are projected on it.)</p><h4 id="derivation-1">Derivation</h4><p>PCA's goal is to find orthogonal directions to represent the original data as much as possible, so we can naturally think of variance as a measure of data representation. If one direction carries the largest variance projected by the original data, this direction can represent the original data to the most extent. Let's first do some derivation <span class="math display">\[ S = \frac{1}{n - 1} B^T B \]</span> where <span class="math inline">\(B = X - \bar X\)</span> is the demeaned <span class="math inline">\(X\)</span> , <span class="math inline">\(\bar X\)</span> is a <span class="math inline">\(n \times 1\)</span> vector whose <span class="math inline">\(i_{th}\)</span> element is the mean of <span class="math inline">\(i_{th}\)</span> row of <span class="math inline">\(X\)</span> and dividing by <span class="math inline">\(n - 1\)</span> is a typical way to correct for the bias introduced by using the sample mean instead of the true population mean.</p><p>Now consider the projection of a vector <span class="math inline">\(x \in \mathbb R^m\)</span> onto another vector/direction <span class="math inline">\(v_i\)</span> is simply the dot product <span class="math inline">\(v_i^T x\)</span>, we can then write the variance of the data projected onto vector <span class="math inline">\(v_1\)</span> as <span class="math display">\[ \frac{1}{n - 1} v_1^T (X - \bar X)^T (X - \bar X) v_1 = v_1^T S v_1 \]</span> and when this projected variance reaches to the maximum, <span class="math inline">\(v_1 = \underset{||v_1||=1}{\operatorname{argmax}} v_1^T S v_1\)</span> finds the first principal component (Note: <span class="math inline">\(v_1\)</span> itself is not a principal component). Then we continue this process by projecting <span class="math inline">\(B\)</span> onto other directions <span class="math inline">\(v_2\)</span> vertical to <span class="math inline">\(v_1\)</span> then onto <span class="math inline">\(v_3\)</span> vertical to <span class="math inline">\(v_2\)</span>. Finally, we will find the first <span class="math inline">\(v_k\)</span> are the first <span class="math inline">\(k\)</span> eigenvectors of <span class="math inline">\(S\)</span> corresponding to the first <span class="math inline">\(k\)</span> largest eigenvalues. This follows the eigen-decomposition format <span class="math display">\[ S V = V D \]</span> where <span class="math inline">\(V\)</span> is the eigenvector matrix of covariance matrix <span class="math inline">\(S\)</span> and <span class="math inline">\(D\)</span> is the eigenvalue matrix. And if we use SVD to the data matrix <span class="math inline">\(B\)</span>, we will note the eigenvector <span class="math inline">\(V\)</span> of covariance matrix <span class="math inline">\(S\)</span> equals to the right singular vector <span class="math inline">\(V\)</span>, and the singular value is proportional to the square root of the eigenvalues of <span class="math inline">\(S\)</span>. <span class="math display">\[ B = U \Sigma V^T \\ S = \frac{1}{n - 1} B^T B = \frac{1}{n - 1} V \Sigma^T \Sigma V^T \\ S V = V \frac{\Sigma \Sigma^T}{n - 1} \]</span> So now we have several ways to compute the principal components <span class="math display">\[ T = B V = U \Sigma \]</span> where <span class="math inline">\(T\)</span> represents the principal components matrix while <span class="math inline">\(V\)</span> can be obtained using SVD to <span class="math inline">\(B\)</span> or eigen-decomposition to <span class="math inline">\(S\)</span>.</p></div></div><nav class="post-pagination"><a class="newer-posts" href="/2020/07/16/algebra/">Previous post<br>Essence of Linear Algebra </a><span class="page-number"></span> <a class="older-posts" href="/2020/04/23/kalman-filter/">Next post<br>A Mysterious Algorithm used in Apollo 11 Guidance</a></nav></div></div><div class="single-column-footer">Proudly published with Hexo<br>Theme <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> by <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a><br>&copy; 2020 <a href="http://yoursite.com">Avalon</a></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.4/dist/umd/popper.min.js" integrity="sha256-EGs9T1xMHdvM1geM8jPpoo8EZ1V1VRsmcJz8OByENLA=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.5.17/dist/vue.min.js" integrity="sha256-FtWfRI+thWlNz2sB3SJbwKx5PgMyKIVgwHCTwa3biXc=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/smooth-scroll@14.2.1/dist/smooth-scroll.polyfills.min.js" integrity="sha256-CI4Gq5E0io1Pv0xM3qPM+NUIOhbIBvC3GiN1Y4KhXpw=" crossorigin="anonymous"></script><script src="/js/journal.js?20019411"></script></body></html>