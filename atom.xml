<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Avalon</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-24T01:17:29.008Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Reese</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>A Mysterious Algorithm used in Apollo 11 Guidance</title>
    <link href="http://yoursite.com/2020/04/23/kalman-filter/"/>
    <id>http://yoursite.com/2020/04/23/kalman-filter/</id>
    <published>2020-04-24T01:05:54.000Z</published>
    <updated>2020-04-24T01:17:29.008Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Kalman-Filter"><a href="#Kalman-Filter" class="headerlink" title="Kalman Filter"></a>Kalman Filter</h2><h3 id="0-State-Space-Models"><a href="#0-State-Space-Models" class="headerlink" title="0. State Space Models"></a>0. State Space Models</h3><p>In <a href="https://en.wikipedia.org/wiki/State-space_representation#Example:_continuous-time_LTI_case" target="_blank" rel="noopener">Wikipedia</a>, a state space model is described as a set of input, output and state variables mixed with some ODE systems. I have no idea what that is with this abstract definition. State variables</p><p>can also be known as hidden states evolving through time in a way that we don’t know how but we would like to use some <strong>dynamics</strong> and <strong>observations</strong> to fit the evolution path.</p><p><strong>An Example</strong></p><p>Let’s look at a random walk example below,</p><p>Say we model some hidden states with two parts, which are its last state and some random noise. Also, we give the mapping relationship from $x_k$ to $z_k$ as below.<br>$$<br>x_k = x_{k - 1} + w_{k - 1}, w_{k} \sim N(0, Q),<br>$$</p><p>$$<br>z_k = x_k + \epsilon_{k}, \epsilon_{k} \sim N(0, R),<br>$$</p><p>where $x_k$ is the state variable and $z_k$ is the observation value, and $w_{k}$, $\epsilon_{k}$ are independent normal random noise.</p><p><img src="random_walk.png" alt="Random walk"></p><p>Basically, we have a state space model about $x_k$, and we would like to know how exactly $x_k$ will evolve in future steps. From the equations above, we can also estimate $\hat x_k$ with current $\hat x_{k - 1}$ and $z_{k-1}$.</p><p><strong>Probabilistic State Space Models</strong></p><p>But usually, a point estimate is less significant than an interval estimate. So here comes Probabilistic State Space Models.</p><p>With the same example above, we can know the distribution of $x_k$ given $x_{k-1}$and the distribution of observations $z_k$ given $x_{k}$.<br>$$<br>p(x_k | x_{k - 1}) = \frac{1}{\sqrt{2 \pi Q}}\exp \left( -\frac{1}{2Q}(x_k - x_{k - 1})^2 \right)<br>$$</p><p>$$<br>p(z_k | x_{k}) = \frac{1}{\sqrt{2 \pi R}}\exp \left( -\frac{1}{2R}(z_k - x_{k})^2 \right)<br>$$</p><p>where $p(\cdot)$ represents a probability density function.</p><p><strong>Assumptions</strong></p><p>Typically, we use $p(x_k | x_{k - 1})$ instead of $p(x_k | x_{1},x_{2},…,x_{k - 1})$ to describe the conditional distribution is due to the Markovian assumption, which is <strong>future states are independent from the past states given the present.</strong><br>$$<br>p(x_k | x_{k - 1}) = p(x_k | x_{1},x_{2},…,x_{k - 1}, z_{1},z_{2},…,z_{k - 1})<br>$$<br>Also, we can also naturally think of the relationship between states and observations. Obviously, <strong>observations only depend on its current states</strong> instead of past observations or past states.<br>$$<br>p(z_k | x_{k}) = p(z_k | x_{1},x_{2},…,x_{k}, z_{1},z_{2},…,z_{k - 1})<br>$$<br>Simpler forms of these assumptions are as below<br>$$<br>p(x_k | x_{1:k-1}, z_{1:k}) = p(x_k | x_{k - 1})<br>$$</p><p>$$<br>p(z_k | x_{1:k}, z_{1:k - 1}) = p(z_k | x_{k})<br>$$</p><h3 id="1-Bayesian-Filter"><a href="#1-Bayesian-Filter" class="headerlink" title="1. Bayesian Filter"></a>1. Bayesian Filter</h3><p>From <a href="https://en.wikipedia.org/wiki/Bayesian_inference" target="_blank" rel="noopener">Bayesian inference</a>, the probability for an event is updated as more evidence or information becomes available, so we would like to use Bayes rules to <strong>simulate</strong> the probability distribution of hidden states $x_t$ with incoming observations.<br>$$<br>\begin{align<em>}<br>p(x_k |z_{1:k}) &amp;= \frac{p(x_k, z_{1:k})}{p( z_{1:k})}\<br>&amp;= \frac{p(z_k | z_{1:k-1}, x_k) p(x_k |z_{1:k-1}) p(z_{1:k-1})}{p(z_{1:k})}\<br>&amp;= \eta p(z_k | x_k) p(x_k | z_{1:k-1}), \space where \space\space \eta = p(z_k | z_{1:k-1})\<br>&amp;= \eta p(z_k | x_k) \int{p(x_k | x_{k-1}, z_{1:k-1})p(x_{k-1} | z_{1:k-1})dx_{k-1}}\<br>&amp;= \eta p(z_k | x_k) \int{p(x_k | x_{k-1})p(x_{k-1} | z_{1:k-1})dx_{k-1}}\<br>\end{align</em>}<br>$$<br>Here comes a recursion formula if we replace $p(x_k |u_{1:k})$ with $Bel(x_k)$ meaning the posterior pdf of $x_k$ at time $k$, so we have<br>$$<br>Bel(x_k) = \eta p(z_k|x_k) \int p(x_k|x_{k-1})Bel(x_{k-1})dx_{k-1}<br>$$<br>where $p(z_k|x_k)$ and $p(x_k|x_{k-1})$ correspond exactly to the 2 hypothetical equations discussed in the random walk example, which are <strong>Observations-States Mapping</strong> and <strong>State Dynamics</strong>. Intuitively, we need state dynamics to make apriori estimates ($\int{p(x_k | x_{k-1})p(x_{k-1} | z_{1:k-1})dx_{k-1}}$) and observations to make likelihood adjustments, thus making the Bayesian rules complete. <a href="https://users.aalto.fi/~ssarkka/course_k2016/handout3.pdf#page=12" target="_blank" rel="noopener">Here</a> shows how the distribution of $x_{k}$ evolves under Bayesian rules.</p><p><strong>A New Input</strong></p><p>Now we introduce another control variable ${u_t}$ which may serve as an additional impact to $x_k$, and we can now imagine the state model has the new form like below:<br>$$<br>x_k = x_{k - 1} + u_{k} + w_{k - 1}, w_{k} \sim N(0, Q),<br>$$</p><p>$$<br>z_k = x_k + \epsilon_{k}, \epsilon_{k} \sim N(0, R),<br>$$</p><p>Also, we cast those 2 assumptions to $u_k$, then we have<br>$$<br>p(x_k | x_{1:k-1}, u_{1:k}, z_{1:k}) = p(x_k | x_{k - 1}, u_k)<br>$$</p><p>$$<br>p(z_k | x_{1:k}, u_{1:k}, z_{:k - 1}) = p(z_k | x_{k})<br>$$</p><p>The target conditional probability distribution has now become posterior distribution $p(x_k | u_{1:k}, z_{1:k})$, we would like to know how the current state behaves given the most recent series of observations and control variables.<br>$$<br>\begin{align<em>}<br>p(x_k |u_{1:k}, z_{1:k}) &amp;= \frac{p(x_k, u_{1:k}, z_{1:k})}{p(u_{1:k}, z_{1:k})}\<br>&amp;= \frac{p(z_k | u_{1:k}, z_{1:k-1}, x_k) p(x_k | u_{1:k}, z_{1:k-1}) p(u_{1:k}, z_{1:k-1})}{p(u_{1:k}, z_{1:k})}\<br>&amp;= \eta p(z_k | x_k) p(x_k | u_{1:k}, z_{1:k-1}), \space where \space\space \eta = p(z_k | u_{1:k}, z_{1:k-1})\<br>&amp;= \eta p(z_k | x_k) \int{p(x_k | x_{k-1}, u_{1:k}, z_{1:k-1})p(x_{k-1} | u_{1:k}, z_{1:k-1})dx_{k-1}}\<br>&amp;= \eta p(z_k | x_k) \int{p(x_k | x_{k-1}, u_k)p(x_{k-1} | u_{1:k-1}, z_{1:k-1})dx_{k-1}}\<br>\end{align</em>}<br>$$<br>Here comes a recursion formula if we replace $p(x_k |u_{1:k}, z_{1:k})$ with $Bel(x_k)$ meaning the posterior pdf of $x_k$ at time $k$, so we have<br>$$<br>Bel(x_k) = \eta p(z_k|x_k) \int p(x_k|x_{k-1},u_k)Bel(x_{k-1})dx_{k-1}<br>$$<br>which is all the same as the previous formula except $u_k$. Now we can separate this recursion formula into two parts<br>$$<br>\overline {Bel}{(x_k)} = \int p(x_k|x_{k-1},u_k)Bel(x_{k-1})dx_{k-1}<br>$$</p><p>$$<br>Bel(x_k) = \eta p(z_k|x_k)\overline {Bel}{(x_k)}<br>$$</p><p>state prediction and state update respectively. We can say $x_k$ is predicted with new inputs of control variables $u_k$ and updated with corresponding observations $z_k$.</p><p>Why we need a new input $u_k$? Simply speaking, we need some other factors rather than pure past states to explain the evolution of $x_k$. To be more specific, Bayesian filter is widely used in probabilistic robotics where there is always robotics control (say robots are controlled to move 2 meters ahead) and the control itself influences a system’s state (the state is the position of the robot).</p><h3 id="2-Kalman-Filter"><a href="#2-Kalman-Filter" class="headerlink" title="2. Kalman Filter"></a>2. Kalman Filter</h3><p>The recursion equation above indicates how the distribution of $x_k$ evolves through time. However we do not know what is the exact distribution of $x_{k-1}$ given a random initial distribution $x_0$. If an initial random distribution is given, then we must follow the recursion formula to integrate over all possible $x_{k-1}$ to get an estimate.</p><p>Here comes the trick of Kalman Filter-using <strong>Gaussian</strong> distributions to describe $x_k$. Because linear combinations of Gaussian distributions are still Gaussian, the states distribution evolution is easier to follow under Gaussian assumptions.</p><p><strong>Assumptions</strong></p><ol><li>State Dynamics and Observation-State Mapping are both linear models:</li></ol><p>$$<br>x_k = F_k x_{k-1} + B_ku_k + w_k<br>$$</p><p>$$<br>z_k = H_k x_{k} + \epsilon_k<br>$$</p><ol start="2"><li>State noise $w_t$ and observation noise $\epsilon_t$ both follow normal distribution:</li></ol><p>$$<br>w_{k} \sim N(0, Q_k)<br>$$</p><p>$$<br>\epsilon_{k} \sim N(0, R_k)<br>$$</p><p>Also, we introduce $P_k = Var(x_k - \hat x_{k|k - 1})$ to describe the variance of estimate error (which is the difference between actual state $x_k$ and estimate state $\hat x_{k|k-1}$).</p><p><strong>Kalman Filter Derivation</strong></p><p>We follow the <strong>state prediction</strong> and <strong>state update</strong> procedure as above to derive $x_k$ and $P_k$</p><ol><li>State Prediction (Arrive at Apriori Estimate):</li></ol><p>$$<br>\hat x_{k|k-1} = F_{k} \hat x_{k-1|k-1} + B_k u_k + w_k<br>$$</p><p>$$<br>\begin {align<em>}<br>P_{k|k-1} &amp;= Var(x_{k} - \hat x_{k|k-1}) \<br>&amp;= Var(F_k(x_{k} - \hat x_{k|k-1}) + w_k) \<br>&amp;= F_k P_k F_k^T + Q_k<br>\end {align</em>}<br>$$</p><ol start="2"><li><p>State Update (Obtain the Posteriori Estimate):<br>$$<br>K_k = P_{k|k-1}H_k^T(H_k P_{k|k-1} H_k^T + R_k)^{-1}<br>$$</p><p>$$<br>\hat x_{k|k} = \hat x_{k|k-1} + K_k (z_k - H_k \hat x_{k|k-1})<br>$$</p><p>$$<br>P_{k|k} = [I - K H_k] P_{k|k-1}<br>$$</p><p>$K$ for <strong>Kalman Gain</strong> is how Kalman Filters extends from Bayesian Filter. Let’s first look at the state update which can be interpreted as <strong>apriori estimate</strong> plus a portion of <strong>predicted observation error</strong>.</p><p>Let’s then understand the structure of $K$, where<br>$$<br>Var(z_k - \hat z_{k|k-1}) = Var(H_k(x_k - \hat x_{k|k-1}) + \epsilon_k) = H_k P_{k|k-1} H_k^T + R_k<br>$$<br>So we can express the portion of predicted observation error as<br>$$<br>K_k (z_k - \hat z_{k|k-1}) = Var[x_k - x_{k|k-1}] H^T_k Var^{-1}[z_k - \hat z_{k|k-1}] (z_k - \hat z_{k|k-1})<br>$$<br>We can interpret $K$ as a compensation term for model prediction uncertainty.</p><p>A large $K$ means there is much more noise in state prediction (a relatively larger $P_{k|k-1}$) than in observation prediction (a relatively smaller $Var(z_k - \hat z_{k|k-1})$), then we apply a large correction term to apriori estimate $\hat x_{k|k-1}$ to approximate a more accurate posteriori $x_{k|k}$. Otherwise, a small $K$ means the current prediction $\hat x_{k|k-1}$ makes some sense so that a small correction is needed.</p><p>How is $K$ derived exactly? Follow the interpretation of $K$, we can think of $K$ must be the result of minimizing the state prediction error $P_{k|k}$. So let’s express $P_{k|k}$ as below<br>$$<br>\begin {align<em>}<br>P_{k|k} &amp;= Var(x_k - \hat x_{k|k}) = Var(x_k - [\hat x_{k|k-1} + K_k (z_k - H_k \hat x_{k|k-1})]) \<br>&amp;= Var(x_k - [\hat x_{k|k-1} + K_k (H_k x_{k} + \epsilon_k - H_k \hat x_{k|k-1})]) \<br>&amp;= Var(x_k - \hat x_{k|k-1} - K_k H_k(x_{k} - \hat x_{k|k-1}) - K_k\epsilon_k ) \<br>&amp;= Var((I - K_k H_k)(x_{k} - \hat x_{k|k-1})) + Var(K_k\epsilon_k ) \<br>&amp;= (I - K_k H_k)P_{k|k-1}(I - K_k H_k)^T + K_k R_k K_k^T\<br>\end {align</em>}<br>$$<br>Now we introduce $S_k = Var(z_k - \hat z_{k|k-1}) = H_k P_{k|k-1} H_k^T + R_k$, then we have<br>$$<br>P_{k|k} = P_{k|k-1} - K_k H_k P_{k|k-1} - P_{k|k-1}H_k^T K_k^T + K_k S_k K_k^T<br>$$<br>Now minimize the trace of $P_{k|k}$ to minimize the squared error, so we have<br>$$<br>\frac{\partial tr(P_{k|k})}{\partial K_k} = -2P_{k|k-1} H_k^T + 2K_k S_k = 0<br>$$<br>then we get $K_k = P_{k|k-1} H_k^T S_k^{-1}$</p></li></ol><p><strong>Kalman Filter Bayesian Derivation</strong></p><p>We stick to the recursion formula derived by Bayesian rules:<br>$$<br>Bel(x_k) = \eta p(z_k|x_k) \int p(x_k|x_{k-1},u_k) Bel(x_{k-1})dx_{k-1}<br>$$<br>Let’s give an initial condition $Bel(x_0) = N(x_0, P_0)$, then for $k=1$ with state dynamics $x_k = F_k x_{k-1} + B_k u_k + w_k$, we know $p(x_1|x_{0},u_1) = N(F_1 x_0 + B_1 u_1, Q_1)$.</p><p>According to <a href="https://users.aalto.fi/~ssarkka/course_k2016/handout3.pdf#page=14" target="_blank" rel="noopener">convolution rules</a> in Gaussian distribution, we can get<br>$$<br>\overline {Bel}(x_1) = N(F_1 x_0 + B_1 u_1, F_1 P_0 F_1^T + Q_1)<br>$$<br>The same goes for any other $k\geq 1$ as the recursion holds true for growing $k$, which is<br>$$<br>\overline {Bel}(x_k) = N(F_k x_{k-1} + B_k u_k, F_k P_{k-1} F_k^T + Q_k)<br>$$<br>For the observation-state mapping part, we know $p(z_k|x_k) = N(H_k x_k, R_t)$, so the problem now becomes computing the distribution of 2 Gaussian variables. This will definitely lead to the same results above like<br>$$<br>\eta N(H_k x_k, R_t) N(F_k x_{k-1} + B_k u_k, F_k P_{k-1} F_k^T + Q_k) \<br>= N(F_k x_{k-1} + B_k u_k + K_k (z_k - H_k (F_k x_{k-1} + B_k u_k)), [I - K H_k] P_{k|k-1})<br>$$<br>for now I have no idea how this can be derived rigorously, but <a href="http://www.tina-vision.net/docs/memos/2003-003.pdf" target="_blank" rel="noopener">P.A. Bromiley, Products and Convolutions of Gaussian Probability Density Functions</a> introduces deriving distribution of the product of n univariate Gaussian variables.</p><p><strong>Reference</strong>:</p><p>[1] <a href="https://www.cnblogs.com/ycwang16/p/5999034.html" target="_blank" rel="noopener">细说Kalman滤波：The Kalman Filter</a></p><p>[2] <a href="https://users.aalto.fi/~ssarkka/course_k2016/handout3.pdf" target="_blank" rel="noopener">Simo Särkkä, Bayesian Filtering Equations and Kalman Filter</a></p><p>[3] <a href="http://www.cs.cmu.edu/~16831-f14/notes/F14/16831_lecture02_prayana_tdecker_humphreh.pdf" target="_blank" rel="noopener">Statistical Techniques in Robotics (16-831, F10) Lecture #02 (Thursday, August 28)Bayes Filtering</a></p><p>[4] <a href="http://people.ciirc.cvut.cz/~hlavac/TeachPresEn/55AutonomRobotics/2015-05-04ReinsteinBayes-ekf.pdf" target="_blank" rel="noopener">Michal Reinštein, From Bayes to Extended Kalman Filter</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Kalman-Filter&quot;&gt;&lt;a href=&quot;#Kalman-Filter&quot; class=&quot;headerlink&quot; title=&quot;Kalman Filter&quot;&gt;&lt;/a&gt;Kalman Filter&lt;/h2&gt;&lt;h3 id=&quot;0-State-Space-Models&quot;
      
    
    </summary>
    
      <category term="Quant" scheme="http://yoursite.com/categories/Quant/"/>
    
    
  </entry>
  
  <entry>
    <title>Canonical Component Analysis</title>
    <link href="http://yoursite.com/2019/10/20/CCA/"/>
    <id>http://yoursite.com/2019/10/20/CCA/</id>
    <published>2019-10-20T22:22:44.000Z</published>
    <updated>2020-01-06T22:55:19.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-A-Quick-Question"><a href="#0-A-Quick-Question" class="headerlink" title="0. A Quick Question"></a>0. A Quick Question</h2><p>Let’s first look at a question:<br>Here is a risk report for 2 underlying yield sensitivity (Position sensitivity)<br>{ 10yr yield sensitivity: - 100k / bp<br>{ 30yr yield sensitivity: + 100k / bp<br>So How do we hedge this risk?</p><p>The most common thing one can think of is doing oridinal leaset regression. The problem with this method is that the in-sample regression relationship will easily break out of sample.<br><strong>There should be an example snippet.</strong><br>However we have more pratical method to find the hedge ratio. And this is where CCA should be introduced.</p><h2 id="1-An-Introduction-to-CCA"><a href="#1-An-Introduction-to-CCA" class="headerlink" title="1. An Introduction to CCA"></a>1. An Introduction to CCA</h2><p>Say there is a pair of cmt rates - ($Y_t^1$, $Y_t^2$). What CCA is trying to do is to find a coitegration vector $[1, -\gamma]^T$ to let the following equation stand</p><p>$$ Y_t^1 - \gamma \cdot Y_t^2 = \mu + \epsilon_t$$</p><p>where $\epsilon_t$ is a stationary white noise process, and therefore also is a <strong>mean-reverting</strong> process.</p><p><strong>This seems like doing OLS regression. But what is the difference?</strong></p><p>What if we extend a pair to a portfolio consisting of more than n cmt rates $Y_t = [Y_t^1, Y_t^2, …, Y_t^n]^T$? The problem also focusing on find a coitegration vector $\gamma = [\gamma_1, \gamma_2, …, \gamma_n]^T$ to let the following equation stand</p><p>$$<br>\gamma^T Y_t = \mu + \epsilon_t<br>$$<br>Theoretically, we can find a cointegration vector to build a mean-reverting time series among as many assets as we want. But in practice, it’s hard to execute them all with proper prices at one time, and the execution costs are also very high.</p><p>So how do we find the cointegration vector $\gamma$? Actually, $\gamma$ is such a vector yielded by CCA that makes $\gamma^T Y_t$ a cononical variable, even it’s the least predictable variable.</p><p>There is also one thing to know - Will regression with different order yield different sets of cointegration vectors? <strong>Seems like the cointegration vectors are the same.</strong> (<strong>Formula and derivation needed.</strong>)</p><h2 id="2-Derivation-of-CCA"><a href="#2-Derivation-of-CCA" class="headerlink" title="2. Derivation of CCA"></a>2. Derivation of CCA</h2><p>Let’s look at how CCA is rigorously derived. There are 2 similar ways of performing CCA - one is introduced in <a href="http://pages.stern.nyu.edu/~dbackus/BCZ/HS/BoxTiao_canonical_Bio_77.pdf" target="_blank" rel="noopener">Box-Tiao(1977)</a> and another is introduced in <a href="https://www.elibrary.imf.org/doc/IMF001/01258-9781451950700/01258-9781451950700/Other_formats/Source_PDF/01258-9781451999181.pdf" target="_blank" rel="noopener">Chou-Ng(1994)</a>. Here we consider the former paper.</p><h3 id="2-1-Self-predictability-Measure"><a href="#2-1-Self-predictability-Measure" class="headerlink" title="2.1 Self-predictability Measure"></a>2.1 Self-predictability Measure</h3><p>Consider a $1 \times k $ vector process ${\mathbb{Z_t}}$ and let $z_t = \mathbb{Z_t} - \mu$, where $\mu$ is a convenient $1 \times k$ vector of origin which is the mean if the process is stationary. Suppose $z_t$ follows the <em>p</em>th order multiple autoregressive model</p><p>$$z_t = \hat z_{t-1}(1) + a_t$$</p><p>where</p><p>$$\hat z_{t-1}(1) = E(z_t|z_{t-1}, z_{t-2},..) = \sum_{l=1}^{p}z_{t-l} \pi_l$$</p><p>is the expectation of $z_t$ conditional on past history up to time $t-1$, the $\pi_l$ are $k \times k$ matrices, ${ a_t}$ is a sequence of independtly and normally distributed $1 \times k$ vector random shocks with mean zero and covariance matrix $\Sigma$, and $a_t$ is independent of $\hat z_{t-1}(1)$ - like the assumptions in OLS. And the $AR(p)$ model can be then represented as</p><p>$$z_t (I - \sum_{l=1}^{p}\pi_l B^l) = a_t$$</p><p>where $I$ is the identity matrix and $B$ is the backshift operator such that $B z_t = z_{t-1}$.</p><p>The process ${z_t}$ is stationary if the determinantal polynomial in $B$, $det(I - \sum_{l=1}^{p}\pi_l B^l)$ has its zeros lying outside the unit circle(<strong>?? recall AR(1) has its coef &lt; 1</strong>), and otherwise the process will be called non-stationary.</p><p>Now, let’s make the problem simpler by setting $k=1$ to narrow down to only 1 time series. Then, if the process is stationary (<strong>if not, can we still derive this?</strong> - look back on stationary’s condtions), due to $a_t$ being independent of $\hat z_{t-1}(1)$.</p><p>$$E(z_t^2) = E({\hat z_{t-1}(1)}^2) + E(a_t^2)$$</p><p>which can be also written as</p><p>$$ \sigma_z^2 = \sigma_{\hat z}^2 + \sigma_a^2$$</p><p>We can then define a quantity $\lambda$ to measure the predictability of a stationary series from its past as $\lambda = \frac{\sigma_{\hat z}^2}{\sigma_z^2} = 1 - \frac{\sigma_a^2}{\sigma_z^2}$.</p><p><strong>Note</strong>: the derivation above only applies to 1 time series. And now $z_t$ is assumed to be stationary.</p><h3 id="2-2-Intuition-of-CCA-Decomposition"><a href="#2-2-Intuition-of-CCA-Decomposition" class="headerlink" title="2.2 Intuition of CCA Decomposition"></a>2.2 Intuition of CCA Decomposition</h3><p>Now let’s consider $k$ processes $z_t$ which represent $k$ diffrent stock market indexes such as <em>Dow Jones Average</em>, <em>Standard and Poors</em> and <em>Russell Index</em>, etc., all of which exhibit dynamic growth.</p><p>It is natural to conjecture that <strong>each</strong> might be represented as some aggregate of one or more common inputs which may be nearly nonstationary (<strong>momentum</strong>), together with other stationary(<strong>mean-reverting</strong>) or white noise components.</p><p>In other words. This leads us to contemplate <strong>linear aggregates of the form $u_t = z_t m$</strong>, where $m$ is such a $k \times 1$ vector that <strong>make $u_t$ a momentum or mean-reverting time series.</strong></p><blockquote><p>Note: $z_t$ is a vector consisting of k time series. And different $m$ will yield different $u_t$. These different time series $u_t$ (whether mean-reverting or momentum) are <strong>aggregates</strong>. And these aggregates are <strong>derived</strong> time series from $z_t$. The process of getting $u_t$ from $z_t$ is called <strong>CCA decompositon</strong> even if these 2 processes are not in the same level ($u_t$ is a transformed or derived process.)</p></blockquote><p><strong>(Can $z_t$ be represented as a linear combination of $u_t$?)</strong></p><p>The aggregates $u_t$ which depend most heavily on the past, namely having large $\lambda$ (refers to $u_t$’s $\lambda$), may serve as useful composite indicators of the overall growth of the stock market (<strong>momentum</strong>). By contrast, the aggregates with $\lambda$ nearly zero may reflect stable contemporaneous relationships (<strong>mean-reverting</strong>) among the orignal indicators.</p><p>The analysis given in this paper yields $k$ ‘conancial’ components $u_t$ from laest to most predictable. Thus we may usefully decompose the k-dimensional space of the observation $z_t$ into stationary and non-stationary subspaces.</p><h3 id="2-3-Derive-Canonical-Variables"><a href="#2-3-Derive-Canonical-Variables" class="headerlink" title="2.3 Derive Canonical Variables"></a>2.3 Derive Canonical Variables</h3><p>Let $\Gamma_j(z) = E(z_t^T z_{t-j})$ be the lag $j$ autocovariance matrix of $z_t$. In the variance form, we have (<strong>the first ‘=’ needs to be proved further.</strong>)</p><p>$$ \Gamma_0(z) = \sum_{l=1}^{p} \Gamma_l(z)\pi_l + \Sigma= \Gamma_0(\hat z) + \Sigma $$</p><p>say, where $\Gamma_0(\hat z)$ is the covariance matrix of $\hat z_{t-1}(1)$. <strong>Until further notice, we shall assume that $\Sigma$ and therefore $\Gamma_0(z)$ are <a href>postive-defnite</a>.</strong></p><p>Now, consider the linear combination $u_t = z_t m$. For $u_t$, we have that $u_t = \hat u_{t-1}(1) + v_t$, where $\hat u_{t-1}(1) = \hat z_{t-1}(1) m$ and $v_t=a_t m$. The predictability of $u_t$ from its past is therefore measured by</p><p>$$ \lambda = \sigma_{\hat u}^2 \sigma_{u}^{-2} = { m \Gamma_{0}(\hat z) m^T } {m \Gamma_{0}(z) m^T }^{-1}$$</p><p>which can be represented in matrix form as</p><p>$$ \Lambda = M \Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z) M^{-1}$$</p><blockquote><p>Note: $M$ is what we are looking for. The logic is we want to find a transformed process ${u_t}$ which is generated by $M$ and the original $z_t$. And we derive that $M$ can be found using eigendecomposition of $\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)$.</p></blockquote><p>This is what we call <a href>eigendecomposition</a>, and therefore we can conclude that for the maximum predictability, $\lambda$ must be the maximum eigenvalue of $\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)$ and $m$ the corresponding eigenvector that makes $u_t$ a momentum time series. Similarly, the eigenvector that corresponds to the smallest eigenvalue will yield the least predictable combination of $z_t$. <strong>This vector is referred to as cointegration vector</strong> that is mainly used in the first question (risk hedging) mentioned at the very beginning.</p><h3 id="Conanical-Transformation"><a href="#Conanical-Transformation" class="headerlink" title="Conanical Transformation"></a>Conanical Transformation</h3><p>This chapter is a bit like PCA transformation.</p><p>Let $\lambda_1, …, \lambda_k$ be the k real eigenvalues of matrix $\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)$. Suppose $\lambda_j$ are ordered with $\lambda_1$ the smallest, and that the k corresponding <a href><strong>linearly independent eigenvectors</strong></a>, $m_1, .., m_k$ from the $k$ columns of a matrix $M$. Then, we can construct a transformed process ${ y_t}$, where</p><p>$$y_t = \hat y_{t-1} (1) + b_t$$</p><p>with</p><p>$$y_t = z_t M, b_t = a_t M, \hat y_{t-1}(1)=\sum_{l=1}^{p} y_{t-l}\pi^1_l$$</p><p>where $\pi^1_l=M^{-1} \pi_l M$</p><p>We now also have</p><p>$$\Gamma_0(y) = \Gamma_0(\hat y) + \Sigma^1$$</p><p>where $\Gamma_0(y)=M \Gamma_0(z)M^T, \Gamma_0(\hat y)=M \Gamma_0(\hat z)M^T, \Sigma^1=M \Sigma M^T$</p><p>Note:</p><ul><li>$ M \Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z) M^{-1} = \Lambda, M \Sigma \Gamma_{0}^{-1}(z) M^{-1} = I - \Lambda$ where $\Lambda$ is a $k \times k$ matrix with elements $(\lambda_1, .., \lambda_k)$</li><li>$0 \leq \lambda_j &lt; 1 \space (j=1,..,k)$</li><li>for $i \neq j, m_i \Gamma_0(z) m_j^T = m_i \Sigma m_j^T = 0$. This makes $\Gamma_0(y), \Gamma_0(\hat y), \Sigma^1$ all diagonal (Otherwise, $\Lambda$ would not be diagonal).</li></ul><p><strong>(And this can be proved by constructing an equation of the previous $\lambda$ formula, which is inspired by my friend Nick.)</strong></p><p>With this <strong>diagonal</strong> propery, we can conclude that this transformation has produced $k$ components series ${ y_{1t}, y_{2t}, .., y_{kt}}$ which are</p><ul><li>ordered from least predictable to most predictable (meaning self-predictbility)</li><li>are contemporaneously independent</li><li>have predictable components ${\hat y_{1(t-1)}(1), y_{2(t-1)}(1), .., y_{k(t-1)}(1)}$ which are also contemporaneously independent</li><li>the same goes for ${ b_{1t}, b_{2t}, .., b_{kt}}$</li></ul><p><strong>Note</strong>: The content above goes for general time series, and the content below goes for AR(1) time series.(Also $M$ above can be computed in another way.)</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>Say we have <a href="https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=yield" target="_blank" rel="noopener">Constant Maturity Treasury</a> rates (CMT rates) data ${z_t}$ from $02/01/2012$ to $06/30/2015$, a part of which is given below.</p><table><thead><tr><th>Date</th><th>6 Mo</th><th>1 Yr</th><th>2 Yr</th><th>3 Yr</th><th>5 Yr</th><th>…</th><th>30 Yr</th></tr></thead><tbody><tr><td>2015-06-30</td><td>0.11</td><td>0.28</td><td>0.64</td><td>1.01</td><td>1.63</td><td>…</td><td>3.11</td></tr><tr><td>2015-06-29</td><td>0.11</td><td>0.27</td><td>0.64</td><td>1.00</td><td>1.62</td><td>…</td><td>3.09</td></tr><tr><td>2015-06-26</td><td>0.08</td><td>0.29</td><td>0.72</td><td>1.09</td><td>1.75</td><td>…</td><td>3.25</td></tr><tr><td>2016-06-25</td><td>0.07</td><td>0.29</td><td>0.68</td><td>1.06</td><td>1.70</td><td>…</td><td>3.16</td></tr><tr><td>….</td><td>….</td><td>….</td><td>….</td><td>….</td><td>….</td><td>…</td><td>….</td></tr><tr><td>2012-02-01</td><td>0.09</td><td>0.13</td><td>0.23</td><td>0.31</td><td>0.72</td><td>…</td><td>3.01</td></tr></tbody></table><p>In this period of time, these CMT rates time series ${z_t}$ are both momentum time series. When we try to fit $AR(1)$ with these sereis with different maturities separately, the $AR(1)$ decaying parameters are around $0.95$ - $0.99$ (2.5 years is not a short term and both of these rates are contemporaneously under the same influence. So in the long term, they are both presenting similar trends).</p><p>But after we do conanical transformation to construct new time series (just as we discussed above) ${y_t}$, the most mean-reverting series has a $0.51$ decaying parameter in $AR(1)$ fitting.</p><p><img src="CCA/CCA.png" alt="CCA"></p><p>Note the constructed seires ${y_t}$ are not corresponding to the orignal cmt rates series ${z_t}$. This is similar to what is given by PCA - the first principle component is not corresponding to the first column of the orignal panel data.</p><p>Application</p><ol><li>Spot small mean-reverting portfolios.</li><li>CCA decomposition to generate detrended data.</li></ol><p>(<strong>Snippets Needed</strong>)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0-A-Quick-Question&quot;&gt;&lt;a href=&quot;#0-A-Quick-Question&quot; class=&quot;headerlink&quot; title=&quot;0. A Quick Question&quot;&gt;&lt;/a&gt;0. A Quick Question&lt;/h2&gt;&lt;p&gt;Let’
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>post</title>
    <link href="http://yoursite.com/2019/08/19/post-1/"/>
    <id>http://yoursite.com/2019/08/19/post-1/</id>
    <published>2019-08-19T15:25:41.000Z</published>
    <updated>2019-09-05T10:28:41.001Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Intro-to-FIQT-Eurodollar-Futures"><a href="#1-Intro-to-FIQT-Eurodollar-Futures" class="headerlink" title="1. Intro to FIQT - Eurodollar Futures"></a>1. Intro to FIQT - Eurodollar Futures</h2><h4 id="Convexity-Adjustment"><a href="#Convexity-Adjustment" class="headerlink" title="Convexity Adjustment"></a>Convexity Adjustment</h4><p>(A time scope grapsh is needed here…)</p><p>There exists a convexity adjustment between LIBOR forward contracts and Eurodollar futures. Hos is this derived is shown below</p><p>Assume $\delta(t, T)$ is the forward price at time $t$ which expires at time $T$, and $L(T)$ is the LIBOR rate at time $T$, $r(t)$ is the risk-free rate process. We can easily know the rational price of zero coupon bond at time $t$ which expires at time $T$, or $T$-forward numeraire is<br>$$ p(t, T) = E_t^Q[e^{-\int_{t}^{T}r(u)du}]$$</p><p>For a LIBOR forward <strong>contract</strong> that was entered at time $t$, its contact value is 0, which is<br>$$<br>\begin{equation}<br>\begin{aligned}<br>0<br>&amp; = E_t^Q \left[ e^{-\int_{t}^{T}r(u)du} [\delta(t, T) - L(T)] \right] \</p><p>&amp; = E_t^{Q}\left[ e^{-\int_{t}^{T}r(u)du} \delta(t, T) - e^{-\int_{t}^{T}r(u)du} L(T)\right]\</p><p>&amp; = E_t^{Q}\left[ e^{-\int_{t}^{T}r(u)du} \delta(t, T) \right] - E_t^{Q}\left[ e^{-\int_{t}^{T}r(u)du} \right] E_t^Q [L(T)] - cov \left( e^{-\int_{t}^{T}r(u)du}, L(T) \right)<br>\</p><p>&amp; = \delta(t, T) \cdot p(t, T) - p(t, T) \cdot E_t^Q [L(T)] - cov \left( e^{-\int_{t}^{T}r(u)du}, L(T) \right)\<br>\end{aligned}<br>\end{equation}<br>$$<br>where we apply the covriance equation $cov(X, Y) = E[X Y] - E[X] E[Y]$, so we have<br>$$\delta(t, T) = E_t^Q [L(T)] + \frac{1}{p(t,T)} cov \left( e^{-\int_{t}^{T}r(u)du}, L(T) \right)$$</p><h4 id="NPV-Effect"><a href="#NPV-Effect" class="headerlink" title="NPV Effect"></a>NPV Effect</h4><p>Consider the value of a forward contract at $t’ &gt; t$ under CSA, a contract that was entered at time $t$, so the difference in contract values on $t’$ and $t$ that exchanges hands at $t’$ is equal to<br>$$ V(t’) - V(t) = E_{t’} \left( e^{-\int_{t’}^{T}r_c(u)du} \right) (F_{CSA}(t’, T) - F_{CSA}(t, T))<br>$$<br>while the difference of futures contract will not be discounted, which is<br>$$ F(t’) - F(t) = F_{CSA}(t’, T) - F_{CSA}(t, T)<br>$$</p><p>Let’s take a look at another example in Pieterbarg(2010)</p><h2 id="2-Historical-Factor-Model"><a href="#2-Historical-Factor-Model" class="headerlink" title="2. Historical Factor Model"></a>2. Historical Factor Model</h2><h4 id="Canonical-Component-Anaylsis"><a href="#Canonical-Component-Anaylsis" class="headerlink" title="Canonical  Component Anaylsis"></a>Canonical Component Anaylsis</h4><p>Let’s first look at a question:<br>Here is a risk report for 2 underlying yield sensitivity (Position sensitivity)<br>{ 10yr yield sensitivity: - 100k / bp<br>{ 30yr yield sensitivity: + 100k / bp<br>So How do we hedge this risk?</p><p>The most common thing one can think of is oridinal leaset regression. The problem with this method is that the in-sample regression relationship will easily break out of sample.<br>** There should be an example snippet.**<br>However we have more pratical method to find the hedge ratio. And this is where CCA should be introduced.</p><p>Say there is a pair of cmt rates - ($Y_t^1$, $Y_t^2$), what we are trying to do is find a coitegration vector to let the following equation stand</p><p>$$ Y_t^1 - \gamma \cdot Y_t^2 = \mu + \epsilon_t$$</p><p>where $\epsilon_t$ is a stationary white noise process.</p><p>What if we extend a pair to a portfolio consisting of more than n cmt rates $Y_t = [Y_t^1, Y_t^2, …, Y_t^n]^T$? The problem also focusing on find a coitegration vector $\gamma = [\gamma_1, \gamma_2, …, \gamma_n]^T$ to let the following equation stand</p><p>$$ \gamma^T Y_t = \mu + \epsilon_t$$</p><p>Theoretically, we can find a cointegration vector to build a mean-reverting time series among as many assets as we want. But in practice, it’s hard to execute them all with proper prices at one time, and the execution costs are also very high.</p><p>There is also one thing to know - Will regression with different order yield different sets of cointegration vectors? <strong>Seems like the cointegration vectors are the same.</strong> (<strong>Formula and derivation needed.</strong>)</p><p>Let’s look at how CCA is rigorously derived. There are 2 similar ways of performing CCA - one is introduced in <a href="https://" target="_blank" rel="noopener">Box-Tiao(1977)</a> and another is introduced in <a href="https://" target="_blank" rel="noopener">Chou and Ng(19xx)</a>.</p><p>Consider a $1 \times k $ vector process ${\mathbb{Z_t}}$ and let $z_t = \mathbb{Z_t} - \mu$, where $\mu$ is a convenient $1 \times k$ vector of origin which is the mean if the process is stationary. Suppose $z_t$ follows the <em>p</em>th order multiple autoregressive model</p><p>$$z_t = \hat z_{t-1}(1) + a_t$$</p><p>where</p><p>$$\hat z_{t-1}(1) = E(z_t|z_{t-1}, z_{t-2},..) = \sum_{l=1}^{p}z_{t-l} \pi_l$$</p><p>is the expectation of $z_t$ conditional on past history up to time $t-1$, the $\pi_l$ are $k \times k$ matrices, ${ a_t}$ is a sequence of independtly and normally distributed $1 \times k$ vector random shocks with mean zero and covariance matrix $\Sigma$, and $a_t$ is independent of $\hat z_{t-1}(1)$ - like the assumptions in OLS. And the $AR(p)$ model can be then represented as</p><p>$$z_t (I - \sum_{l=1}^{p}\pi_l B^l) = a_t$$</p><p>where $I$ is the identity matrix and $B$ is the backshift operator such that $B z_t = z_{t-1}$.</p><p>The process ${z_t}$ is stationary if the determinantal polynomial in $B$, $det(I - \sum_{l=1}^{p}\pi_l B^l)$ has its zeros lying outside the unit circle(<strong>?? recall AR(1) has its coef &lt; 1</strong>), and otherwise the process will be called non-stationary.</p><p>Now, let’s make the problem simpler by setting $k=1$ to narrow down to only 1 time series. Then, if the process is stationary (<strong>if not, can we still derive this?</strong> - look back on stationary’s condtions), due to $a_t$ being independent of $\hat z_{t-1}(1)$.</p><p>$$E(z_t^2) = E({\hat z_{t-1}(1)}^2) + E(a_t^2)$$</p><p>which can be also written as</p><p>$$ \sigma_z^2 = \sigma_{\hat z}^2 + \sigma_a^2$$</p><p>We can then define a quantity $\lambda$ to measure the predictability of a stationary series from its past as $\lambda = \frac{\sigma_{\hat z}^2}{\sigma_z^2} = 1 - \frac{\sigma_a^2}{\sigma_z^2}$. <strong>Note</strong>: the derivation above only applies to 1 time series.</p><h4 id="Intuition"><a href="#Intuition" class="headerlink" title="Intuition"></a>Intuition</h4><p>Now let’s go back to the $k \times 1$ vector, and we can think of these $k$ processes represent $k$ diffrent stock market index such as <em>Dow Jones Average</em>, <em>Standard and Poors</em> and <em>Russell Index</em>, etc., all of which exhibit dynamic growth. It is natural to conjecture that <strong>each</strong> might be represented as some aggregate of one or more common inputs which may be nearly nonstationary (<strong>momentum</strong>), together with other stationary(<strong>mean-reverting</strong>) or white noise components.</p><p><strong>Here is the trick.</strong> It leads to a natrual contemplate linear aggregates of the form $u_t = z_t m $, where $m$ is the cononical matrix whose column are such vectors that <strong>make $u_t$ a multi time series consiting of momentum and mean-reverting time series.</strong></p><p>And we call these time series <strong>aggregates</strong>. The aggregates which depend most heavily on the past, namely having large $\lambda$ ($\lambda$ here refers to $u_t$’s $\lambda$), may serve as useful composite indicators of the overall growth of the stock market (<strong>momentum</strong>). By contrast, the aggregates with $\lambda$ nearly zero may reflect stable contemporaneous relationships (<strong>mean-reverting</strong>) among the orignal indicators.</p><p>The analysis given in this paper yields $k$ ‘conancial’ components from laest to most predictable. Thus we may usefully decompose the k-dimensional space of the observation $z_t$ into stationary and non-stationary subspaces.</p><h4 id="Derive-Canonical-Variables"><a href="#Derive-Canonical-Variables" class="headerlink" title="Derive Canonical Variables"></a>Derive Canonical Variables</h4><p>Let $\Gamma_j(z) = E(z_t^T z_{t-j})$ be the lag $j$ autocovariance matrix of $z_t$. In the variance form, we have (<strong>the first ‘=’ needs to be proved further.</strong>)</p><p>$$ \Gamma_0(z) = \sum_{l=1}^{p} \Gamma_l(z)\pi_l + \Sigma= \Gamma_0(\hat z) + \Sigma $$</p><p>say, where $\Gamma_0(\hat z)$ is the covariance matrix of $\hat z_{t-1}(1)$. <strong>Until further notice, we shall assume that $\Sigma$ and therefore $\Gamma_0(z)$ are postive-defnite.</strong></p><p>Now, consider the linear combination $u_t = z_t m$. For $u_t$, we have that $u_t = \hat u_{t-1}(1) + v_t$, where $\hat u_{t-1}(1) = \hat z_{t-1}(1) m$ and $v_t=a_t m$. The predictability of $u_t$ from its past is therefore measured by</p><p>$$ \lambda = \sigma_{\hat u}^2 \sigma_{u}^{-2} = { m \Gamma_{0}(\hat z) m^T } {m \Gamma_{0}(z) m^T }^{-1}$$</p><p>which can be represented in matrix form as</p><p>$$ \Lambda = M \Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z) M^{-1}$$</p><p>(Note: M is what we are looking for. The logic is we want to find a transformed process ${u_t}$ which is generated by $M$ and the original $z_t$. And we derive that M can be found using eigenvector decmoposition of $???$.)</p><p>This is what we call eigen vector decomposition, and therefore we can conclude that for the maximum predictability, $\lambda$ must be the eigenvalue of $\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)$ and $m$ the corresponding eigenvector that makes $u_t$ a momentum time series. Similarly, the eigenvector that corresponds to the smallest eigenvalue will yield the least predictable combination of $z_t$. <strong>This vector is referred to as coitegration vector</strong> that is mainly used in the first question (risk hedging) mentioned at the very beginning.</p><h3 id="Conanical-Transformation"><a href="#Conanical-Transformation" class="headerlink" title="Conanical Transformation"></a>Conanical Transformation</h3><p>This chapter is a bit like PCA transformation.</p><p>Let $\lambda_1, …, \lambda_k$ be the k real eigenvalues of matrix $\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)$. Supposed $\lambda_j$ are ordered with $\lambda_1$ the smallest, and that the k corresponding linearly independent eigenvectors, $m_1, .., m_k$ from the k columns of a matrix $M$. Then, we can construct a transformed process ${ y_t}$, where</p><p>$$y_t = \hat y_{t-1} (1) + b_t$$</p><p>with</p><p>$$y_t = z_t M, b_t = a_t M, \hat y_{t-1}(1)=\sum_{l=1}^{p} y_{t-l}\pi^1_l, \pi^1_l=M^{-1} \pi_l M ???$$</p><p>We now also have</p><p>$$\Gamma_0(y) = \Gamma_0(\hat y) + \Sigma^1$$</p><p>where $\Gamma_0(y)=M \Gamma_0(z)M^T, \Gamma_0(\hat y)=M \Gamma_0(\hat z)M^T, \Sigma^1=M \Sigma M^T$</p><p>Note:</p><ul><li>$ M \Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z) M^{-1} = \Lambda, M \Sigma \Gamma_{0}^{-1}(z) M^{-1} = I - \Lambda$ where $\Lambda$ is a $k \times k$ matrix with elements $(\lambda_1, .., \lambda_k)$</li><li>$0 \leq \lambda_j &lt; 1 \space (j=1,..,k)$</li><li>This makes $M \Sigma M^T, M \Gamma_0(\hat z) M^T, M \Gamma_0(z) M^T$ are all diagnoal (easy to prove).</li></ul><p>With this diagonal propery, we can conclude that this transformation has produced $k$ components series ${ y_{1t}, y_{2t}, .., y_{kt}}$ which are</p><ul><li>ordered from least predictable to most predictable</li><li>are contemporaneously independent</li><li>have predictable components ${\hat y_{1(t-1)}(1), y_{2(t-1)}(1), .., y_{k(t-1)}(1)}$ which are also contemporaneously independent</li><li>the same goes for ${ b_{1t}, b_{2t}, .., b_{kt}}$</li></ul><p><strong>Note</strong>: The content above goes for general time series, and the content below goes for AR(1) time series.(Also $M$ above can be computed in another way.)</p><p>Application</p><ol><li>Spot small mean-reverting portfolios.</li><li>CCA decomposition to generate detrended data.</li></ol><p>(<strong>Snippets Needed</strong>)</p><h2 id="3-Term-Structure-Model"><a href="#3-Term-Structure-Model" class="headerlink" title="3. Term Structure Model"></a>3. Term Structure Model</h2><h2 id="4-TSM-fitting"><a href="#4-TSM-fitting" class="headerlink" title="4. TSM fitting"></a>4. TSM fitting</h2><h2 id="5-Signal-Research-Framework"><a href="#5-Signal-Research-Framework" class="headerlink" title="5. Signal Research Framework"></a>5. Signal Research Framework</h2><h2 id="6-Livie-Eurodollar-Futures-Trading"><a href="#6-Livie-Eurodollar-Futures-Trading" class="headerlink" title="6. Livie Eurodollar Futures Trading"></a>6. Livie Eurodollar Futures Trading</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;1-Intro-to-FIQT-Eurodollar-Futures&quot;&gt;&lt;a href=&quot;#1-Intro-to-FIQT-Eurodollar-Futures&quot; class=&quot;headerlink&quot; title=&quot;1. Intro to FIQT - Eurod
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>电影的尽头是电影共产主义</title>
    <link href="http://yoursite.com/2019/07/30/post/"/>
    <id>http://yoursite.com/2019/07/30/post/</id>
    <published>2019-07-30T14:36:49.000Z</published>
    <updated>2019-08-05T19:52:18.003Z</updated>
    
    <content type="html"><![CDATA[<p>标题和副标题都是我瞎起的，只有摘要是认真写的。</p><p>起因是听观影风向标一五年戛纳那期节目时，被波米科普了戛纳颁奖的前前后后，其中谈论的马伯庸对《刺客聂隐娘》的批评言论及其引发的争论（马在未看过电影的前提下<a href="https://www.zhihu.com/question/30567872" target="_blank" rel="noopener">喷《聂》是连故事都讲不清的电影云云</a>，引发了包括但不限于[观众与导演表达权利是否对等]/[电影的故事中心论]等一系列争论），结合我后来了解到的波米，使我联想到了许多曾在我心中杂乱扎根但又从未厘清的一些树木。</p><h2 id="电影史话里的鸡和蛋"><a href="#电影史话里的鸡和蛋" class="headerlink" title="电影史话里的鸡和蛋"></a>电影史话里的鸡和蛋</h2><p>关于电影本质的讨论无休无止，且从不会达成一致，如果沿着电影的原教旨轨迹出发，从卢米埃尔兄弟发明电影之初，到卡努杜等人为电影发表《第七艺术的诞生》，尚在探索阶段的电影并没有什么本质可言——拍电影几乎就是固定场景一镜到底——其艺术属性相比于其他艺术形式相当薄弱单调，但其间并不是没有先驱探索——梅里爱摒弃一镜到底拍出一众奇观电影，埃德温·鲍特最早使用镜头变换拍出《火车大劫案》——电影也才有了剪辑和镜头语言的概念，电影的叙事使命也应运而生，再到后来，格里菲斯将前人技法发扬光大发明了蒙太奇，至此，逐渐丰富的电影表现手法使得电影的叙事技法不断翻出花样，也使得电影的艺术属性得到初步完善。</p><p>你可以说，叙事需求是电影在形式和技法探索途中无可避免的产物，可以说叙事就是一种新探索出的电影技法，也可以说正是因为电影人为了更好地讲故事才催生了电影里那些先锋形式和革新技法，但不管怎么样，电影叙事作为从属于电影艺术形式的一个单位，都不能脱离电影媒介而独立存在——无论我们在谈论哪部电影故事讲的如何如何，它都不只是那个故事的文本本身——还天然包含了电影独有的艺术属性。</p><p>而被冠之以“不会讲故事”名号的《聂隐娘》正是通过一种全新的探索性的电影语言对一段时间/历史做出呈现，它打破的不仅是常见的电影叙事手法，还有传统武侠电影的类型技法（比如镜头调度/场景设置）。</p><h2 id="为什么要探索宇宙？"><a href="#为什么要探索宇宙？" class="headerlink" title="为什么要探索宇宙？"></a>为什么要探索宇宙？</h2><p>我始终觉得，看电影不是批大字报，不是党同伐异，是有来有回，是愿者上钩，关注它作为电影在整体和个体上的不同与相同，理解电影和电影以外，达成对作品和作者的理解，而退一步讲，如果我不喜欢不明白，能不能抑制住想要玩命否定立刻站队的冲动，而换一种更有效的办法来行使作为观众的权利去完成自我表达呢？在这里我想到了<a href="http://www.lettersofnote.com/2012/08/why-explore-space.html" target="_blank" rel="noopener">NASA和赞比亚修女Mary Jucunda之间的通信</a>，修女看到美国宇航局每年的巨额花费时，她说的是“当世界上还有那么多小孩吃不饱饭时你们为什么花这么多钱在一个探索火星的项目上”，而不是“当世界上还有那么多小孩吃不饱饭时你们花这么多钱探索火星真是<del><strong>残忍无情傻逼</strong></del>”，NASA方面给出的回信很长，最后一段是这样写的</p><blockquote><p>Very fortunately though, the space age not only holds out a mirror in which we can see ourselves, it also provides us with the technologies, the challenge, the motivation, and even with the optimism to attack these tasks with confidence. What we learn in our space program, I believe, is fully supporting what Albert Schweitzer had in mind when he said: “I am looking at the future with concern, but with good hope.”</p></blockquote><p>从某种意义上，修女对于NASA的疑惑也就像观众我辈对于《聂》的不解，就像是自此处向远方投去的眺望，和想要努力看清远方的热切，这样想的话侯导又何尝不是在眺望他自己的远方呢，用另一种方式去看目不能及足的世界，靠时间的堆积到达自己眺望过的地方，用福柯的话来讲就是<strong>“对知识的热情，如果仅仅导致某种程度的学识的增长，而不是以这样或那样的方式尽可能使求知者偏离自我的话，那这种热情还有什么价值可言？在人生中：如果人们进一步观察和思考，有些时候就绝对需要提出这样的问题：了解人能否采取与自己原有的思维方式不同的方式思考，能否采取与自己原有的观察方式不同的方式感知。”</strong>，这些不同本身就是意义所在，是特瑞吉列姆和詹姆斯卡梅隆向克利斯马克横跨三十年的致敬，是诺兰在雷乃将死之前完成的《盗梦空间》（虽然诺兰说他<a href="https://artsbeat.blogs.nytimes.com/2010/06/30/a-man-and-his-dream-christopher-nolan-and-inception/?mtrref=undefined&gwh=F68694E1AE10624206DE88A261025F03&gwt=pay" target="_blank" rel="noopener">没看过《去年在马里昂巴德》</a>，这又可以延申到其他话题——有关电影与评论的创作关系，以及历史的撞车），那些看上去空无一物的东西被时间赋予了丰富的普适意义，并将继续繁茂。</p><h2 id="马克思会梦见克里奥佩特拉的鼻子吗？"><a href="#马克思会梦见克里奥佩特拉的鼻子吗？" class="headerlink" title="马克思会梦见克里奥佩特拉的鼻子吗？"></a>马克思会梦见克里奥佩特拉的鼻子吗？</h2><p>以上讨论的语境是十足电影主义的，提起电影主义自然会想起网络迷踪那期节目里的桌面电影大盘点，以及随之而来的问题——桌面电影能代表电影吗？波米的担忧在于桌面电影失掉了基本的电影语言（剪辑）会使得电影界限变得模糊（<del>吗？</del>），此类视频化电影的兴起与其迅速增长的受众群体会降低了电影制作的门槛（<del>吗？</del>），电影会逐渐被视频化（<del>吗？</del>）</p><p>有一种从历史唯物主义出发的观点是这样的——即使电影变成那样，那也不过是历史所做出的客观扬弃，是历史使得电影从最初的卢米埃尔演变成如今的桌面电影，让电影艺术产生时间上不可逆的融合，你能阻挡历史进程的客观行进吗？我不能，但我可以选择我参与历史的方式，历史它对电影完成怎样的时代融合不取决于个人，但取决于每个个体的加总，是观众和作者的加总（我实在不想从什么历史唯物的角度讨论电影的走向，那种口吻实在像拿着某版标准答案狂喷自己学生无能的中学老师一样，未免太恃己太执果索因了些）。</p><h2 id="我自己画极坐标"><a href="#我自己画极坐标" class="headerlink" title="我自己画极坐标"></a>我自己画极坐标</h2><p>说到这里，又可以延申出，观众和作者在电影融合途中的角色，我想这里应该有一个作品意识（观看意识/创作意识）的坐标轴，观众的最左是被投喂什么就吃什么（院线电影观众），最右则是不甘心做他人的茧（艺术馆观众），而导演的最左是唯观众与市场论者（商业片导演），最右则是绝对意义上的电影艺术探索者（艺术片导演）（我能立刻想到的就是阿巴斯，也大概有些理解为什么戈达尔说电影始于格里菲斯止于阿巴斯，因为无论是前者还是后者，他们对电影艺术可能性的探索都极具开创性，而阿巴斯又更纯粹一些，以至于让人相信无论是他的作品还是他对电影技作可能性的挖掘都饱满丰富，生生长流，当然这也与戈达尔本人对电影的实践做法相似），毫无疑问诺兰应该处于这个坐标的中间——似乎没有导演比他更懂大多数观众了，也似乎没有人比他在商业片类型化的创新上更娴熟了，但他能算作一个纯粹的作者导演吗？写到这里，也发现这个坐标轴只是狭隘一隅，除了时间维度的扩展（这里又可以延申到时间轴上的大起大落型选手），更应该加入新的维度。</p><h4 id="这简直是一个无穷无尽的DFS啊！"><a href="#这简直是一个无穷无尽的DFS啊！" class="headerlink" title="这简直是一个无穷无尽的DFS啊！"></a>这简直是一个无穷无尽的DFS啊！</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;标题和副标题都是我瞎起的，只有摘要是认真写的。&lt;/p&gt;&lt;p&gt;起因是听观影风向标一五年戛纳那期节目时，被波米科普了戛纳颁奖的前前后后，其中谈论的马伯庸对《刺客聂隐娘》的批评言论及其引发的争论（马在未看过电影的前提下&lt;a href=&quot;https://www.zhihu.com/
      
    
    </summary>
    
      <category term="物质现实复原" scheme="http://yoursite.com/categories/%E7%89%A9%E8%B4%A8%E7%8E%B0%E5%AE%9E%E5%A4%8D%E5%8E%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>Class bulti-in methods</title>
    <link href="http://yoursite.com/2019/07/13/test/"/>
    <id>http://yoursite.com/2019/07/13/test/</id>
    <published>2019-07-13T20:25:16.000Z</published>
    <updated>2019-07-30T14:32:48.296Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-getattribute-method"><a href="#1-getattribute-method" class="headerlink" title="1. __getattribute__ method"></a>1. __getattribute__ method</h3><p>This method usually implements a class’s getter to <strong>get</strong> whatever you request (<strong>attributes and methods</strong>) from an object/a class.<br>I don’t know for sure the difference with __getattr__</p><p>A common way of implementing this method is as follows:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">class</span> <span class="token class-name">test</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lo<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> hi<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>array <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span>lo<span class="token punctuation">,</span> high<span class="token operator">=</span>hi<span class="token punctuation">,</span> size<span class="token operator">=</span>size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getattribute__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">)</span><span class="token punctuation">:</span>        builtin_members <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'count'</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> name <span class="token keyword">in</span> builtin_members<span class="token punctuation">:</span>             <span class="token keyword">return</span> super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__getattribute__<span class="token punctuation">(</span>name<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">count</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>size<span class="token punctuation">(</span>self<span class="token punctuation">.</span>array<span class="token punctuation">)</span></code></pre><p><strong>Question</strong>: What is the following codes’ output?</p><pre class=" language-python"><code class="language-python">t <span class="token operator">=</span> test<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>The answer is 1.</p><p>And what about the results when we change the test’s structure to this:</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">class</span> <span class="token class-name">test</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lo<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> hi<span class="token operator">=</span><span class="token number">20</span><span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>array <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span>lo<span class="token punctuation">,</span> high<span class="token operator">=</span>hi<span class="token punctuation">,</span> size<span class="token operator">=</span>size<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getattribute__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">)</span><span class="token punctuation">:</span>        builtin_members <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'count'</span><span class="token punctuation">]</span>        <span class="token keyword">if</span> name <span class="token keyword">in</span> builtin_members<span class="token punctuation">:</span>             <span class="token keyword">return</span> super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__getattribute__<span class="token punctuation">(</span>name<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> object<span class="token punctuation">.</span>__getattribute__<span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">count</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> np<span class="token punctuation">.</span>size<span class="token punctuation">(</span>self<span class="token punctuation">.</span>array<span class="token punctuation">)</span></code></pre><p>The answer is 10.</p><p>And the reason is</p><ol><li><code>test</code> inherits from <code>object</code>, so <code>super().__getattribute__(name)</code> = <code>object.__getattribute__(self, name)</code></li><li>the first class doesn’t implement the case when <strong>non-builtin members</strong> are retrived, so it automatically returns <code>None</code> whose size is 1.</li></ol><p>Sometimes, there are many other complex implementation of an attribute getter.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-getattribute-method&quot;&gt;&lt;a href=&quot;#1-getattribute-method&quot; class=&quot;headerlink&quot; title=&quot;1. __getattribute__ method&quot;&gt;&lt;/a&gt;1. __getattribute_
      
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
      <category term="Magic Methods" scheme="http://yoursite.com/categories/Python/Magic-Methods/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2019/06/24/hello-world/"/>
    <id>http://yoursite.com/2019/06/24/hello-world/</id>
    <published>2019-06-25T01:51:47.291Z</published>
    <updated>2019-06-25T01:51:47.291Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class=" language-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash"><code class="language-bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
