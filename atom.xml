<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Avalon</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-08-03T16:40:36.387Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Reese</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Namespaces and Scope in Python</title>
    <link href="http://yoursite.com/2020/07/27/namespaces/"/>
    <id>http://yoursite.com/2020/07/27/namespaces/</id>
    <published>2020-07-27T15:06:44.000Z</published>
    <updated>2020-08-03T16:40:36.387Z</updated>
    
    <content type="html"><![CDATA[<h3 id="an-example">An Example</h3><p>Let's first look at an example and see what the output should be when executed.</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> <span class="dv">10</span></a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="op">&gt;&gt;&gt;</span> <span class="kw">def</span> foo():</a><a class="sourceLine" id="cb1-3" data-line-number="3">...     <span class="bu">print</span>(x)</a><a class="sourceLine" id="cb1-4" data-line-number="4">...     x <span class="op">+=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb1-5" data-line-number="5"><span class="op">&gt;&gt;&gt;</span> foo()</a></code></pre></div><p>The answer is as below</p><div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> foo()</a><a class="sourceLine" id="cb2-2" data-line-number="2">Traceback (most recent call last):</a><a class="sourceLine" id="cb2-3" data-line-number="3">  ...</a><a class="sourceLine" id="cb2-4" data-line-number="4"><span class="pp">UnboundLocalError</span>: local variable <span class="st">&#39;x&#39;</span> referenced before assignment</a></code></pre></div><p>This is where namespaces and scopes are used in Python. Before we find out these concepts, let's first answer what names are in Python. A name in Python is just a way to access an object.</p><div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> a <span class="op">=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb3-2" data-line-number="2"><span class="op">&gt;&gt;&gt;</span> b <span class="op">=</span> <span class="kw">lambda</span> x: x</a><a class="sourceLine" id="cb3-3" data-line-number="3"><span class="op">&gt;&gt;&gt;</span> <span class="kw">class</span> C:</a><a class="sourceLine" id="cb3-4" data-line-number="4">...     <span class="cf">pass</span></a><a class="sourceLine" id="cb3-5" data-line-number="5"><span class="op">&gt;&gt;&gt;</span> D <span class="op">=</span> C()</a></code></pre></div><p>In the snippet above, <code>a</code>, <code>b</code>, <code>C</code>, <code>D</code> are both names and we can see that a name can reference an integer, a function, a class or an object. Then what are namespaces in Python?</p><h3 id="namespaces-and-scopes">Namespaces and Scopes</h3><p>Simply speaking, a namespace is a system to control names in a program. There may be multiple names in one program and multilayers of namespaces ensure that these names won’t lead to any conflict. Generally, names with a shared name in different namespaces but reference different objects. Let's look at an example below to get the picture.</p><div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> <span class="kw">def</span> x():</a><a class="sourceLine" id="cb4-2" data-line-number="2">...     x <span class="op">=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb4-3" data-line-number="3">...     <span class="kw">def</span> foo():</a><a class="sourceLine" id="cb4-4" data-line-number="4">...         x <span class="op">=</span> <span class="dv">2</span></a></code></pre></div><p>As we can see, there are 3 <code>x</code> in the above snippet, while <code>x</code> in the first line references a function and <code>x</code> in the second line references <code>1</code> and <code>x</code> in the fourth line references <code>2</code>. Most importantly, the value of <code>x</code> varies when it is used in different namespaces. In this case, the outermost <code>x</code> is defined in the <strong>global</strong> namespace and <code>x</code> in the middle is defined in the <strong>enclosed</strong> namespace while the innermost <code>x</code> is defined in the <strong>local</strong> namespace. Specifically,</p><ul><li><p>A local namespace covers local names within a function or a class. Python creates this namespace for every function called in a program. It remains active until the function returns.</p></li><li>An enclosed namespace typically arises in higher order functions and it covers the nested function and other names. Python creates this namespace for every higher order function. It also remains active until the function returns.</li><li>A global namespace covers the names from various imported modules and other names. Python creates this namespace for every module included in a program. It will last until the program ends.</li><li><p>A built-in namespace covers all built-in functions and built-in exception names like <code>print</code>, <code>len</code>, <code>ValueError</code> and so on. Python creates it as the interpreter starts and keeps it until you exit.</p></li></ul><p>Correspondingly, scopes determine which namespace a name belongs to and ensure that names can be used without any prefix. This guideline is called the <strong>LEGB</strong> rule illustrated as below.</p><h3 id="the-legb-rule">The LEGB Rule</h3><figure><img src="scopes.png" alt="scope"><figcaption>scope</figcaption></figure><p>This picture explains the order in which these namespaces are to be searched for scope resolution. Specifically, if a variable is not defined in the local scope, then it will be searched in outer scopes to check if there is such a variable.</p><div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> <span class="dv">10</span></a><a class="sourceLine" id="cb5-2" data-line-number="2"><span class="op">&gt;&gt;&gt;</span> <span class="kw">def</span> foo():</a><a class="sourceLine" id="cb5-3" data-line-number="3">...     <span class="bu">print</span>(x)</a><a class="sourceLine" id="cb5-4" data-line-number="4">...     x <span class="op">+=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb5-5" data-line-number="5"><span class="op">&gt;&gt;&gt;</span> foo()</a><a class="sourceLine" id="cb5-6" data-line-number="6">Traceback (most recent call last):</a><a class="sourceLine" id="cb5-7" data-line-number="7">  ...</a><a class="sourceLine" id="cb5-8" data-line-number="8"><span class="pp">UnboundLocalError</span>: local variable <span class="st">&#39;x&#39;</span> referenced before assignment</a></code></pre></div><p>Let's get back to the above example. There is a <code>x</code> variable in the global namespace but the assignment <code>x+=1</code> declares a variable in the local namespace and assign a value recursively. Hence, <code>print(x)</code> first finds and calls on a local variable without any reference and raises an exception. Even <code>print(x)</code> is executed before the assignment, there is still an error about how <code>x</code> should be properly used. This is because an assignment to a variable in a function makes that variable local to that function . Even <code>x += 1</code> changes to <code>x = 1</code>, there will be the same error. The compiler precompiles everything and assign each variable with its namespace.</p><h3 id="the-implementation-in-python">The Implementation in Python</h3><p>To amend the above snippet, we can use the keyword <code>global</code> or <code>nonlocal</code>. <code>global</code> is used to assign a value to a global object within a local namespace while <code>nonlocal</code> is used to assign a value to a object in a higher order function.</p><div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> x <span class="op">=</span> <span class="dv">10</span></a><a class="sourceLine" id="cb6-2" data-line-number="2"><span class="op">&gt;&gt;&gt;</span> <span class="kw">def</span> foo():</a><a class="sourceLine" id="cb6-3" data-line-number="3">...     <span class="kw">global</span> x</a><a class="sourceLine" id="cb6-4" data-line-number="4">...     <span class="bu">print</span>(x)</a><a class="sourceLine" id="cb6-5" data-line-number="5">...     x <span class="op">+=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb6-6" data-line-number="6"><span class="op">&gt;&gt;&gt;</span> foo()</a><a class="sourceLine" id="cb6-7" data-line-number="7"><span class="dv">10</span></a><a class="sourceLine" id="cb6-8" data-line-number="8"><span class="op">&gt;&gt;&gt;</span> <span class="bu">print</span>(x)</a><a class="sourceLine" id="cb6-9" data-line-number="9"><span class="dv">11</span></a></code></pre></div><div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> <span class="kw">def</span> foo():</a><a class="sourceLine" id="cb7-2" data-line-number="2">...     x <span class="op">=</span> <span class="dv">10</span></a><a class="sourceLine" id="cb7-3" data-line-number="3">...     <span class="kw">def</span> foo2():</a><a class="sourceLine" id="cb7-4" data-line-number="4">...         <span class="kw">nonlocal</span> x</a><a class="sourceLine" id="cb7-5" data-line-number="5">...         <span class="bu">print</span>(x)</a><a class="sourceLine" id="cb7-6" data-line-number="6">...         x <span class="op">+=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb7-7" data-line-number="7">...     foo2(x)</a><a class="sourceLine" id="cb7-8" data-line-number="8">...     <span class="bu">print</span>(x)</a><a class="sourceLine" id="cb7-9" data-line-number="9"><span class="op">&gt;&gt;&gt;</span> foo()</a><a class="sourceLine" id="cb7-10" data-line-number="10"><span class="dv">10</span></a><a class="sourceLine" id="cb7-11" data-line-number="11"><span class="dv">11</span></a></code></pre></div><p>In Python, namespaces are implemented in the form of dictionaries. It maintains a name-to-object mapping where names act as keys and objects as values. And we can use built-in functions <code>globals</code> and <code>locals</code> to see these mappings.</p><div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1">a <span class="op">=</span> <span class="dv">10</span></a><a class="sourceLine" id="cb8-2" data-line-number="2"><span class="kw">def</span> function():</a><a class="sourceLine" id="cb8-3" data-line-number="3">    a <span class="op">=</span> <span class="dv">11</span></a><a class="sourceLine" id="cb8-4" data-line-number="4">    <span class="bu">print</span>(<span class="bu">locals</span>())</a><a class="sourceLine" id="cb8-5" data-line-number="5">    <span class="kw">def</span> foo():</a><a class="sourceLine" id="cb8-6" data-line-number="6">        a <span class="op">=</span> <span class="dv">12</span></a><a class="sourceLine" id="cb8-7" data-line-number="7">        <span class="bu">print</span>(<span class="bu">locals</span>())</a><a class="sourceLine" id="cb8-8" data-line-number="8">    <span class="cf">return</span> foo</a><a class="sourceLine" id="cb8-9" data-line-number="9"></a><a class="sourceLine" id="cb8-10" data-line-number="10"><span class="bu">print</span>(<span class="bu">globals</span>())</a><a class="sourceLine" id="cb8-11" data-line-number="11"><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&#39;__main__&#39;</span>:</a><a class="sourceLine" id="cb8-12" data-line-number="12">    function()()</a></code></pre></div><p>And we can get the below output if we run the above snippet.</p><div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1">{<span class="st">&#39;__name__&#39;</span>: <span class="st">&#39;__main__&#39;</span>, <span class="st">&#39;__doc__&#39;</span>: <span class="va">None</span>, <span class="st">&#39;__package__&#39;</span>: <span class="va">None</span>, <span class="st">&#39;__loader__&#39;</span>: ..., <span class="st">&#39;__spec__&#39;</span>: <span class="va">None</span>, <span class="st">&#39;__annotations__&#39;</span>: {}, <span class="st">&#39;__builtins__&#39;</span>: <span class="op">&lt;</span>module <span class="st">&#39;builtins&#39;</span> (built<span class="op">-</span><span class="kw">in</span>)<span class="op">&gt;</span>, <span class="st">&#39;__file__&#39;</span>: <span class="st">&#39;...&#39;</span>, <span class="st">&#39;__cached__&#39;</span>: <span class="va">None</span>, <span class="st">&#39;a&#39;</span>: <span class="dv">10</span>, <span class="st">&#39;function&#39;</span>: <span class="op">&lt;</span>function function at <span class="bn">0x000001D740ED64C8</span><span class="op">&gt;</span>}</a><a class="sourceLine" id="cb9-2" data-line-number="2">{<span class="st">&#39;a&#39;</span>: <span class="dv">11</span>}</a><a class="sourceLine" id="cb9-3" data-line-number="3">{<span class="st">&#39;a&#39;</span>: <span class="dv">12</span>}</a></code></pre></div><p>These 3 dictionaries represent 3 different namespaces where the key of <code>'a'</code> in different namespaces has different values. Something special about <code>globals</code> is that it is not read-only meaning we can add a key-value pair or update the existing key-value pair to the dictionary like</p><div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1"><span class="bu">globals</span>()[<span class="st">&#39;a&#39;</span>] <span class="op">=</span> <span class="dv">13</span></a><a class="sourceLine" id="cb10-2" data-line-number="2"><span class="bu">print</span>(<span class="st">&#39;a =&#39;</span>, a) <span class="co"># The output will be &#39;a = 13&#39;.</span></a></code></pre></div><h3 id="class-and-instance-namespaces">Class and Instance Namespaces</h3><p>Things are a bit different when it comes to classes and instances. Unlike what is covered above, accessing an instance attribute or a class attribute requires a prefix typically with the keyword <code>self</code>. For example, what is the expected output when we run the snippet as below?</p><div class="sourceCode" id="cb11"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">class</span> A:</a><a class="sourceLine" id="cb11-2" data-line-number="2">    a <span class="op">=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb11-3" data-line-number="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, val):</a><a class="sourceLine" id="cb11-4" data-line-number="4">        <span class="bu">print</span>(a)</a><a class="sourceLine" id="cb11-5" data-line-number="5">        <span class="va">self</span>.a <span class="op">=</span> val</a><a class="sourceLine" id="cb11-6" data-line-number="6">        <span class="bu">print</span>(a)</a><a class="sourceLine" id="cb11-7" data-line-number="7">_ <span class="op">=</span> A(<span class="dv">2</span>)</a></code></pre></div><p>The answer is that it will comes with a <code>NameError: name 'a' is not defined</code> in line 4. Because of the scope resolution, <code>a</code> is first searched in the constructor's local namespace and then searched in the global namespace while it doesn't access any class attribute at all. Another great example is</p><div class="sourceCode" id="cb12"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb12-1" data-line-number="1"><span class="kw">class</span> A:</a><a class="sourceLine" id="cb12-2" data-line-number="2">    a <span class="op">=</span> <span class="dv">10</span></a><a class="sourceLine" id="cb12-3" data-line-number="3">    b <span class="op">=</span> <span class="bu">list</span>(a <span class="op">+</span> i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>))</a></code></pre></div><p>There will be also a <code>NameError: name 'a' is not defined</code> in line 3 since what is inside the list is a generator expression. In Python, generator expressions are implemented in a function scope. The scope of names defined in a class block is limited to the class block; it does not extend to the code blocks of methods - this includes generator expressions in list comprehensions.</p><p>But if we change <code>print(a)</code> into <code>print(self.a)</code> in line 4 and line 6, then we will get <code>1</code> and <code>2</code> as the output.</p><div class="sourceCode" id="cb13"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw">class</span> A:</a><a class="sourceLine" id="cb13-2" data-line-number="2">    a <span class="op">=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb13-3" data-line-number="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, val):</a><a class="sourceLine" id="cb13-4" data-line-number="4">        <span class="bu">print</span>(<span class="va">self</span>.a)</a><a class="sourceLine" id="cb13-5" data-line-number="5">        <span class="va">self</span>.a <span class="op">=</span> val</a><a class="sourceLine" id="cb13-6" data-line-number="6">        <span class="bu">print</span>(<span class="va">self</span>.a)</a><a class="sourceLine" id="cb13-7" data-line-number="7">_ <span class="op">=</span> A(<span class="dv">2</span>)</a></code></pre></div><p>The first <code>self.a</code> in line 4 references the class variable defined in line 2 while the second <code>self.a</code> in line 6 references the instance variable defined in line 5. If we delete the second line and there is no such class variable,</p><div class="sourceCode" id="cb14"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb14-1" data-line-number="1"><span class="kw">class</span> A:</a><a class="sourceLine" id="cb14-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, val):</a><a class="sourceLine" id="cb14-3" data-line-number="3">        <span class="bu">print</span>(<span class="va">self</span>.a)</a><a class="sourceLine" id="cb14-4" data-line-number="4">        <span class="va">self</span>.a <span class="op">=</span> val</a><a class="sourceLine" id="cb14-5" data-line-number="5">_ <span class="op">=</span> A(<span class="dv">2</span>)</a></code></pre></div><p>then the search will proceed to look in its base class which is the <code>Object</code> class to see if there is such a variable called <code>a</code>.</p><p>Generally speaking,</p><ol type="1"><li>if the same attribute name occurs in both an instance and in its corresponding class, then attribute lookup prioritizes the instance.</li><li><p>If the attribute is not found in the instance attributes, then the search proceeds to look in the class attributes.</p></li><li>If the attribute is not found in the class, then the search proceeds to look in the base class.</li><li><p>If this class has multiple parent classes, then the search follows a Method Resolution Order(<a href="https://www.python.org/download/releases/2.3/mro/" target="_blank" rel="noopener">MRO</a>) to continue. This can be simply thought of as a depth-first, left-to-right and not searching twice in the same class procedure.</p></li></ol><p>Here is an official <a href="https://docs.python.org/3/reference/executionmodel.html?highlight=naming%20binding#naming-and-binding" target="_blank" rel="noopener">documentation</a> explaining the resolution of names just for reference.</p><h3 id="appendix-classmethod-and-staticmethod">Appendix: <code>@classmethod</code> and <code>@staticmethod</code></h3><p>Technically, this is not the content of namespaces and scopes in Python, but it is kind of similar to accessing method attributes from an instance level and a class level.</p><div class="sourceCode" id="cb15"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb15-1" data-line-number="1"><span class="kw">class</span> A:</a><a class="sourceLine" id="cb15-2" data-line-number="2">    <span class="kw">def</span> foo(arg1, arg2):</a><a class="sourceLine" id="cb15-3" data-line-number="3">        <span class="bu">print</span>(arg1, arg2)</a></code></pre></div><p>Can we call <code>foo</code> method from the class level and from the instance level?</p><div class="sourceCode" id="cb16"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb16-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> A.foo(<span class="dv">1</span>, <span class="dv">2</span>) </a><a class="sourceLine" id="cb16-2" data-line-number="2"><span class="dv">1</span> <span class="dv">2</span></a><a class="sourceLine" id="cb16-3" data-line-number="3"><span class="op">&gt;&gt;&gt;</span> A().foo(<span class="dv">2</span>)  </a><a class="sourceLine" id="cb16-4" data-line-number="4"><span class="op">&lt;</span>__main__.A <span class="bu">object</span> at <span class="bn">0x000001F51417C4C8</span><span class="op">&gt;</span> <span class="dv">2</span></a></code></pre></div><p>If <code>foo</code> is called as an instance method, then there is only 1 parameter remaining since <code>arg1</code> is treated as the object. That is to say, <code>A().foo(2)</code> is equivalent to <code>A.foo(A(), 2)</code>. But if there is a function without any argument, then we cannot call this function from an instance level.</p><div class="sourceCode" id="cb17"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb17-1" data-line-number="1"><span class="kw">class</span> A:</a><a class="sourceLine" id="cb17-2" data-line-number="2">    <span class="kw">def</span> foo():</a><a class="sourceLine" id="cb17-3" data-line-number="3">        <span class="bu">print</span>(<span class="st">&quot;Unbound Methods.&quot;</span>)</a></code></pre></div><p>Instances of <code>A</code> cannot call <code>foo</code> because <code>foo</code> is an unbound method (while unbound methods are removed in Python 3). Methods that do not have an instance of the class as the first argument are known as unbound methods. Here comes the <code>@staticmethod</code> decorator.</p><div class="sourceCode" id="cb18"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb18-1" data-line-number="1"><span class="kw">class</span> A:</a><a class="sourceLine" id="cb18-2" data-line-number="2">    <span class="at">@staticmethod</span> </a><a class="sourceLine" id="cb18-3" data-line-number="3">    <span class="kw">def</span> foo():</a><a class="sourceLine" id="cb18-4" data-line-number="4">        <span class="bu">print</span>(<span class="st">&quot;Static Methods.&quot;</span>)</a></code></pre></div><p>For now, <code>@staticmethod</code> makes <code>foo</code> bound to instances of <code>A</code> and we will get consistent results if we call <code>foo</code> either from a class or from an instance. Actually, <code>@staticmthod</code> is to ensure the consistency and to make instances work smoothly with this method (removing <code>@staticmethod</code> doesn't influence calling <code>foo</code> from a class).</p><div class="sourceCode" id="cb19"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb19-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> A.foo()</a><a class="sourceLine" id="cb19-2" data-line-number="2">Static Methods.</a><a class="sourceLine" id="cb19-3" data-line-number="3"><span class="op">&gt;&gt;&gt;</span> A().foo()</a><a class="sourceLine" id="cb19-4" data-line-number="4">Static Methods.</a></code></pre></div><p>Static methods do not require instance creation and are typically called from a class level. They are faster and usually implemented as utility functions. This is similar to where static methods are used in other languages like JAVA.</p><p>Where are <code>@classmethod</code> used in Python? Let's look at an example</p><div class="sourceCode" id="cb20"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb20-1" data-line-number="1"><span class="kw">class</span> Date:</a><a class="sourceLine" id="cb20-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, day<span class="op">=</span><span class="dv">1</span>, month<span class="op">=</span><span class="dv">1</span>, year<span class="op">=</span><span class="dv">1970</span>):</a><a class="sourceLine" id="cb20-3" data-line-number="3">        <span class="va">self</span>.day <span class="op">=</span> day</a><a class="sourceLine" id="cb20-4" data-line-number="4">        <span class="va">self</span>.month <span class="op">=</span> month</a><a class="sourceLine" id="cb20-5" data-line-number="5">        <span class="va">self</span>.year <span class="op">=</span> year</a><a class="sourceLine" id="cb20-6" data-line-number="6">    </a><a class="sourceLine" id="cb20-7" data-line-number="7">    <span class="at">@classmethod</span></a><a class="sourceLine" id="cb20-8" data-line-number="8">    <span class="kw">def</span> fromString(cls, dateStr):</a><a class="sourceLine" id="cb20-9" data-line-number="9">        day, month, year <span class="op">=</span> dateStr.split(<span class="st">&#39;-&#39;</span>)</a><a class="sourceLine" id="cb20-10" data-line-number="10">        <span class="cf">return</span> cls(day, month, year)</a></code></pre></div><p>As we can see, the above method decorated by <code>classmethod</code> takes a date in string and returns an instance of <code>Date</code>. That's where <code>@classmethod</code> decorators are mostly used. Typically, class methods take <code>cls</code> as the first argument mostly to return a new object. There are some examples in Python like</p><div class="sourceCode" id="cb21"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb21-1" data-line-number="1"><span class="op">&gt;&gt;&gt;</span> <span class="bu">dict</span>.fromkeys(<span class="st">&quot;ab&quot;</span>)</a><a class="sourceLine" id="cb21-2" data-line-number="2">{<span class="st">&#39;a&#39;</span>: <span class="va">None</span>, <span class="st">&#39;b&#39;</span>: <span class="va">None</span>}</a></code></pre></div><p>When are static methods and class methods to be introduced? When the method only depends on its parameters while the <strong>instance state has no effect</strong> on the method behavior but it's something <strong>relative</strong> to the class. Then the method can be implemented as static. If it doesn't have anything to do with the class, then it's better to implement it as a standalone function. As for class methods, <code>cls</code> must be used in the implementation otherwise static methods or standalone functions are better.</p><p>Here is a <a href="https://www.tutorialspoint.com/class-method-vs-static-method-in-python" target="_blank" rel="noopener">table</a> displaying the <strong>difference between static methods and class methods</strong></p><table><colgroup><col style="width:40%"><col style="width:59%"></colgroup><thead><tr class="header"><th>Class Methods</th><th>Static Methods</th></tr></thead><tbody><tr class="odd"><td>Take <code>cls</code> as first argument.</td><td>Can go without any parameter.</td></tr><tr class="even"><td>Can access and modify the class state.</td><td>Cannot access or modify the class or the instance state.</td></tr><tr class="odd"><td>Mostly implemented as factory methods.</td><td>Mostly implemented as utility methods.</td></tr></tbody></table><h3 id="appendix-2-single-and-double-underscores">Appendix 2: Single and Double underscores</h3><p><strong>Single Underscore</strong></p><p>Names, in a class, with a leading underscore like <code>_foo</code> are simply to indicate that the attribute or method is intended to be private even though there is nothing really private in Python. From <a href="https://www.python.org/dev/peps/pep-0008/" target="_blank" rel="noopener">PEP-8</a></p><blockquote><p><code>_single_leading_underscore</code>: weak &quot;internal use&quot; indicator. E.g. <code>from M import *</code> does not import objects whose names start with an underscore.</p><p><code>single_trailing_underscore_</code>: used by convention to avoid conflicts with Python keyword, e.g.</p><blockquote><p>def __init__(self, class_='class')</p></blockquote></blockquote><p><strong>Double Underscore (Name Mangling)</strong></p><blockquote><p><code>__double_leading_underscore</code>: when naming a class attribute, invokes name mangling (inside class FooBar, <code>__boo</code> becomes <code>_FooBar__boo</code>.</p></blockquote><p>For example,</p><div class="sourceCode" id="cb22"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb22-1" data-line-number="1"><span class="kw">class</span> A:</a><a class="sourceLine" id="cb22-2" data-line-number="2">    <span class="kw">def</span> __foo(<span class="va">self</span>):</a><a class="sourceLine" id="cb22-3" data-line-number="3">        <span class="cf">pass</span></a><a class="sourceLine" id="cb22-4" data-line-number="4"><span class="op">&gt;&gt;&gt;</span> a <span class="op">=</span> A()</a><a class="sourceLine" id="cb22-5" data-line-number="5"><span class="op">&gt;&gt;&gt;</span> a.__foo()</a><a class="sourceLine" id="cb22-6" data-line-number="6"><span class="pp">AttributeError</span>: <span class="st">&#39;A&#39;</span> <span class="bu">object</span> has no attribute <span class="st">&#39;__foo&#39;</span></a><a class="sourceLine" id="cb22-7" data-line-number="7"><span class="op">&gt;&gt;&gt;</span> a._A__foo()</a></code></pre></div><p>Name mangling is intended to give classes an easy way to define “private” instance variables and methods, without having to worry about instance variables defined by derived classes, or mucking with instance variables by code outside the class. Polymorphism is therefore associated with name mangling to access corresponding methods for different instances. This also reflects that all methods in Python are <a href="https://www.geeksforgeeks.org/virtual-function-cpp/" target="_blank" rel="noopener">virtual</a>.</p><blockquote><p><code>__double_leading_and_trailing_underscore__</code>: &quot;magic&quot; objects or attributes that live in user-controlled namespaces. E.g. <code>__init__</code>, <code>__import__</code> or <code>__file__</code>. Never invent such names; only use them as documented.</p></blockquote><h3 id="reference">Reference</h3><ol type="1"><li><a href="https://docs.python.org/3/tutorial/classes.html" target="_blank" rel="noopener">Classes Documentation</a></li><li><a href="https://www.geeksforgeeks.org/namespaces-and-scope-in-python/" target="_blank" rel="noopener">Namespaces and Scope in Python - GeeksforGeeks</a></li><li><a href="https://stackoverflow.com/questions/136097/difference-between-staticmethod-and-classmethod" target="_blank" rel="noopener">Difference between staticmethod and classmethod</a></li><li><a href="https://stackoverflow.com/questions/29614907/python-when-to-use-static-method-over-class-method" target="_blank" rel="noopener">When to use static method over class method?</a></li><li><a href="https://libcst.readthedocs.io/en/latest/metadata.html#scope-metadata" target="_blank" rel="noopener">Scope Metadata</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;an-example&quot;&gt;An Example&lt;/h3&gt;&lt;p&gt;Let&#39;s first look at an example and see what the output should be when executed.&lt;/p&gt;&lt;div class=&quot;sourceC
      
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
      <category term="Namespaces" scheme="http://yoursite.com/categories/Python/Namespaces/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Essence of Linear Algebra</title>
    <link href="http://yoursite.com/2020/07/16/algebra/"/>
    <id>http://yoursite.com/2020/07/16/algebra/</id>
    <published>2020-07-16T13:54:17.000Z</published>
    <updated>2020-08-19T23:31:36.691Z</updated>
    
    <content type="html"><![CDATA[<h3 id="announcements">Announcements</h3><p><strong>Contents below are basically notes of <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" target="_blank" rel="noopener">Essence of linear algebra</a> from <a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw" target="_blank" rel="noopener">3Blue1Brown</a>.</strong></p><h3 id="what-is-exactly-a-vector">What is exactly a vector?</h3><p>There are mainly 3 perspectives as below</p><ul><li>Physics student - Vectors are arrows with specific directions and lengths.</li><li>CS student - Vectors are ordered lists of numbers.</li><li>Mathematician - Vectors are arrows and at the same time ordered lists of numbers.</li></ul><p>If there is a coordinate system say <span class="math inline">\(x-y\)</span> plane, we can easily imagine an arrow with its tail sitting at the origin. No matter which direction it points to and how long it is, it is a 2-dimensional vector which can be easily visualized. In the meantime, the coordinates of this vector is a pair of numbers that tell you how to get from the tail of the vector, at the origin, to its tip. For example, the first number of vector <span class="math inline">\(\vec{v} = [2 \space 3]^T\)</span> tells you how far to walk along the <span class="math inline">\(x\)</span>-axis and the second number tells you how far to walk along the <span class="math inline">\(y\)</span>-axis after that. The same goes for n-dimensional space.</p><h4 id="vector-operations">Vector Operations</h4><p>With the coordinates definition, it is straightforward to define the vector addition. Imagine there are 2 vectors <span class="math inline">\(\vec{u}, \vec{v}\)</span> and move the second one so that its tail sits at the tip of the first one. Then draw a new vector from the tail of the first one to the tip of the second one and that new vector is the sum of these 2 vectors. <span class="math display">\[ \vec{u} + \vec{v} = \begin{bmatrix} u_{1} \\ u_{2} \end{bmatrix} + \begin{bmatrix} v_{1} \\ v_{2} \end{bmatrix} = \begin{bmatrix} u_{1} + v_{1} \\ u_{2} + v_{2} \end{bmatrix} \]</span> And another vector operation is multiplication by a number. <span class="math inline">\(2\vec{v}\)</span> simply means stretching the original vector so that it's twice the original length. <span class="math inline">\(\frac{1}{3}\vec{v}\)</span> means squishing <span class="math inline">\(\vec{v}\)</span> so that it's <span class="math inline">\(\frac{1}{3}\)</span> of the original length. This process is called <strong>scaling</strong>. And these numbers to scale these vectors are <strong>scalars</strong>. <span class="math display">\[ a \vec{v} = a \begin{bmatrix} v_{1} \\ v_{2} \end{bmatrix} = \begin{bmatrix} a v_{1} \\ av_{2} \end{bmatrix} \]</span></p><h3 id="linear-combinations-span-and-basis-vectors">Linear combinations, span, and basis vectors</h3><p>Let us look at a vector <span class="math inline">\(\vec{v} = \begin{bmatrix} 2 \\ 3 \end{bmatrix}\)</span>. If we use the above vector operations to express this vector using 2 special vectors <span class="math inline">\(\vec{i} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span> and <span class="math inline">\(\vec{j} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}\)</span> which are 2 unit vectors in x-direction and y-direction (Also, <span class="math inline">\(\vec{i}\)</span> and <span class="math inline">\(\vec{j}\)</span> are the typical basis vectors of the x-y coordinate system), we can get a linear combination <span class="math inline">\(\vec{v} = 2 \vec{i} + 3\vec{j}\)</span>. It is natural to think of <span class="math inline">\(\vec{v}\)</span> as adding <span class="math inline">\(\vec{i}\)</span> scaled by 2 and <span class="math inline">\(\vec{j}\)</span> scaled by 3. This &quot;adding scaled vectors&quot; process is using linear combinations of basis vectors to express any 2-D vector. And this is a key point if we are to discuss concepts below.</p><h4 id="span">Span</h4><p>The span of <span class="math inline">\(\vec{u}\)</span> and <span class="math inline">\(\vec{v}\)</span> is the set of all their linear combinations - <span class="math inline">\(a \vec{u} + b \vec{v}\)</span> with <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> varying over all real numbers. In other words, the span of these 2 vectors is also defining what are all the possible vectors you can reach using these 2 vectors and 2 fundamental operations - vector addition and scalar multiplication. If <span class="math inline">\(\vec{u}\)</span> and <span class="math inline">\(\vec{v}\)</span> line up, their span is just a line. If <span class="math inline">\(\vec{u}\)</span> and <span class="math inline">\(\vec{v}\)</span> are both zero vectors, their span is just a point. In most cases, their span is the entire infinite sheet of 2-D space.</p><p>From this perspective, linearly dependent vectors arise when one vector can be removed from a set of multiple vectors without reducing the span. And this vector can be expressed as a linear combination of the others because it's already in the span of the others. In other words, if each vector does add another dimension to the span, they are linearly independent.</p><h4 id="basis">Basis</h4><p>The basis of a vector space is a set of linearly independent vectors that span the full space.</p><h3 id="matrices-and-linear-transformations">Matrices and Linear Transformations</h3><p>Transformation is a fancy word for function. In the context of linear algebra, we would like to think about transformations that take in some input vector and spit out another vector. And the word transformation suggests how an input vector is converted to the output vector. It may experience spinning, stretching or reversing.</p><p>A transformation is linear if 1) all lines must remain lines without getting curved and 2) the origin must remain fixed in place. In general, linear transformations can be seen as <strong>keeping grid lines parallel and evenly spaced</strong>. A straightforward example is a rotation about the origin.</p><p>Now the question now becomes <strong>how should we describe any linear transformation numerically?</strong> The answer is super simple - we can show the <strong>transformed basis vector matrix</strong> to represent this process. An intuitive explanation is that given the basis vectors and every vector in their span is a certain linear combination of the basis. And because of the property of <strong>keeping grid lines parallel and evenly spaced</strong> in linear transformations, a vector starting off a certain linear combination of basis vectors still ends up the same linear combination of the transformed basis vectors. Mathematically, we can express the process <span class="math display">\[ i = \begin{bmatrix} 1 \\ 0 \end{bmatrix} \rightarrow \begin{bmatrix} 1 \\ -2 \end{bmatrix}, \space j = \begin{bmatrix} 0 \\ 1 \end{bmatrix} \rightarrow \begin{bmatrix} 3 \\ 0 \end{bmatrix} \\ \]</span> as the basis vector transformation, and the process <span class="math display">\[ \begin{bmatrix} x \\ y \end{bmatrix} = x\begin{bmatrix} 1 \\ 0 \end{bmatrix} + y \begin{bmatrix} 0 \\ 1 \end{bmatrix} = \begin{bmatrix} 1 &amp; 0\\ 0 &amp; 1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} \rightarrow \begin{bmatrix} x \\ y \end{bmatrix} = x\begin{bmatrix} 1 \\ -2 \end{bmatrix} + y \begin{bmatrix} 3 \\ 0 \end{bmatrix} = \begin{bmatrix} 1 &amp; -2\\ 3 &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} \]</span> as linear transformation of any vector. This is exactly &quot;adding scaled vectors&quot; stated above in <strong>Linear Combination</strong> chapter. If we omit the vector <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span> to simplify the process and we can get <span class="math display">\[ \begin{bmatrix} 1 &amp; 0\\ 0 &amp; 1 \end{bmatrix} \rightarrow \begin{bmatrix} 1 &amp; -2\\ 3 &amp; 0 \end{bmatrix} \]</span> As we can see, the column vector tells what the original basis vector <span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span> become after transformed. Obviously, the basis vector matrix transforms to another one representing a certain type of linear transformation and all vectors in the original span follow that.</p><p>If I am given a matrix <span class="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix}\)</span>, I will say it indicates a <span class="math inline">\(90^\circ\)</span> counterclockwise rotation in 2-D space. Correspondingly, <span class="math inline">\(\begin{bmatrix} 0 &amp; 1 \\ -1 &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix}\)</span> means rotating vector <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span> <span class="math inline">\(90^\circ\)</span> counterclockwise about the origin. The important thing of introducing linear transformation is <strong>seeing any matrix as a certain linear transformation</strong> because it will be easier to understand concepts like matrix multiplication, determinant, eigenvectors and others.</p><h3 id="matrix-multiplication-as-composition">Matrix Multiplication as Composition</h3><p>Now consider 3 matrices <span class="math display">\[ R = \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix}, \space S = \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix}, RS = \begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 0 \end{bmatrix} \]</span> where <span class="math inline">\(R\)</span> represents a <span class="math inline">\(90^\circ\)</span> clockwise rotation, <span class="math inline">\(S\)</span> represents a shear and <span class="math inline">\(RS\)</span> means a rotation and shear. Note that <span class="math inline">\(RS\)</span> describes an overall effect of a rotation then a shear. It is equivalent to carrying out 2 successive actions to a vector like below. <span class="math display">\[ \begin{bmatrix} 1 &amp; 1 \\ 0 &amp; 1 \end{bmatrix} \left( \begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} \right) = \begin{bmatrix} 1 &amp; -1 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} \]</span> where left hand side shows first rotating and shearing vector <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span> and right hand side shows the composite transformation. And this is because of the geometric meaning of matrix multiplication which <strong>applying one transformation then another</strong>.</p><p>Now we take a loot at how matrix multiplication is done mathematically <span class="math display">\[ \begin{bmatrix} e &amp; f \\ g &amp; h \end{bmatrix} \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} = \begin{bmatrix} a \begin{bmatrix} e \\ g \end{bmatrix} + c \begin{bmatrix} f \\ h \end{bmatrix} &amp;&amp; b \begin{bmatrix} e \\ g \end{bmatrix} + d \begin{bmatrix} f \\ h \end{bmatrix} \end{bmatrix} \]</span> we can think of matrix multiplication as transforming basis vectors under different rules. Naturally, the first column vector is the vector <span class="math inline">\(\begin{bmatrix} a \\ c \end{bmatrix}\)</span> after transformed and second column follows the similar procedure.</p><p>Here are some matrix multiplication properties which can be easily proved by this thought.</p><ul><li><p>Associativity</p><p><span class="math inline">\((AB)C = A(BC)\)</span> can be seen as first applying transformation represented by <span class="math inline">\(C\)</span> and then <span class="math inline">\(AB\)</span> and also can be seen as first applying composite transformation represented by <span class="math inline">\(BC\)</span> and then <span class="math inline">\(A\)</span>. They are equivalent in transforming as the composite transformation <span class="math inline">\(ABC\)</span>.</p></li><li><p>Commutativity</p><p><span class="math inline">\(AB \neq BA\)</span> can be proved if A is a rotation and B is a shear. <span class="math inline">\(AB\)</span> is a shear-then-rotate transformation that will make basis vectors point close together while <span class="math inline">\(BA\)</span> is a rotate-then-shear transformation giving basis vectors pointing far part.</p></li></ul><h3 id="determinant">Determinant</h3><p>We have known any matrix represents a certain type of linear transformation and we describe the scaling size of a linear transformation represented by a matrix using <strong>determinant</strong>. If the determinant of a transformation is <span class="math inline">\(3\)</span> then this transformation increases the area of a region by a factor of <span class="math inline">\(3\)</span>. Let's look at the below matrix <span class="math display">\[ \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 2 \end{bmatrix} \]</span> whose linear transformation means stretching all vectors in <span class="math inline">\(y\)</span> direction by the factor of <span class="math inline">\(2\)</span>. This turns out to increase all areas by the factor of <span class="math inline">\(2\)</span>. And this scaling factor is exactly described by determinant of the matrix.</p><p>Sometimes, the determinant of a matrix can be <strong>negative</strong>. In this case, the absolute value of the determinant still indicates the scaling factor while the negative sign means the orientation determined by basis vectors is now different from the original. To make it more clearly, we can think the original 2-D space as a sheet and the sheet is now flipped after the linear transformation is done.</p><p>Mathematically, the determinant of a 2-D matrix is computed as <span class="math display">\[ det\left( \begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} \right) = ab - cd \]</span> and <span class="math inline">\(ab - cd\)</span> is exactly the area of parallelogram whose adjacent edges are <span class="math inline">\(\begin{bmatrix} a \\ c \end{bmatrix}\)</span> and <span class="math inline">\(\begin{bmatrix} b \\ d \end{bmatrix}\)</span>. And if we extend the determinant computation to 3-D, we will see the determinant is exactly the volume of a parallelepiped spanned by the matrix's column vectors.</p><p>Also, determinant of matrices satisfy the following rules <span class="math display">\[ det(M_1 M_2) = det(M_1) det(M_2) \]</span> because the transformation represented by <span class="math inline">\(M_1 M_2\)</span> is equivalent to carrying out <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> successively and therefore the overall scaling factor is the multiplication of the separate scaling factors.</p><h3 id="inverse-matrices-column-space-and-null-space">Inverse Matrices, Column Space and Null Space</h3><p>From the perspective of linear transformations, these concepts will look different if we understand these concepts in the usual computation way. Let's first look at a system of equations <span class="math display">\[ \begin{matrix} 2x + 5y + 3z = -3 \\ 4x + 0y + 8z = 0 \\ 1x + 3y + 0z = 2 \end{matrix} \]</span> which is super familiar when we were at primary school. But if we present this linear system using matrix multiplication, we can get <span class="math display">\[ \begin{bmatrix} 2 &amp; 5 &amp; 3 \\ 4 &amp; 0 &amp; 8 \\ 1 &amp; 3 &amp; 0 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = \begin{bmatrix} -3 \\ 0 \\ 2 \end{bmatrix} \]</span> which is also very intuitive if we express the above system as <span class="math inline">\(A \vec{x} = \vec{b}\)</span> recall the geometric meaning of matrix multiplication. Obviously, <span class="math inline">\(A\)</span> indicates a linear transformation and solving <span class="math inline">\(A \vec{x} = \vec{b}\)</span> means we are looking for an <span class="math inline">\(\vec{x}\)</span> which lands on <span class="math inline">\(\vec{b}\)</span> after transformed. In short, we can think of a certain vector is stretched and rotated to become <span class="math inline">\(\begin{bmatrix} -3 \\ 0 \\ 2 \end{bmatrix}\)</span> and this is exactly what we are looking for. How? Let's first consider the situation where <span class="math inline">\(det(A) \neq 0\)</span> meaning the transformation doesn't shrink the space dimension. And we can use <span class="math inline">\(A\)</span>'s inverse to get the solution <span class="math display">\[ A^{-1} A \vec{x} = \vec{x} = A^{-1} \vec{b} \]</span> Note that <span class="math inline">\(\vec{x}\)</span> is transformed to <span class="math inline">\(\vec{b}\)</span> under <span class="math inline">\(A\)</span> and <span class="math inline">\(\vec{b}\)</span> is transformed back to <span class="math inline">\(\vec{x}\)</span> under <span class="math inline">\(A^{-1}\)</span>. This is pretty similar to the concept of <strong>functions and inverse functions</strong> <span class="math display">\[ f(x) = y \\ f^{-1}(y) = x \\ f^{-1}(f(x)) = f^{-1}(y) = x \]</span> we can see any value becomes itself if mapped by a function then mapped by the corresponding inverse function. And the same idea goes for linear transformations. <span class="math display">\[ A^{-1} A \vec{x} = \vec{x} \]</span> In general, <span class="math inline">\(A^{-1}\)</span> is a unique transformation that we will end up back where we started if we apply <span class="math inline">\(A\)</span> then apply <span class="math inline">\(A^{-1}\)</span>. <span class="math inline">\(A^{-1} A\)</span> comes to a transformation that does nothing and this is also called identity transformation. Geometrically, <span class="math inline">\(A^{-1}\)</span> transforms every vector back to what they are before they are transformed by <span class="math inline">\(A\)</span>. For example, if <span class="math inline">\(A\)</span> is a counterclockwise rotation by <span class="math inline">\(90^{\circ}\)</span> and then <span class="math inline">\(A^{-1}\)</span> is a clockwise rotation by <span class="math inline">\(90^{\circ}\)</span>.</p><p>However, if <span class="math inline">\(det(A) = 0\)</span>, it means <span class="math inline">\(A\)</span> squishes a high-dimension space into a low-dimension space like squishing a plane into a line. At this time, there is no inverse matrix <span class="math inline">\(A^{-1}\)</span> because we cannot &quot;unsquish&quot; a line into a plane. At least, that's not something a function can do since that would require an individual vector to convert to a multiple vectors while function is always a 1-to-1 mapping.</p><p>We have a new terminology <strong>rank</strong> to describe these situations where <span class="math inline">\(det(A) = 0\)</span>. For a 3-d matrix <span class="math inline">\(A\)</span>, if the output of the transformation is a line meaning it's one-dimensional, we say the transformation <span class="math inline">\(A\)</span> has a rank of <span class="math inline">\(1\)</span>. Similarly, if the output is a plane meaning it's two-dimensional, we say the transformation <span class="math inline">\(A\)</span> has a rank of <span class="math inline">\(2\)</span>. So <strong>rank</strong> means the number of dimensions in the output of a transformation.</p><p>To sum up, the set of all possible outputs <span class="math inline">\(A \vec{x}\)</span> is called the column space of the matrix <span class="math inline">\(A\)</span>. This is pretty natural because the column vectors of <span class="math inline">\(A\)</span> tells us where the basis vectors land after transformation. If the rank of a matrix is as high as it can be, it means it equals the number of columns and we call the matrix <strong>full rank</strong>. If all basis vectors land on a line for a 3-D matrix, then the column space is a line and <span class="math inline">\(rank(A) = 1\)</span>. Now solving the equation has become the question that if the target vector <span class="math inline">\(\vec{b}\)</span> is within the span of columns of <span class="math inline">\(A\)</span>. Let's continue with the above example. If <span class="math inline">\(\vec{b}\)</span> happens to be on that line where all basis vectors land after transformation, then there are infinite solutions to that equation. However, if <span class="math inline">\(\vec{b}\)</span> happens to be out of scope of the span, there is no solution.</p><p>Let's also consider a special vector which is always in the column space whatever the transformation, and this vector is called zero vector. And the set of all possible vectors that land on the origin after transformation is called <strong>null space</strong> or <strong>kernel</strong> of your matrix. It's the space of all vectors that become null. And when we try to solve an equation like <span class="math inline">\(A \vec{x} = \vec{0}\)</span>, the null space gives us all possible solutions to this equation. Also, we call this kind of equations with the name of <strong><a href="https://en.wikipedia.org/wiki/System_of_linear_equations#Homogeneous_systems" target="_blank" rel="noopener">Homogeneous Linear Equations</a></strong>.</p><h3 id="non-square-matrices-as-transformations-between-dimensions">Non-square Matrices as Transformations between Dimensions</h3><p>For a non-square matrix, we also use linear transformation perspective to interpret the geometric meaning it stands for. For example, <span class="math display">\[ \begin{bmatrix} 3 &amp; 1 \\ 4 &amp; 1 \\ 5 &amp; 9 \end{bmatrix} \]</span> which is a <span class="math inline">\(3 \times 2\)</span> matrix and its column vectors still indicate where the original 2-D space basis vectors land after transformation. As we can see, 2 columns indicate that the input space has 2 basis vectors and 3 rows indicate that the landing spots for each of these basis vectors is described with three separate coordinates. This matrix reveals a transformation that maps 2 dimensions to 3 dimensions. However, this matrix is still full rank because the number of column equals the rank of the column space. Note, the rank is still 2 even the matrix represents a mapping from 2-D to 3-D. This is because the set of all possible outputs after transformation still span a plane in 3-D space instead of a 3-D space.</p><p>Similarly, the below <span class="math inline">\(2 \times 3\)</span> matrix <span class="math display">\[ \begin{bmatrix} 3 &amp; 1 &amp; 5\\ 4 &amp; 1 &amp; 5\\\end{bmatrix} \]</span> represents a mapping from 3-D to 2-D because the 3 columns indicate the input space has 3 basis vectors while the 2 rows indicate the landing spots for each of these basis vectors is described with only 2 coordinates. We can think of this process as squishing and projecting the 3 orthogonal basis onto a 2-D plane.</p><p>Now, let's look at a <span class="math inline">\(1 \times 2\)</span> matrix <span class="math display">\[ \begin{bmatrix} 3 &amp; 2 \end{bmatrix} \]</span> which represents the process of smashing a plane into a line while keeping evenly spaced dots remain evenly spaced after mapped.</p><p>To sum up, for non-square matrices, the number of columns and the number of rows represent the dimensions of input space and the dimensions of output space respectively. And there is no determinant for non-square matrices. This is because the determinant of a matrix indicates the scaling size of transformation in the same space and within the same dimension. However, we cannot measure how the size of space change over dimensions.</p><h3 id="dot-products-and-duality">Dot Products and Duality</h3><p>A fuller understanding of the role the dot products play in math can only be found in the light of linear transformations.</p><p>Let's first review the standard introduction of dot products and its geometric meaning. <span class="math display">\[ \vec{v} \cdot \vec{w} = \begin{bmatrix} a \\ b \end{bmatrix} \cdot \begin{bmatrix} c \\ d \end{bmatrix} = ac + bd\\ \vec{v} \cdot \vec{w} = (Length \space of \space projected \space \vec{w}) (Length \space of \space \vec{v}) \\ \vec{v} \cdot \vec{w} = (Length \space of \space projected \space \vec{v}) (Length \space of \space \vec{w}) \]</span> One surprisingly amazing property of dot products is that the order of this projection and multiplication process doesn't matter. We can project <span class="math inline">\(\vec{v}\)</span> onto <span class="math inline">\(\vec{w}\)</span> and multiply the projected length of <span class="math inline">\(\vec{v}\)</span> by the length of <span class="math inline">\(\vec{w}\)</span> and we can also project <span class="math inline">\(\vec{w}\)</span> onto <span class="math inline">\(\vec{v}\)</span> and multiply the projected length of <span class="math inline">\(\vec{w}\)</span> by the length of <span class="math inline">\(\vec{v}\)</span>. And this actually can be proved by building similar triangles or as follows <span class="math display">\[ \vec{v} \cdot \vec{w} = |\vec{w}| cos&lt;\vec{v}, \vec{w}&gt; |\vec{v}| = |\vec{w}| |\vec{v}| cos&lt;\vec{v}, \vec{w}&gt; \]</span> And another tricky point is how is the perspective of projection and multiplication associated with the perspective of multiplying coordinates of pairs and adding them together?</p><p>To answer this, let's recall the geometric meaning of <span class="math inline">\(1 \times 2\)</span> matrix covered in last chapter, say a matrix like below <span class="math display">\[ \begin{bmatrix} 3 &amp; -2 \end{bmatrix} \]</span> which means a transformation where 2 basis vectors in 2-D space have now landed on 3 and -2 on a 1-D number line. And if we apply this transformation to a certain 2-D vector, we can get <span class="math display">\[ \begin{bmatrix} 3 &amp; -2 \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = 3a -2b \]</span> where <span class="math inline">\(3a-2b\)</span> is exactly where the original vector <span class="math inline">\(\begin{bmatrix} a \\ b \end{bmatrix}\)</span> land on the number line after transformed. And this matrix multiplication operation is numerically equivalent to the dot products between <span class="math inline">\(\begin{bmatrix} a \\ b \end{bmatrix}\)</span> and <span class="math inline">\(\begin{bmatrix} 3 \\ -2 \end{bmatrix}\)</span>. So it's natural to declare there is a nice association between <span class="math inline">\(1 \times 2\)</span> matrix and a 2-D vector. Let's look at the image below</p><figure><img src="dot.JPG" alt="Dot"><figcaption>Dot</figcaption></figure><p>In the 2-D coordinate system, we have a vector <span class="math inline">\(\vec{u}\)</span> and we also have basis vectors sitting on <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> axis. Also, we draw a number line through <span class="math inline">\(\vec{u}\)</span> to show where these 2-D vectors will land. From the image, using a line of symmetry, we can easily tell the basis vector <span class="math inline">\(\hat{i}\)</span> sitting on <span class="math inline">\(x\)</span> axis is converted to a number exactly the same as the <span class="math inline">\(x\)</span> coordinate of <span class="math inline">\(\vec{u}\)</span>. And the same goes for the other basis vector <span class="math inline">\(\hat{j}\)</span>. Till now, we have found a 2-D to 1-D linear projection transformation restricted by <span class="math inline">\(\vec{u}\)</span> and the entries of corresponding <span class="math inline">\(1 \times 2\)</span> matrix describing the transformation are exactly the coordinates of <span class="math inline">\(\vec{u}\)</span>. So this just explains why taking a dot product among vectors can be interpreted as projecting a vector onto the span of the other one and taking the length. <span class="math display">\[ \begin{bmatrix} u_x &amp; u_y \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = a \cdot u_x + b \cdot u_y \\ \begin{bmatrix} u_x \\ u_y \end{bmatrix} \cdot \begin{bmatrix} a \\ b \end{bmatrix} = a \cdot u_x + b \cdot u_y \\ \]</span> Let us think about the process again! We had a linear transformation from 2-D space to the number line which was not defined by numerical dot products. It was just defined by projecting space onto a copy of the number line decided by the vector <span class="math inline">\(\vec{u}\)</span>. Because the transformation is linear, it was necessarily described by some <span class="math inline">\(1 \times 2\)</span> matrix whose entries are the coordinates <span class="math inline">\(\vec{u}\)</span>. And since multiplying this matrix by another 2-D vector <span class="math inline">\(\vec{v}\)</span> is the same as taking a product between <span class="math inline">\(\vec{u}\)</span> and <span class="math inline">\(\vec{v}\)</span>. This transformation and the vector is inescapably related to each other. The punch line here is, for any linear transformation whose output space is the number line, there is going to be a unique vector corresponding to that transformation. In this sense, applying the transformation is the same thing as taking a product. This is an example duality: the dual of a linear transformation from <span class="math inline">\(n\)</span> dimension to <span class="math inline">\(1\)</span> dimension is a vector in that <span class="math inline">\(n\)</span> dimension.</p><p>A takeaway here is that a vector sometimes can be interpreted as a linear transformation instead of an arrow in space.</p><h3 id="cross-product">Cross Product</h3><p>For 2-D vectors, the cross product of 2 2-D vectors is a new vector. And the length of the vector will be the area of a parallelogram spanned by these 2 vectors. The direction of the new vector is going to be perpendicular to that parallelogram and can be told with right hand rules. Specifically, the cross product is defined as <span class="math display">\[ \begin{bmatrix} v_1 \\ v_2 \\ v_3 \end{bmatrix} \times \begin{bmatrix} w_1 \\ w_2 \\ w_3 \end{bmatrix} = det \left( \begin{bmatrix} \hat{i} &amp; v_1 &amp; w_1 \\ \hat{j} &amp; v_2 &amp; w_2 \\ \hat{k} &amp; v_3 &amp; w_3 \end{bmatrix} \right) \]</span> Recall that for a <span class="math inline">\(2 \times 1\)</span> matrix, there is always a 2-D vector (which is the dual vector of that transformation) that corresponds to it. And performing the transformation is the same as taking a product with that vector. This is called duality. While this does not only apply to <span class="math inline">\(2 \times 1\)</span> matrix, it also applies to any matrix if the corresponding linear transformation's output space is <span class="math inline">\(1\)</span> dimension. And the cross product also embodies the idea of duality.</p><p>To explain how duality is applied in cross product, let's plan to</p><ol type="1"><li><p>Define a 3d-to-1d linear transformation in terms of <span class="math inline">\(\hat{v}\)</span> and <span class="math inline">\(\hat{w}\)</span>,</p></li><li>Find its dual vector <span class="math inline">\(\hat{p}\)</span> in 3-D space,</li><li><p>Show that this dual vector <span class="math inline">\(\hat{p} = \hat{v} \times \hat{w}\)</span>.</p></li></ol><p>And this is all because this transformation displays the connection between the computation and the geometry of the cross product.</p><p>Recall that in 2D space, the cross product of <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span> is simply the determinant of the matrix whose column vectors are <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span>. This is also the are of the parallelogram spanned by these 2 vectors. And we will naturally think of the volume of some parallelepiped as the cross product among 3D vectors. But the question is how the parallelepiped looks like? Now we consider a function <span class="math display">\[ L\left( \begin{bmatrix} x \\ y \\ z \end{bmatrix} \right) = det \left( \begin{bmatrix} x &amp; v_1 &amp; w_1 \\ y &amp; v_2 &amp; w_2 \\ z &amp; v_3 &amp; w_3 \end{bmatrix} \right) \]</span> which describes a parallelepiped spanned by <span class="math inline">\(\vec{v}\)</span>, <span class="math inline">\(\vec{w}\)</span> and an unknown 3D vector. And an important feature about this function is its linearity. Based on that, we can bring the idea of duality, which means we can introduce a <span class="math inline">\(1 \times 3\)</span> matrix to describe the 3D-to-1D transformation, <span class="math display">\[ \begin{bmatrix} v_2 w_3 - v_3 w_2 &amp; v_3 w_1 - v_1 w_3 &amp; v_1 w_2 - v_2 w_1 \end{bmatrix} \begin{bmatrix} x \\ y \\ z \end{bmatrix} = det \left( \begin{bmatrix} x &amp; v_1 &amp; w_1 \\ y &amp; v_2 &amp; w_2 \\ z &amp; v_3 &amp; w_3 \end{bmatrix} \right) \]</span> and there is a corresponding vector and taking dot product of it is equivalent as performing that transformation. <span class="math display">\[ \begin{bmatrix} v_2 w_3 - v_3 w_2 \\ v_3 w_1 - v_1 w_3 \\ v_1 w_2 - v_2 w_1 \end{bmatrix} \cdot \begin{bmatrix} x \\ y \\ z \end{bmatrix} = det \left( \begin{bmatrix} x &amp; v_1 &amp; w_1 \\ y &amp; v_2 &amp; w_2 \\ z &amp; v_3 &amp; w_3 \end{bmatrix} \right) \]</span> So the function above is built to find such a vector <span class="math inline">\(\vec{p}\)</span> that taking a dot product between <span class="math inline">\(\vec{p}\)</span> and <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span> is equivalent to the determinant of the matrix whose column vectors are <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span>, <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span>.</p><p>And this also gives the geometric meaning of <span class="math inline">\(\vec{p}\)</span>. Because <span class="math inline">\(\vec{p}\)</span> is such a 3D vector that taking a dot product between <span class="math inline">\(\vec{p}\)</span> and <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span> is equivalent to the signed volume of the parallelepiped whose spanned by <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span>, <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span>. To state the geometric property more clearly, let's decompose the volume of the above parallelepiped as</p><p><span class="math display">\[ (Area \space of \space parallelogram \space spanned \space by \space \vec{v}, \vec{w}) \times (Component \space of \begin{bmatrix} x \\ y \\ z \end{bmatrix} perpendicular \space to \space \vec{v}, \vec{w}) \]</span> From this perspective, the function above is projecting the vector <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span> onto a line perpendicular to <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span>, then multiplying the length of the projection by the area of the parallelogram spanned by <span class="math inline">\(\vec{v}\)</span> and <span class="math inline">\(\vec{w}\)</span>. Also, this is the same as taking a product between <span class="math inline">\(\begin{bmatrix} x \\ y \\ z \end{bmatrix}\)</span> and a vector perpendicular to <span class="math inline">\(\vec{v}\)</span>, <span class="math inline">\(\vec{w}\)</span> with a length <span class="math inline">\(=\)</span> the area of that parallelogram. So this is the geometric meaning of <span class="math inline">\(\vec{p}\)</span>.</p><p>To integrate the geometry and computation perspective, <span class="math inline">\(\vec{p}\)</span> and <span class="math inline">\(\vec{v} \times \vec{w}\)</span> are 2 dual vectors of the same linear transformation, so they must be the same. So we have presented how the cross product of two 3D vectors is computed and its geometric meaning.</p><h3 id="change-of-basis">Change of Basis</h3><p>Till now, we have always used a coordinate system to translate between vectors and a set of numbers. And there are 2 special vectors <span class="math inline">\(\hat{i}\)</span> and <span class="math inline">\(\hat{j}\)</span> called basis vectors of the standard coordinate system. Each vector in the coordinate system is a linear combination of these basis vectors. Now let's think about what will happen if we change the set of basis vectors into a different set.</p><p>Let's consider another set of basis vectors <span class="math inline">\(\hat{b_1} = \begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span> and <span class="math inline">\(\hat{b_2} = \begin{bmatrix} -1 \\ 1 \end{bmatrix}\)</span>, this is just like <span class="math inline">\(\hat{i} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span> and <span class="math inline">\(\hat{b_2} = \begin{bmatrix} 0 \\ 1 \end{bmatrix}\)</span> in our system. Now it's natural to know how to translate vectors between different coordinate systems. For example, if a vector is expressed as <span class="math inline">\(\begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span> in the <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> system, what does it look like in our system? Likewise, if a vector is expressed as <span class="math inline">\(\begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span> in our system, what does it look like in the <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> system?</p><p>Now let's look at the matrix whose columns are <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> as below <span class="math display">\[ \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1\end{bmatrix} \]</span> which transforms the basis vectors into a new set of basis vectors, and also transforms the original coordinate system into a new coordinate system. But numerically, this new basis vectors are still expressed using our original language. Therefore, if a vector is expressed as <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span> in our language, what it will look like in the <span class="math inline">\(\hat{b_1}\)</span>. <span class="math inline">\(\hat{b_2}\)</span> system is <span class="math display">\[ \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1\end{bmatrix} \begin{bmatrix} x \\ y\end{bmatrix} \]</span> and this is because a vector is always the same linear combination of basis vectors whatever the transformation is.</p><p>In the opposite, if a vector is expressed as <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span> in the <span class="math inline">\(\hat{b_1}\)</span>. <span class="math inline">\(\hat{b_2}\)</span> system, what it is in our original system? <span class="math display">\[ \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1\end{bmatrix}^{-1} \begin{bmatrix} x \\ y\end{bmatrix} \]</span> and this is because of the same reason as above.</p><p>Since vectors are not the only thing expressed with coordinates, the question now becomes how we translate matrices/ linear transformations between different coordinate systems? For example, what a <span class="math inline">\(90^{\circ}\)</span> clockwise rotation looks like in the <span class="math inline">\(\hat{b_1}\)</span>. <span class="math inline">\(\hat{b_2}\)</span> system? <span class="math display">\[ \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1\end{bmatrix}^{-1} \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; -1\end{bmatrix} \begin{bmatrix} 2 &amp; -1 \\ 1 &amp; 1\end{bmatrix} \]</span> this process is like first performing the transformation to express basis vectors in our language and then performing rotation. This intermediate matrix is the rotated <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> basis vectors in our language. The remaining step is to translate the intermediate one into a final one in the language of <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> system. The composition of these 3 matrices gives us the <span class="math inline">\(90^{\circ}\)</span> clockwise rotation in the language of <span class="math inline">\(\hat{b_1}\)</span>, <span class="math inline">\(\hat{b_2}\)</span> system.</p><p>Whenever we see an expression like <span class="math inline">\(A^{-1} M A\)</span>, it suggests a translation or an empathy in a mathematical way, where <span class="math inline">\(M\)</span> represents an intuitive linear transformation and the other 2 matrices represent the empathy, the translator or the shift in perspective. It still indicates the same transformation but from other perspective.</p><h3 id="eigenvectors-and-eigenvalues">Eigenvectors and Eigenvalues</h3><p>Let's think about a matrix like <span class="math inline">\(\begin{bmatrix} 3 &amp; 1 \\ 0 &amp; 2\end{bmatrix}\)</span> and a random vector <span class="math inline">\(\begin{bmatrix} x \\ y \end{bmatrix}\)</span>. If we apply the linear transformation represented by that matrix to that vector, the vector is most likely to get knocked off the vector's span (the line passing through the origin and its tip) during the transformation. But there are some special vectors that do remain on their own span, meaning the effect that the matrix has on such a vector is just stretching or squishing like a scalar. For this specific example, <span class="math inline">\(\begin{bmatrix} 1 \\ 0 \end{bmatrix}\)</span> is such a special vector which is stretched to 3 times itself and still lands on <span class="math inline">\(x\)</span> axis. And due to linearity, any other vector on the <span class="math inline">\(x\)</span> axis (the vector's span) will also be stretched out by a factor of 3 during the transformation. In summary, these vectors are so-called eigenvectors of the transformation, and there are corresponding eigenvalues measuring the factor it stretches or squishes during that transformation. If we translate what we plan to do above into mathematical terms, we can get <span class="math display">\[ A \vec{v} = \lambda \vec{v} \]</span> which is equivalent to solving <span class="math inline">\((A - \lambda I)\vec{v} = \vec{0}\)</span>. If we want a non-zero solution of <span class="math inline">\(\vec{v}\)</span>, then the transformation <span class="math inline">\(A - \lambda I\)</span> must be a dimension reducing one, meaning a zero determinant. We can think of <span class="math inline">\(\lambda\)</span> as a disturbance term and changes the linear transformation <span class="math inline">\(A\)</span> in a way that the new transformation squishes space into a lower dimension or equivalently the new column vectors are colinear.</p><p>And why are these useful things to think about? Let's consider some 3D rotation. If we can find an eigenvector for that rotation, then we find the axis of that rotation. It's much easier to think about a 3D rotation in terms of some axis of rotation with some angle compared to a <span class="math inline">\(3 \times 3\)</span> matrix.</p><p>There are some takeaways about solving eigenvalues and eigenvectors</p><ul><li>It doesn't matter if there is a negative eigenvalue as long as the eigenvector stays on the line it spans out without getting knocked off.</li><li>A transformation doesn't have to have eigenvectors. For example, there are no eigenvectors for a <span class="math inline">\(90^{\circ}\)</span> rotation since any non-zero vector is rotated and moves its own span.</li><li>There may be only 1 eigenvalue but more than 1 eigenvectors for a transformation. Say a transformation that stretches everything by 2. It only has the eigenvalue of 2 but every vector in the plane is the eigenvector with that eigenvalue.</li></ul><p>What if all basis vectors are eigenvectors? If we write the above eigenvector equation in matrix format, it's $A V = V $ where columns of <span class="math inline">\(V\)</span> are eigenvectors of <span class="math inline">\(A\)</span> and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix whose diagonal elements are the corresponding eigenvalues. Then the linear transformation <span class="math inline">\(A\)</span> is express as <span class="math inline">\(V^{-1} A V = \Lambda\)</span> in the language of eigen-basis system, and we can then get <span class="math inline">\(A^n = V^{-1} \Lambda^{n} V\)</span> consequently. Because <span class="math inline">\(\Lambda\)</span> is a diagonal matrix, it's much easier to compute <span class="math inline">\(\Lambda^n\)</span> compared to compute the <span class="math inline">\(n_{th}\)</span> power of a non-diagonal matrix.</p><p>What if the basis vectors are not eigenvectors? Because of the great properties above, we would like to perform change of basis so that these eigenvectors become our basis vectors. But that can only happen when there are enough eigenvectors to span a full space. Mathematically, we can get basis vectors as follows <span class="math display">\[ V^{-1} A V = \Lambda \]</span> and this is also called diagonalization.</p><h3 id="abstract-vector-spaces">Abstract Vector Spaces</h3><p>Let's go back to the original question and ask what are vectors? Are they lists of numbers or ordered arrows? We won't call vectors as list of numbers considering determinant and eigenvectors don't care about the coordinate system. In summary, vectors can be specified as ordered arrows or lists of numbers but they are technically not the full definition of vectors. In mathematical terms, they are called vector spaces. And there are 8 axioms any vector space must satisfy so that those vector operations, dot products and eigen-things are valid.</p><h3 id="other-topics">Other Topics</h3><p><strong>Orthogonal Matrix</strong></p><p>An <strong>orthogonal matrix</strong> is a square matrix whose columns and rows are orthogonal unit vectors (orthonormal vectors). Mathematically speaking, for the <span class="math inline">\(i_{th}\)</span> and <span class="math inline">\(j_{th}\)</span> column <span class="math inline">\(C_i\)</span>, <span class="math inline">\(C_j\)</span> of the orthogonal matrix <span class="math inline">\(A\)</span>, we have <span class="math display">\[ \langle C_i, C_j \rangle = \delta_{ij} \]</span> where <span class="math inline">\(\delta_{ij} = 1\)</span> for <span class="math inline">\(i=j\)</span> while <span class="math inline">\(\delta_{ij} = 0\)</span> for <span class="math inline">\(i \neq j\)</span>. Then we can derive <span class="math display">\[ A^T A = \begin{bmatrix} C_1^T \\ ... \\C_n^T \end{bmatrix} \begin{bmatrix} C_1 &amp; ... &amp; C_n \end{bmatrix} = [\langle C_i, C_j \rangle]_{1\leq i,j \leq n}=I_n \]</span></p><p>which is also <span class="math inline">\(A^T = A^{-1}\)</span>. Besides, <span class="math inline">\(A A^T = I_n\)</span> holds true and <span class="math inline">\(A^T\)</span> is also an orthogonal matrix.</p><p>Note, if a matrix has pairwise orthogonal column vectors but not pairwise orthogonal row vectors, then it is not an orthogonal matrix. For example, <span class="math inline">\(\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\1 &amp; 0 &amp; 0\end{bmatrix}\)</span> is such a matrix.</p><p>Geometrically, an orthogonal matrix represents an <strong>orthogonal transformation that preserves a symmetric inner product</strong>. To put it simply, an orthogonal transformation is either a rigid rotation or a rotation followed by a flip. Either of them does not stretch or squish the original space and preserves lengths of vectors and angles between vectors after transformation. Mathematically, these properties for orthogonal matrices can be expressed as</p><ul><li><p><span class="math inline">\(\langle \vec{v}, \vec{w} \rangle = \langle A\vec{v}, A\vec{w} \rangle\)</span></p></li><li><p><span class="math inline">\(||\vec{v}|| = ||A \vec{v}||\)</span></p></li><li><p><span class="math inline">\(det(A) = 1 \space or -1\)</span></p></li></ul><p>Besides, the product of orthogonal matrices is also orthogonal. Orthogonal matrices are the real analogue of <a href="https://en.wikipedia.org/wiki/Unitary_matrix" target="_blank" rel="noopener">unitary matrices</a> (which require complex square matrices). And a number of <a href="https://en.wikipedia.org/wiki/Canonical_form#Linear_algebra" target="_blank" rel="noopener">decompositions</a> involve unitary/ orthogonal matrices including <span class="math inline">\(QR\)</span> decomposition, Singular value decomposition and Eigenvalue decomposition of a symmetric matrix.</p><h3 id="transpose-of-matrices">Transpose of Matrices</h3><p>The transpose of a matrix is flipping a matrix over its diagonal and switching its rows and columns in a way that <span class="math inline">\(A_{ij} = A^T_{ji}\)</span>. But from the perspective of linear transformation, how do we interpret the transpose a matrix geometrically? There is a <a href="https://math.stackexchange.com/questions/2192992/truly-intuitive-geometric-interpretation-for-the-transpose-of-a-square-matrix" target="_blank" rel="noopener">discussion</a> and giving 3 perspectives of the transpose of matrices. What I find the most intuitive is the one based on Singular Value Decomposition. Let's first find out how we interpret SVD geometrically as below</p><figure><img src="SVD.png" alt="SVD"><figcaption>SVD</figcaption></figure><p>which is the geometric representation of SVD <span class="math inline">\(A = U \Sigma V^T\)</span> where <span class="math inline">\(V\)</span> and <span class="math inline">\(U\)</span> are orthogonal matrices while <span class="math inline">\(\Sigma\)</span> is a diagonal matrix. And therefore the linear transformation represented by <span class="math inline">\(A\)</span> can be seen as the successive actions of first rotating (<span class="math inline">\(V^T\)</span>), then scaling (<span class="math inline">\(\Sigma\)</span>) and finally rotating (<span class="math inline">\(U\)</span>). This is what we are getting when we look at the image above from the left to the right.</p><p>The transpose of a matrix <span class="math inline">\(A^T = V \Sigma U^T\)</span> can then be derived. At the same time, <span class="math inline">\(U^T = U^{-1}\)</span> and <span class="math inline">\(V = {(V^T)}^{-1}\)</span> hold true because they are orthogonal matrices and we can rewrite the transpose of <span class="math inline">\(A\)</span> as <span class="math inline">\(A^T = {(V^T)}^{-1} \Sigma U^{-1}\)</span>. Since <span class="math inline">\(A^{-1}\)</span> can be seen as such a linear transformation that transforms whatever is transformed by <span class="math inline">\(A\)</span> back to its original state, then <span class="math inline">\(V\)</span> is rotating whatever is rotated by <span class="math inline">\(V^T\)</span> to the original and <span class="math inline">\(U^T\)</span> is rotating whatever is rotated by <span class="math inline">\(U\)</span> to the original state. And therefore we can also interpret <span class="math inline">\(A^T\)</span> as the successive actions of first rotating, then scaling and finally rotating. What is different is that <span class="math inline">\(A^T\)</span> is doing exactly the opposite actions of <span class="math inline">\(A\)</span> in terms of rotations.</p><p>With this interpretation, we can easily prove <span class="math inline">\(det(A) = det(A^T)\)</span> and <a href="https://proofwiki.org/wiki/Determinant_of_Transpose" target="_blank" rel="noopener">here</a> is another brief proof.</p><h3 id="less-intuitive-concepts-and-conclusions">Less Intuitive Concepts and Conclusions</h3><ul><li><p>Interpret the trace of matrices geometrically. <span class="math inline">\(Trace(AB) = Trace(BA)\)</span>.</p></li><li><p><span class="math inline">\(Rank(column \space space \space of \space A) = Rank(row \space space \space of \space A)\)</span></p></li><li><p><a href="https://en.wikipedia.org/wiki/Kernel_(linear_algebra)" target="_blank" rel="noopener">Kernel</a>, <a href="https://en.wikipedia.org/wiki/Image_(mathematics)" target="_blank" rel="noopener">Image</a> and <a href="https://en.wikipedia.org/wiki/Rank%E2%80%93nullity_theorem" target="_blank" rel="noopener">Rank Nullity Theorem</a></p><p>We have covered the concept of kernel in Null Spaces Chapter. If there is a <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> representing a linear transformation <span class="math inline">\(T: V \rightarrow W\)</span>, then the kernel of <span class="math inline">\(T\)</span> is defined as <span class="math display">\[ Kernel(T) = \{ \vec{v} \in V | T(\vec{v}) = \vec{0}\} \]</span> where <span class="math inline">\(\vec{0}\)</span> is the zero vector in <span class="math inline">\(W\)</span>.</p><p>While the image of <span class="math inline">\(T\)</span> or the range of <span class="math inline">\(T\)</span> is defined as <span class="math display">\[ Image(T) = \{T(\vec{v}) | \vec{v} \in V\} \]</span> Note, the image of <span class="math inline">\(T\)</span> is a subspace of the output space <span class="math inline">\(W\)</span> while the kernel of <span class="math inline">\(T\)</span> is a subspace of the input space <span class="math inline">\(V\)</span>. Here is a <a href="https://www.youtube.com/watch?v=vyYrvhbDhW4" target="_blank" rel="noopener">video</a> illustrating this.</p><p>And the <strong>Rank Nullity Theorem</strong> is stated as <span class="math display">\[ Rank(A) + dim(Kernel(T)) = n \]</span> Intuitively, we can think of <span class="math inline">\(n\)</span> as the number of dimensions in input space while <strong>rank</strong> means the number of dimensions in output space and <span class="math inline">\(dim(Kernel(T))\)</span> naturally is the dimension lost in performing the transformation <span class="math inline">\(T\)</span>. For example, for a <span class="math inline">\(1 \times 2\)</span> matrix <span class="math inline">\(A\)</span> that squishes a space into a number line, we know that there is one dimension of information missing in the 2D-to-1D transformation.</p></li><li><p>Solutions to Linear Systems</p><p>For a linear system <span class="math inline">\(Ax = b\)</span>, we can use the concept of rank and linear transformation to analyze if there is any solution to this system. From the perspective of linear transformation, <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(A\)</span> represents a transformation and <span class="math inline">\(x\)</span> is a vector in the input vector space and <span class="math inline">\(b\)</span> is another vector in the output vector space. The equation can be interpreted as the question <strong>if vector <span class="math inline">\(b\)</span> is within the output space of <span class="math inline">\(A\)</span></strong>. Mathematically speaking, that is <span class="math display">\[ rank([A \space b]) == rank(A) \]</span> where <span class="math inline">\([A \space b]\)</span> is an augmented matrix. If <span class="math inline">\(rank([A \space b]) &gt; rank(A)\)</span>, it means <span class="math inline">\(b\)</span> is out of space and there is no solution to that equation. If <span class="math inline">\(rank([A \space b]) = rank(A)\)</span>, it means <span class="math inline">\(b\)</span> is within space and there is one or many solution to that equation. As for it's a scenario with one solution or many solutions, it depends on <span class="math inline">\(rank(A) == n\)</span>. If <span class="math inline">\(rank(A)==n\)</span>, then there is only one solution. Otherwise, there are infinite solutions.</p></li><li><p>Characteristic Polynomial of Square Matrices</p><p>The characteristic polynomial of a square matrix is a polynomial which is invariant under matrix similarity and has the eigenvalues as roots. <span class="math display">\[ f_A(\lambda) = |\lambda I - A| \]</span> For <span class="math inline">\(B = Q^{-1}A Q\)</span>, the characteristic polynomial is <span class="math display">\[ f_B(\lambda) = |\lambda I - Q^{-1}A Q| = |Q^{-1}(\lambda I - A)Q| = |\lambda I - A| \]</span> which is the same as <span class="math inline">\(A\)</span>'s.</p></li></ul><h4 id="reference">Reference</h4><ol type="1"><li><a href="https://www.quora.com/What-is-the-geometric-interpretation-of-the-transpose-of-a-matrix" target="_blank" rel="noopener">What is the geometric interpretation of the transpose of a matrix?</a></li><li><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab" target="_blank" rel="noopener">Essence of Linear Algebra</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;announcements&quot;&gt;Announcements&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Contents below are basically notes of &lt;a href=&quot;https://www.youtube.com/watch?v=fNk_zzaMo
      
    
    </summary>
    
      <category term="Math" scheme="http://yoursite.com/categories/Math/"/>
    
      <category term="Engineering" scheme="http://yoursite.com/categories/Math/Engineering/"/>
    
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
      <category term="Linear Algebra" scheme="http://yoursite.com/tags/Linear-Algebra/"/>
    
  </entry>
  
  <entry>
    <title>Singular Value Decomposition</title>
    <link href="http://yoursite.com/2020/05/02/SVD/"/>
    <id>http://yoursite.com/2020/05/02/SVD/</id>
    <published>2020-05-03T00:48:35.000Z</published>
    <updated>2020-08-19T23:33:33.032Z</updated>
    
    <content type="html"><![CDATA[<h2 id="singular-value-decomposition">Singular Value Decomposition</h2><p>Singular Value Decomposition (SVD) is widely used in many applications such as Data Reduction , Data-Driven Generation of Fourier transform (FFT), Recommendation Algorithms, Facial Recognition and also can derive other algorithms like PCA.</p><h3 id="derivation">Derivation</h3><p>We give a data matrix consisting of a bunch of column vectors <span class="math inline">\({x_k} \in \mathbb{R}^n\)</span> for <span class="math inline">\(k=\{1, 2, .., m\}\)</span> as below, where <span class="math inline">\(x_k\)</span> can be a vector summarizing the facial characteristics of a person <span class="math inline">\(k\)</span> or a vector describing what a snapshot looks like at time <span class="math inline">\(k\)</span>. <span class="math display">\[ X = \begin{bmatrix} | &amp; | &amp; ... &amp; | \\ x_1 &amp; x_2 &amp; ... &amp; x_m \\ | &amp; | &amp; ... &amp; | \\ \end{bmatrix} \]</span> And what SVD does is decompose the above data matrix into a product of three other matrices <span class="math display">\[ X = U \Sigma V^T \]</span> where left singular vectors <span class="math inline">\(U\)</span> and right singular vectors <span class="math inline">\(V\)</span> are unitary matrices, and <span class="math inline">\(\Sigma\)</span> is a diagonal matrix. Specifically, we have <span class="math display">\[ \begin{align} X &amp;= \begin{bmatrix} | &amp; | &amp; ... &amp; | \\ u_1 &amp; u_2 &amp; ... &amp; u_n \\ | &amp; | &amp; ... &amp; | \\ \end{bmatrix} \begin{bmatrix} \sigma_1 &amp; &amp; &amp; \\ &amp; \sigma_2 &amp; &amp; \\ &amp; &amp; ... &amp; \\ &amp; &amp; &amp; \sigma_m \\ &amp; &amp; \hline\huge 0 &amp; \\ &amp; &amp; &amp; \\ \end{bmatrix} \begin{bmatrix} - &amp; v_1^T &amp; - \\ - &amp; v_2^T &amp; - \\ &amp; ... &amp; \\ - &amp; v_m^T &amp; - \\ \end{bmatrix} \\ \end{align} \]</span> where unitary matrices means <span class="math inline">\(U U^T = U^T U = I_{n \times n}\)</span> and <span class="math inline">\(V V^T = V^T V = I_{m \times m}\)</span>.</p><p>The elements <span class="math inline">\(\sigma_k\)</span> in <span class="math inline">\(\Sigma\)</span> are so-called singular values, and they are ordered like <span class="math inline">\(\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_m\)</span> by importance. At the same time, the importance is also reflected in <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> where <span class="math inline">\(u_1\)</span> is more important than <span class="math inline">\(u_2\)</span> in representing <span class="math inline">\(n\)</span> rows of <span class="math inline">\(X\)</span> because of their corresponding singular values <span class="math inline">\(\sigma_1\)</span> and <span class="math inline">\(\sigma_2\)</span>.</p><p>Intuitively, if <span class="math inline">\(x_1\)</span> represents one person's face, we can interpret <span class="math inline">\(U\)</span> as &quot;eigen&quot; faces which will be reshaped into those original faces, while the column vectors of <span class="math inline">\(V^T\)</span> along with singular values <span class="math inline">\(\sigma\)</span> serve as the reshaping factor for all <span class="math inline">\(u_1, u_2, ..., u_m\)</span> to make <span class="math inline">\(x_1, x_2, ..., x_m\)</span>.</p><p>And SVD can be done to any matrices and <span class="math inline">\(U\)</span>, <span class="math inline">\(\Sigma\)</span>, <span class="math inline">\(V\)</span> are unique and guaranteed to exist.</p><h3 id="matrix-approximation">Matrix Approximation</h3><p>Let's do introduce another form of SVD assuming <span class="math inline">\(n &gt;&gt; m\)</span> when there are a lot more measurements/observations $$ <span class="math display">\[\begin{align} X &amp;= \begin{bmatrix} | &amp; | &amp; ... &amp; | \\ u_1 &amp; u_2 &amp; ... &amp; u_n \\ | &amp; | &amp; ... &amp; | \\ \end{bmatrix} \begin{bmatrix} \sigma_1 &amp; &amp; &amp; \\ &amp; \sigma_2 &amp; &amp; \\ &amp; &amp; ... &amp; \\ &amp; &amp; &amp; \sigma_m \\ &amp; &amp; \hline\huge 0 &amp; \\ &amp; &amp; &amp; \\ \end{bmatrix} \begin{bmatrix} - &amp; v_1^T &amp; - \\ - &amp; v_2^T &amp; - \\ &amp; ... &amp; \\ - &amp; v_m^T &amp; - \\ \end{bmatrix} \\ &amp;= \sigma_1 u_1 v_1^T + \sigma_2 u_2 v_2^T + ... + \sigma_m u_m v_m^T \\ &amp;= \hat U \hat \Sigma {\hat V} ^T\\ \end{align}\]</span> $$ This is usually called economy SVD.</p><p>Recall that SVD is expressed as decomposing <span class="math inline">\(X\)</span> into 2 sets of orthogonal bases (<span class="math inline">\(u_1, u_2,..., u_m\)</span> and <span class="math inline">\(v_1, v_2,..., v_m\)</span>) and a singular values diagonal matrix. With the economy SVD formula, we can tell SVD can also be expressed as a sum of a series of rank-1 matrices (rank-1 matrices means <span class="math inline">\(\sigma_k u_k v_k^T\)</span>). This is called the <a href="https://www2.math.uconn.edu/~leykekhman/courses/MATH3795/Lectures/Lecture_9_Linear_least_squares_SVD.pdf#page=7" target="_blank" rel="noopener">dyadic decomposition</a> of <span class="math inline">\(A\)</span>, which decomposes the matrix <span class="math inline">\(A\)</span> of rank <span class="math inline">\(r\)</span> into sum of <span class="math inline">\(r\)</span> matrices of rank 1.</p><p>Naturally, as more rank-1 matrices are summed up, the summation increasingly improves the approximation of <span class="math inline">\(X\)</span>. Hence, if we truncate <span class="math inline">\(\hat U, \hat \Sigma, \hat V\)</span> at rank <span class="math inline">\(r\)</span> and chop the summation off to only keep the first <span class="math inline">\(r\)</span> columns of <span class="math inline">\(\hat U\)</span>, a <span class="math inline">\(r \times r\)</span> submatrix of <span class="math inline">\(\hat \Sigma\)</span> and the first <span class="math inline">\(r\)</span> rows of <span class="math inline">\(\hat V^T\)</span>, then we have an rank-r matrix <span class="math display">\[ X \approx \tilde U \tilde \Sigma \tilde V^T \]</span> We can conclude <span class="math inline">\(\tilde U \tilde \Sigma \tilde V^T\)</span> is the best estimator of <span class="math inline">\(X\)</span> with rank <span class="math inline">\(r\)</span>. Specifically, this conclusion is given by <a href="https://en.wikipedia.org/wiki/Low-rank_approximation" target="_blank" rel="noopener">Eckart-Young Theorem</a> <span class="math display">\[ \underset{\tilde x \space s.t. \space rank(\tilde x) = r}{\operatorname{argmin}} || X - \tilde X||_F = \tilde U \tilde \Sigma \tilde V^T \]</span> where <span class="math inline">\(F\)</span> represents the Frobenius Norm for matrices (<span class="math inline">\(||A||_F = \sqrt{\sum_{i, j}(A_{i,j})^2}\)</span>). It guarantees the best possible matrix approximation to <span class="math inline">\(X\)</span> at rank <span class="math inline">\(r\)</span> is given by the first <span class="math inline">\(r\)</span> truncated SVD. <strong>Note: </strong>Since truncated <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are not square matrices anymore, <span class="math inline">\(\tilde U \tilde U^T = I\)</span> and <span class="math inline">\(\tilde V \tilde V^T = I\)</span> does not hold true anymore.</p><h3 id="svd-in-dominant-correlations">SVD in Dominant Correlations</h3><p>If we compute the covariance matrix of <span class="math inline">\(X\)</span>, we can get <span class="math display">\[ X^T X = V \hat \Sigma \hat U^T \hat U \hat \Sigma V^T = V \hat \Sigma^2 V^T \]</span> which follows the eigen-decomposition form of <span class="math inline">\(X^T X V = V \Sigma^2\)</span>. Therefore, the right singular vectors <span class="math inline">\(V\)</span> become the eigenvectors of the covariance matrix of column-wise matrix <span class="math inline">\(X\)</span>. And the squared singular values make up the eigenvalues matrix <span class="math inline">\(\Sigma^2\)</span> .</p><p>If we do the same thing to another covariance matrix, we have <span class="math display">\[ X X^T = \hat U \hat \Sigma V^T V \hat \Sigma \hat U^T = \hat U \hat \Sigma^2 \hat U^T \]</span> Similarly, the left singular vectors <span class="math inline">\(U\)</span> become the eigenvectors of the covariance matrix of row-wise matrix <span class="math inline">\(X^T\)</span>. This covariance matrix calculates covariance among observations. And the eigenvalues matrix is the same as above.</p><p>Something about <span class="math inline">\(\hat U\)</span> and <span class="math inline">\(\hat V\)</span></p><p>Sometimes when <span class="math inline">\(X\)</span> represents a huge dataset, one issue here is that <span class="math inline">\(\hat U\)</span> is hard to compute because <span class="math inline">\(X X^T\)</span> is a really huge matrix. But we can use the property that <span class="math inline">\(X X^T\)</span> and <span class="math inline">\(X X^T\)</span> shared the same eigenvalues matrix <span class="math inline">\(\Sigma\)</span> to compute <span class="math inline">\(\hat U = X V \hat \Sigma^{-1}\)</span>.</p><p>Unitary Transformations</p><p>Derived from <span class="math inline">\(X = \hat U \hat \Sigma V^T\)</span>, we have <span class="math inline">\(X V = \hat U \hat \Sigma\)</span> or <span class="math inline">\(\hat U^T X =\hat \Sigma V^T\)</span> which explains the unitary transformations between two eigen bases. <span class="math inline">\(m\)</span> dimensional eigen bases <span class="math inline">\(V\)</span> multiplying by <span class="math inline">\(X\)</span> are mapped into <span class="math inline">\(n\)</span> dimensional eigen bases <span class="math inline">\(\hat U\)</span>. And <span class="math inline">\(\hat U\)</span> can also be mapped into <span class="math inline">\(V\)</span>.</p><h3 id="svd-in-solving-linear-systems-of-equations">SVD in Solving Linear Systems of Equations</h3><p>Below is a super normal linear system <span class="math display">\[ A x = b \]</span> where <span class="math inline">\(A\)</span> is a known <span class="math inline">\(n \times m\)</span> matrix, <span class="math inline">\(b\)</span> is a known <span class="math inline">\(n \times 1\)</span> vector and <span class="math inline">\(x\)</span> is an unknown <span class="math inline">\(m \times 1\)</span> vector.</p><p>When <span class="math inline">\(n &lt; m\)</span>, the linear system is underdetermined as below <span class="math display">\[ \begin{bmatrix} | &amp; | &amp; ... &amp; | \\ A_1 &amp; A_2 &amp; ... &amp; A_m \\ | &amp; | &amp; ... &amp; | \\ \end{bmatrix} \begin{bmatrix} x_1\\ x_2 \\ x_3\\ x_4 \\ ... \\ x_m\\ \end{bmatrix} = \begin{bmatrix} b_1\\ b_2 \\ ... \\ b_n\\ \end{bmatrix} \]</span> and there are many solutions <span class="math inline">\(x\)</span> given <span class="math inline">\(b\)</span> because there is much more freedom among <span class="math inline">\(x\)</span> than among <span class="math inline">\(b\)</span> to uniquely determine the values of <span class="math inline">\(x\)</span>.</p><p>When <span class="math inline">\(n &gt; m\)</span>, the linear system is overdetermined as below <span class="math display">\[ \begin{bmatrix} | &amp; | &amp; ... &amp; | \\ | &amp; | &amp; ... &amp; | \\ A_1 &amp; A_2 &amp; ... &amp; A_m \\ | &amp; | &amp; ... &amp; | \\ | &amp; | &amp; ... &amp; | \\ \end{bmatrix} \begin{bmatrix} x_1\\ ... \\x_m\\\end{bmatrix} = \begin{bmatrix} b_1\\ b_2 \\ b_3 \\ ... \\b_n\\\end{bmatrix} \]</span> and there are generally zero solutions <span class="math inline">\(x\)</span> given <span class="math inline">\(b\)</span> because there is not enough freedom for <span class="math inline">\(x\)</span> to satisfy all these <span class="math inline">\(b\)</span> constrictions.</p><p>What SVD allows us to do is approximately inverts <span class="math inline">\(A\)</span> to compute so-called <strong>pseudo inverse matrix</strong> <span class="math inline">\(A^{\dagger}\)</span> to give a best <span class="math inline">\(x\)</span> as the solution to the above linear system when <span class="math inline">\(n \neq m\)</span>. Below is the solution <span class="math display">\[ x = A^{\dagger} b \]</span> where <span class="math inline">\(A^{\dagger} = V \hat \Sigma^{-1} \hat U^T\)</span> is referred to as <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse" target="_blank" rel="noopener">Moore–Penrose</a> (left) pseudo inverse derived from <span class="math inline">\(A = \hat U \hat \Sigma V^T\)</span>. And this pseudo matrix <span class="math inline">\(A^{\dagger}\)</span> has these optimal properties under different circumstances as below</p><ul><li><span class="math inline">\(A^{\dagger}\)</span> has the minimum 2-norm <span class="math inline">\(min ||x||_2, s.t. Ax = b\)</span> among all possible solutions when the linear system is underdetermined.</li><li><span class="math inline">\(A^{\dagger}\)</span> minimizes the error <span class="math inline">\(min||Ax - b||_2, s.t.Ax=b\)</span> when the linear system is overdetermined, which is also the most common OLS.</li></ul><h3 id="least-squares-regression-and-svd">Least Squares Regression and SVD</h3><p>Recall that if there exists a solution to the linear system <span class="math inline">\(Ax = b\)</span>, it basically means <span class="math inline">\(b\)</span> is within the span of the columns of <span class="math inline">\(A\)</span>, considering <span class="math inline">\(Ax\)</span> simply means the linear combinations of column vectors of <span class="math inline">\(A\)</span>. Now let's look at new linear combinations obtained by SVD as below <span class="math display">\[ A \tilde x = A A^{\dagger} b = \hat U \hat \Sigma V^T V \hat \Sigma^{-1} \hat U^T b = \hat U \hat U^T b \]</span> Even though the estimate <span class="math inline">\(\tilde x\)</span> does not equal to <span class="math inline">\(b\)</span> (because truncated <span class="math inline">\(\hat U\)</span> makes <span class="math inline">\(\hat U \hat U^T \neq I\)</span>), it is still the best least square solution projecting <span class="math inline">\(b\)</span> onto the span of the columns of <span class="math inline">\(U\)</span> which is also the span of the columns of <span class="math inline">\(A\)</span>.</p><p>Now consider a one-dimensional example <span class="math inline">\(ax = b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are <span class="math inline">\(n \times 1\)</span> vector. If there is a unique solution to this equation, then <span class="math inline">\((a, b)\)</span> constitutes a straight line and <span class="math inline">\(x\)</span> is then the slope of this line. But the often case is that these <span class="math inline">\((a, b)\)</span> dots do not lie on the same line, thus making SVD useful to find a best possible line to fit these <span class="math inline">\((a, b)\)</span> dots.</p><figure><img src="SVD_LS.JPG" alt="SVD_LS"><figcaption>SVD_LS</figcaption></figure><p>As the above graph shows, the true line represents <span class="math inline">\(ax = b\)</span> while in reality there are observation errors in measuring <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> and SVD finds the regression/fitted line based on theses noisy <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p><h3 id="principal-component-analysis-pca-and-svd">Principal Component Analysis (PCA) and SVD</h3><p>PCA is a <strong>projection method</strong> that uses an orthogonal transformation to convert a set of observations of <strong>possibly correlated variables</strong> into a set of values of <strong>linearly uncorrelated variables</strong> called principal components (PCs).</p><p>Intuitively, PCA converts observations of variables into a set of PCs to independently represent the original data with different importance. So we can use some of PCs to reduce data dimension while keeping the information of data as much as possible.</p><p>(The PC searching process is sequential, in each step, PCA identifies the axis that accounts for the largest amount of variance once the residuals are projected on it.)</p><h4 id="derivation-1">Derivation</h4><p>PCA's goal is to find orthogonal directions to represent the original data as much as possible, so we can naturally think of variance as a measure of data representation. If one direction carries the largest variance projected by the original data, this direction can represent the original data to the most extent. Let's first do some derivation <span class="math display">\[ S = \frac{1}{n - 1} B^T B \]</span> where <span class="math inline">\(B = X - \bar X\)</span> is the demeaned <span class="math inline">\(X\)</span> , <span class="math inline">\(\bar X\)</span> is a <span class="math inline">\(n \times 1\)</span> vector whose <span class="math inline">\(i_{th}\)</span> element is the mean of <span class="math inline">\(i_{th}\)</span> row of <span class="math inline">\(X\)</span> and dividing by <span class="math inline">\(n - 1\)</span> is a typical way to correct for the bias introduced by using the sample mean instead of the true population mean.</p><p>Now consider the projection of a vector <span class="math inline">\(x \in \mathbb R^m\)</span> onto another vector/direction <span class="math inline">\(v_i\)</span> is simply the dot product <span class="math inline">\(v_i^T x\)</span>, we can then write the variance of the data projected onto vector <span class="math inline">\(v_1\)</span> as <span class="math display">\[ \frac{1}{n - 1} v_1^T (X - \bar X)^T (X - \bar X) v_1 = v_1^T S v_1 \]</span> and when this projected variance reaches to the maximum, <span class="math inline">\(v_1 = \underset{||v_1||=1}{\operatorname{argmax}} v_1^T S v_1\)</span> finds the first principal component (Note: <span class="math inline">\(v_1\)</span> itself is not a principal component). Then we continue this process by projecting <span class="math inline">\(B\)</span> onto other directions <span class="math inline">\(v_2\)</span> vertical to <span class="math inline">\(v_1\)</span> then onto <span class="math inline">\(v_3\)</span> vertical to <span class="math inline">\(v_2\)</span>. Finally, we will find the first <span class="math inline">\(v_k\)</span> are the first <span class="math inline">\(k\)</span> eigenvectors of <span class="math inline">\(S\)</span> corresponding to the first <span class="math inline">\(k\)</span> largest eigenvalues. This follows the eigen-decomposition format <span class="math display">\[ S V = V D \]</span> where <span class="math inline">\(V\)</span> is the eigenvector matrix of covariance matrix <span class="math inline">\(S\)</span> and <span class="math inline">\(D\)</span> is the eigenvalue matrix. And if we use SVD to the data matrix <span class="math inline">\(B\)</span>, we will note the eigenvector <span class="math inline">\(V\)</span> of covariance matrix <span class="math inline">\(S\)</span> equals to the right singular vector <span class="math inline">\(V\)</span>, and the singular value is proportional to the square root of the eigenvalues of <span class="math inline">\(S\)</span>. <span class="math display">\[ B = U \Sigma V^T \\ S = \frac{1}{n - 1} B^T B = \frac{1}{n - 1} V \Sigma^T \Sigma V^T \\ S V = V \frac{\Sigma \Sigma^T}{n - 1} \]</span> So now we have several ways to compute the principal components <span class="math display">\[ T = B V = U \Sigma \]</span> where <span class="math inline">\(T\)</span> represents the principal components matrix while <span class="math inline">\(V\)</span> can be obtained using SVD to <span class="math inline">\(B\)</span> or eigen-decomposition to <span class="math inline">\(S\)</span>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;singular-value-decomposition&quot;&gt;Singular Value Decomposition&lt;/h2&gt;&lt;p&gt;Singular Value Decomposition (SVD) is widely used in many applicat
      
    
    </summary>
    
      <category term="Math" scheme="http://yoursite.com/categories/Math/"/>
    
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
      <category term="Linear Algebra" scheme="http://yoursite.com/tags/Linear-Algebra/"/>
    
      <category term="Data Engineering" scheme="http://yoursite.com/tags/Data-Engineering/"/>
    
  </entry>
  
  <entry>
    <title>A Mysterious Algorithm used in Apollo 11 Guidance</title>
    <link href="http://yoursite.com/2020/04/23/kalman-filter/"/>
    <id>http://yoursite.com/2020/04/23/kalman-filter/</id>
    <published>2020-04-24T01:05:54.000Z</published>
    <updated>2020-08-19T23:24:59.971Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kalman-filter">Kalman Filter</h2><h3 id="state-space-models">0. State Space Models</h3><p>In <a href="https://en.wikipedia.org/wiki/State-space_representation#Example:_continuous-time_LTI_case" target="_blank" rel="noopener">Wikipedia</a>, a state space model is described as a set of input, output and state variables mixed with some ODE systems. I have no idea what that is with this abstract definition. State variables</p><p>can also be known as hidden states evolving through time in a way that we don't know how but we would like to use some <strong>dynamics</strong> and <strong>observations</strong> to fit the evolution path.</p><p><strong>An Example</strong></p><p>Let's look at a random walk example below,</p><p>Say we model some hidden states with two parts, which are its last state and some random noise. Also, we give the mapping relationship from <span class="math inline">\(x_t\)</span> to <span class="math inline">\(z_t\)</span> as below. <span class="math display">\[ x_t = x_{t - 1} + w_{t - 1}, w_{t} \sim N(0, Q), \\ z_t = x_t + \epsilon_{t}, \epsilon_{t} \sim N(0, R) \]</span></p><p>where <span class="math inline">\(x_t\)</span> is the state variable and <span class="math inline">\(z_t\)</span> is the observation value, and <span class="math inline">\(w_t\)</span>, <span class="math inline">\(\epsilon_{t}\)</span> are independent normal random noise.</p><p><img src="kalman-filter//random_walk.png" align="center/"></p><figure><img src="random_walk.png" alt="Random walk"><figcaption>Random walk</figcaption></figure><p>Basically, we have a state space model about <span class="math inline">\(x_t\)</span>, and we would like to know how exactly <span class="math inline">\(x_t\)</span> will evolve in future steps. From the equations above, we can also estimate <span class="math inline">\(\hat x_t\)</span> with current <span class="math inline">\(\hat x_{t - 1}\)</span> and <span class="math inline">\(z_{t-1}\)</span>.</p><p><strong>Probabilistic State Space Models</strong></p><p>But usually, a point estimate is less significant than an interval estimate. So here comes Probabilistic State Space Models.</p><p>With the same example above, we can know the distribution of <span class="math inline">\(x_t\)</span> given <span class="math inline">\(x_{t-1}\)</span>and the distribution of observations <span class="math inline">\(z_t\)</span> given <span class="math inline">\(x_{t}\)</span>. <span class="math display">\[ p(x_t | x_{t - 1}) = \frac{1}{\sqrt{2 \pi Q}}\exp \left( -\frac{1}{2Q}(x_t - x_{t - 1})^2 \right) \\ p(z_t | x_t) = \frac{1}{\sqrt{2 \pi R}}\exp \left( -\frac{1}{2R}(z_t - x_t)^2 \right) \]</span></p><p>where <span class="math inline">\(p(\cdot)\)</span> represents a probability density function.</p><p><strong>Assumptions</strong></p><p>Typically, we use <span class="math inline">\(p(x_t | x_{t - 1})\)</span> instead of <span class="math inline">\(p(x_t | x_{1},x_{2},...,x_{t - 1})\)</span> to describe the conditional distribution is due to the Markovian assumption, which is <strong>future states are independent from the past states given the present.</strong> <span class="math display">\[ p(x_t | x_{t - 1}) = p(x_t | x_{1},x_{2},...,x_{t - 1}, z_{1},z_{2},...,z_{t - 1}) \]</span> Also, we can also naturally think of the relationship between states and observations. Obviously, <strong>observations only depend on its current states</strong> instead of past observations or past states. <span class="math display">\[ p(z_t | x_t) = p(z_t | x_{1},x_{2},...,x_{t}, z_{1},z_{2},...,z_{t - 1}) \]</span> Simpler forms of these assumptions are as below <span class="math display">\[ p(x_t | x_{1:t-1}, z_{1:t}) = p(x_t | x_{t - 1}) \\ p(z_t | x_{1:t}, z_{1:t - 1}) = p(z_t | x_t) \]</span></p><h3 id="bayesian-filter">1. Bayesian Filter</h3><p>Before we talk about Bayesian Filter, let's focus on what filtering is. Filtering is actually one of the statistical inference methods</p><ul><li><strong><em>Filtering</em></strong>. Filtering means to recover the state variable <span class="math inline">\(x_t\)</span> given <span class="math inline">\(z_{1:t}\)</span>, that is, to remove the measurement errors from the data.</li><li><strong><em>Prediction</em></strong>. Prediction means to forecast <span class="math inline">\(x_{t+h}\)</span> or <span class="math inline">\(z_{t+h}\)</span> for <span class="math inline">\(h&gt;0\)</span> given <span class="math inline">\(z_{1:t}\)</span>, where <span class="math inline">\(t\)</span> is the forecast origin.</li><li><strong><em>Smoothing</em></strong>. Smoothing is to estimate <span class="math inline">\(x_t\)</span> given <span class="math inline">\(z_{1:T}\)</span>, where <span class="math inline">\(T&gt;t\)</span>.</li></ul><p>From <a href="https://en.wikipedia.org/wiki/Bayesian_inference" target="_blank" rel="noopener">Bayesian inference</a>, the probability for an event is updated as more evidence or information becomes available, so we would like to use Bayes rules to <strong>simulate</strong> the probability distribution of hidden states <span class="math inline">\(x_t\)</span> with incoming observations. <span class="math display">\[ \begin{align*} p(x_t |z_{1:t}) &amp;= \frac{p(x_t, z_{1:t})}{p( z_{1:t})}\\ &amp;= \frac{p(z_t | z_{1:t-1}, x_t) p(x_t |z_{1:t-1}) p(z_{1:t-1})}{p(z_{1:t})}, \space where \space p(z_t | z_{1:t-1}, x_t) = p(z_t | x_t) \\ &amp;= \eta p(z_t | x_t) p(x_t | z_{1:t-1}), \space where \space\space \eta = p(z_t | z_{1:t-1})\\ &amp;= \eta p(z_t | x_t) \int{p(x_t | x_{t-1}, z_{1:t-1})p(x_{t-1} | z_{1:t-1})dx_{t-1}}\\ &amp;= \eta p(z_t | x_t) \int{p(x_t | x_{t-1})p(x_{t-1} | z_{1:t-1})dx_{t-1}}\\ \end{align*} \]</span> Here comes a recursion formula if we replace <span class="math inline">\(p(x_t |u_{1:t})\)</span> with <span class="math inline">\(Bel(x_t)\)</span> meaning the posterior pdf of <span class="math inline">\(x_t\)</span> at time <span class="math inline">\(k\)</span>, so we have <span class="math display">\[ Bel(x_t) = \eta p(z_t|x_t) \int p(x_t|x_{t-1})Bel(x_{t-1})dx_{t-1} \]</span> where <span class="math inline">\(p(z_t|x_t)\)</span> and <span class="math inline">\(p(x_t|x_{t-1})\)</span> correspond exactly to the 2 hypothetical equations discussed in the random walk example, which are <strong>Observations-States Mapping</strong> and <strong>State Dynamics</strong>. Intuitively, we need state dynamics to make apriori estimates (<span class="math inline">\(\int{p(x_t | x_{t-1})p(x_{t-1} | z_{1:t-1})dx_{t-1}}\)</span>) and observations to make likelihood adjustments, thus making the Bayesian rules complete. The slides of <a href="https://users.aalto.fi/~ssarkka/course_k2016/handout3.pdf#page=12" target="_blank" rel="noopener">Bayesian Filtering Equations andKalman Filter from Simo Särkkä in Aalto University</a> shows how the distribution of <span class="math inline">\(x_{t}\)</span> evolves under Bayesian rules.</p><p><strong>A New Input</strong></p><p>Now we introduce another control variable <span class="math inline">\({u_t}\)</span> which may serve as an additional impact to <span class="math inline">\(x_t\)</span>, and we can now imagine the state model has the new form like below: <span class="math display">\[ x_t = x_{t - 1} + u_t + w_{t - 1}, w_t \sim N(0, Q), \\ z_t = x_t + \epsilon_t, \epsilon_t \sim N(0, R) \]</span></p><p>Also, we cast those 2 assumptions to <span class="math inline">\(u_t\)</span>, then we have <span class="math display">\[ p(x_t | x_{1:t-1}, u_{1:t}, z_{1:t}) = p(x_t | x_{t - 1}, u_t) \\ p(z_t | x_{1:t}, u_{1:t}, z_{1:t - 1}) = p(z_t | x_t) \]</span></p><p>The target conditional probability distribution has now become posterior distribution <span class="math inline">\(p(x_t | u_{1:t}, z_{1:t})\)</span>, we would like to know how the current state behaves given the most recent series of observations and control variables. <span class="math display">\[ \begin{align*} p(x_t |u_{1:t}, z_{1:t}) &amp;= \frac{p(x_t, u_{1:t}, z_{1:t})}{p(u_{1:t}, z_{1:t})}\\ &amp;= \frac{p(z_t | u_{1:t}, z_{1:t-1}, x_t) p(x_t | u_{1:t}, z_{1:t-1}) p(u_{1:t}, z_{1:t-1})}{p(u_{1:t}, z_{1:t})}\\ &amp;= \eta p(z_t | x_t) p(x_t | u_{1:t}, z_{1:t-1}), \space where \space\space \eta = p(z_t | u_{1:t}, z_{1:t-1})\\ &amp;= \eta p(z_t | x_t) \int{p(x_t | x_{t-1}, u_{1:t}, z_{1:t-1})p(x_{t-1} | u_{1:t}, z_{1:t-1})dx_{t-1}}\\ &amp;= \eta p(z_t | x_t) \int{p(x_t | x_{t-1}, u_t)p(x_{t-1} | u_{1:t-1}, z_{1:t-1})dx_{t-1}}\\ \end{align*} \]</span> Here comes a recursion formula if we replace <span class="math inline">\(p(x_t |u_{1:t}, z_{1:t})\)</span> with <span class="math inline">\(Bel(x_t)\)</span> meaning the posterior pdf of <span class="math inline">\(x_t\)</span> at time <span class="math inline">\(t\)</span>, so we have <span class="math display">\[ Bel(x_t) = \eta p(z_t|x_t) \int p(x_t|x_{t-1}, u_t) Bel(x_{t-1})dx_{t-1} \]</span> which is all the same as the previous formula except <span class="math inline">\(u_t\)</span>. Now we can separate this recursion formula into two parts <span class="math display">\[ \overline {Bel}{(x_t)} = \int p(x_t|x_{t-1}, u_t) Bel(x_{t-1})dx_{t-1} \\ Bel(x_t) = \eta p(z_t|x_t)\overline {Bel}{(x_t)} \]</span></p><p>state prediction and state update respectively. We can say <span class="math inline">\(x_t\)</span> is predicted with new inputs of control variables <span class="math inline">\(u_t\)</span> and updated with corresponding observations <span class="math inline">\(z_t\)</span>.</p><p>Why we need a new input <span class="math inline">\(u_t\)</span>? Simply speaking, we need some other factors rather than pure past states to explain the evolution of <span class="math inline">\(x_t\)</span>. To be more specific, Bayesian filter is widely used in probabilistic robotics where there is always robotics control (say robots are controlled to move 2 meters ahead) and the control itself influences a system's state (the state is the position of the robot).</p><h3 id="kalman-filter-1">2. Kalman Filter</h3><p>The recursion equation above indicates how the distribution of <span class="math inline">\(x_t\)</span> evolves through time. However we do not know what is the exact distribution of <span class="math inline">\(x_{t-1}\)</span> given a random initial distribution <span class="math inline">\(x_0\)</span>. If an initial random distribution is given, then we must follow the recursion formula to integrate over all possible <span class="math inline">\(x_{t-1}\)</span> to get an estimate.</p><p>Here comes the trick of Kalman Filter-using <strong>Gaussian</strong> distributions to describe <span class="math inline">\(x_t\)</span>. Because linear combinations of Gaussian distributions are still Gaussian, the states distribution evolution is easier to follow under Gaussian assumptions.</p><p><strong>Notations and Assumptions</strong> <span class="math display">\[ x_{t+1} = A_{t} x_{t} + w_t \\ z_t = H_t x_{t} + \epsilon_t \]</span></p><ul><li><span class="math inline">\(A_t\)</span> is the state transition matrix.</li><li><p><span class="math inline">\(H_t\)</span> is the observation-state mapping matrix.</p></li><li><span class="math inline">\(F_t = \{z_1, . . . , z_t \}\)</span> is the information available at time <span class="math inline">\(t\)</span> (inclusive).</li><li><span class="math inline">\(x_{t|j} = E(x_t|F_j)\)</span> is the conditional mean of <span class="math inline">\(x_t\)</span> given <span class="math inline">\(F_j\)</span>.</li><li><span class="math inline">\(z_{t|j} = E(z_t|F_j)\)</span> is the conditional mean of <span class="math inline">\(z_t\)</span> given <span class="math inline">\(F_j\)</span>.</li><li><span class="math inline">\(\Sigma_{t|j} = Var(x_t|F_j)\)</span> is the conditional variance of <span class="math inline">\(x_t\)</span> given <span class="math inline">\(F_j\)</span> and also the <span class="math inline">\(t-j\)</span> step state estimation error at the same time.</li><li><span class="math inline">\(v_t= z_t - z_{t|t-1}\)</span> is the 1-step-ahead forecast error.</li><li><span class="math inline">\(V_t= Var(v_t|F_{t-1})\)</span> is the variance of the 1-step-ahead forecast error, which is at the same time the unconditional variance <span class="math inline">\(Var(v_t)\)</span>.</li><li><p><span class="math inline">\(Cov(v_t, z_j) = E[E(v_t z_j|F_{t-1})] = E[z_j E(v_t |F_{t-1})] = 0, \space \space j &lt; t\)</span> indicates the 1-step-ahead forecast error is uncorrelated (and hence independent) with the past observations <span class="math inline">\(z_j\)</span>.</p></li></ul><p>It's easy to derive that <span class="math display">\[ E(v_t) = E[E(v_t|F_{t-1})] = E[E(z_t - z_{t|t-1}|F_{t-1})] = E(z_{t|t-1} - z_{t|t-1}) = 0 \\ \begin{align} V_t &amp;= Var(z_t - z_{t|t-1}|F_{t-1}) \\ &amp;= Var(H_t x_t + \epsilon_t - H_t x_{t|t-1} |F_{t-1}) \\ &amp;= H_t \Sigma_{t|t-1} H_t^T + \sigma^2_{\epsilon} \end{align} \]</span> which is to say <span class="math display">\[ v_t|F_{t-1} \sim N(0, V_t) \]</span> with <span class="math inline">\(x_t|F_{t-1} \sim N(x_{t|t-1}, \Sigma_{t|t-1})\)</span>, we know the joint distribution of <span class="math inline">\(x_t\)</span> and <span class="math inline">\(v_t\)</span> given <span class="math inline">\(F_{t-1}\)</span> is a multi-variate normal distribution. The remaining question is what is the conditional covariance between <span class="math inline">\(x_t\)</span> and <span class="math inline">\(z_t\)</span> given <span class="math inline">\(F_{t-1}\)</span>. <span class="math display">\[ \begin{align} Cov(x_t, v_t | F_{t-1}) &amp;= Cov(x_t, H_tx_t + \epsilon_t - H_t x_{t|t-1}| F_{t-1}) \\ &amp;= Cov(x_t, H_t(x_t - x_{t|t-1})| F_{t-1}) + Cov(x_t, \epsilon_t | F_{t-1}) \\ &amp;= Cov(x_t - x_{t|t-1}, H_t(x_t - x_{t|t-1})| F_{t-1}) \\ &amp;= \Sigma_{t|t-1} H_t^T \end{align} \]</span> So the joint distribution is $$ \begin{bmatrix} x_t \ v_t \end{bmatrix}<em>{F</em>{t-1}}</p><p></p><p>N(\begin{bmatrix} x_{t|t-1} \ 0 \end{bmatrix}, \begin{bmatrix} <em>{t|t-1} &amp; H_t </em>{t|t-1} \ <em>{t|t-1} H_t^T &amp; V_t \end{bmatrix}) <span class="math display">\[ And the goal of the Kalman filter is to update knowledge of the state variable recursively when new data become available. In other words, knowing the conditional distribution of $x_t$ given $F_{t−1}$ and the new observation $z_t$, we would like to obtain the conditional distribution of $x_t$ given $F_t$. Before that, let&#39;s first introduce some theorems about conditional mean and variance. Suppose that $x$, $y$, and $z$ are three random vectors such that their joint distribution is multivariate normal. In addition, assume that the diagonal block covariance matrix $\Sigma_{ww}$ is nonsingular for $w = x, y, z$, and $\Sigma_{yz} = 0$. Then, \]</span> 1. E(x|y) = <em>x + </em>{xy} </em>{yy}^{-1} (y - <em>y)\ 2. Var(x|y) = </em>{xx} - <em>{xx} </em>{yy}^{-1} <em>{yx} \ 3. E(x|y, z) = E(x|y) + </em>{xz} <em>{zz}^{-1} (z - <em>z) \ 4. Var(x|y, z) = Var(x|y) - </em>{xz} </em>{zz}^{-1} <em>{zx} <span class="math display">\[ And therefore we can get \]</span> x</em>{t|t} = E(x_t|F_{t-1}, v_t) = x_{t|t-1} + <em>{t|t-1} H_t^T V_t^{-1}v_t \ </em>{t|t} = Var(x_t|F_{t-1}, v_t) = <em>{t|t-1} - </em>{t|t-1} H_t^T V_t^{-1} H_t <em>{t|t-1} <span class="math display">\[ If we define $K_t = \Sigma_{t|t-1} H_t^T V_t^{-1}$, then we have \]</span> x</em>{t|t} = x_{t|t-1} + K_t v_t \ <em>{t|t} = (I - K_t H_t) </em>{t|t-1} $$</p><p>These are 2 filtering equations. Besides that, Kalman Filter also gives prediction based on the filtered results as below. <span class="math display">\[ x_{t+1|t} = E(A_t x_t + w_t|F_t) = A_t x_{t|t} \\ \Sigma_{t+1|t} = Var(x_t + w_t |F_t) = Var(x_t|F_t) + Var(w_t|F_t) = \Sigma_{t|t} + \sigma^2_{\epsilon} \]</span> With the initial condition <span class="math inline">\(\mu_1 \sim N(\mu_{1|0}, \Sigma_{1|0})\)</span>, the Kalman Filtering is ready for the above dynamics.</p><p><strong>Another way of deriving K from an optimization perspective</strong></p><p><span class="math inline">\(K_t\)</span> for <strong>Kalman Gain</strong> is how Kalman Filters extends from Bayesian Filter. The state filtering can be decomposed of <strong>apriori estimate</strong> plus a portion of <strong>forecast error</strong> which is</p><p><span class="math display">\[ Var(z_t - z_{t|t - 1}) = Var(H_t (x_t - x_{t|t-1}) + \epsilon_k) = H_t P_{t|t-1} H_t^T + \sigma^2_{\epsilon} \]</span> So we can express the portion of predicted observation error as <span class="math display">\[ K_t (z_t - z_{t|t-1}) = Var[x_t - x_{t|t-1}] H^T_t Var^{-1}[z_t - z_{t|t-1}] (z_t - z_{t|t-1}) \]</span> We can interpret <span class="math inline">\(K_t\)</span> as a compensation term for model forecast uncertainty.</p><p>A large <span class="math inline">\(K_t\)</span> means there is much more noise in state forecast (a relatively larger <span class="math inline">\(\Sigma_{t|t-1}\)</span>) than in observation forecast (a relatively smaller <span class="math inline">\(Var(z_t - z_{t|t-1})\)</span>), then we apply a large correction term to apriori estimate <span class="math inline">\(x_{t|t-1}\)</span> to approximate a more accurate posteriori <span class="math inline">\(x_{t|t}\)</span>. Otherwise, a small <span class="math inline">\(K_t\)</span> means the current forecast <span class="math inline">\(x_{t|t-1}\)</span> makes some sense so that a small correction is fine.</p><p>How is <span class="math inline">\(K_t\)</span> derived exactly? Follow the interpretation of <span class="math inline">\(K_t\)</span>, we can think of <span class="math inline">\(K_t\)</span> must be the result of minimizing the state estimation error <span class="math inline">\(\Sigma_{t|t}\)</span>. So let's express <span class="math inline">\(\Sigma_{t|t}\)</span> as below <span class="math display">\[ \begin {align*} \Sigma_{t|t} &amp;= Var(x_t - x_{t|t}) = Var(x_t - [x_{t|t-1} + K_t (z_t - z_{t|t-1})]) \\ &amp;= Var(x_t - [x_{t|t-1} + K_t (H_t x_t + \epsilon_t - H_t x_{t|t-1})]) \\ &amp;= Var(x_t - x_{t|t-1} - K_t H_t(x_t - x_{t|t-1}) - K_t \epsilon_t) \\ &amp;= Var((I - K_t H_t)(x_t - x_{k|k-1})) + Var(K_t \epsilon_t) \\ &amp;= (I - K_t H_t)\Sigma_{t|t-1}(I - K_t H_t)^T + K_t \sigma^2_{\epsilon} K_t^T\\ \end {align*} \]</span> Since <span class="math inline">\(V_t = Var(z_t - z_{t|t-1}) = H_t \Sigma_{t|t-1} H_t^T + \sigma^2_{\epsilon}\)</span>, then we have <span class="math display">\[ \Sigma_{t|t} = \Sigma_{t|t-1} - K_t H_t \Sigma_{t|t-1} - \Sigma_{t|t-1} H_t^T K_t^T + K_t V_t K_t^T \]</span> Now minimize the trace of <span class="math inline">\(P_{t|t}\)</span> to minimize the squared error, so we have <span class="math display">\[ \frac{\partial tr(P_{t|t})}{\partial K_t} = -2P_{t|t-1} H_t^T + 2K_t V_t = 0 \]</span> then we get <span class="math inline">\(K_t = P_{t|t-1} H_t^T V_t^{-1}\)</span></p><p><strong>Kalman Filter Derivation from an Intuitive Bayesian Perspective</strong></p><p>We stick to the recursion formula derived by Bayesian rules: <span class="math display">\[ Bel(x_t) = \eta p(z_t|x_t) \int p(x_t|x_{t-1},u_t) Bel(x_{t-1})dx_{t-1} \]</span> Let's give an initial condition <span class="math inline">\(Bel(x_0) = N(x_0, P_0)\)</span>, then for <span class="math inline">\(k=1\)</span> with state dynamics <span class="math inline">\(x_{t+1} = A_t x_{t} + B_{t+1} u_{t+1} + w_t\)</span>, we know <span class="math inline">\(p(x_1|x_{0},u_1) = N(A_0 x_0 + B_1 u_1, \sigma_{w,0}^2)\)</span>.</p><p>According to <a href="https://users.aalto.fi/~ssarkka/course_k2016/handout3.pdf#page=14" target="_blank" rel="noopener">convolution rules</a> in Gaussian distribution, we can get <span class="math display">\[ \overline {Bel}(x_1) = N(A_0 x_0 + B_1 u_1, A_0 \Sigma_{1|0} A_0^T + \sigma_{w,0}^2) \]</span> The same goes for any other <span class="math inline">\(t \geq 1\)</span> as the recursion holds true for growing <span class="math inline">\(t\)</span>, which is <span class="math display">\[ \overline {Bel}(x_t) = N(A_{t-1} x_{t-1} + B_t u_t, A_{t-1} \Sigma_{t|t-1} A_{t-1}^T + \sigma_{w,t-1}^2) \]</span> For the observation-state mapping part, we know <span class="math inline">\(p(z_t|x_t) = N(H_t x_t, \sigma_{\epsilon, t}^2)\)</span>, so the problem now becomes computing the distribution of the product of 2 Gaussian variables. This will definitely lead to the same results above like <span class="math display">\[ \eta N(H_t x_t, \sigma_{\epsilon,t}^2) N(A_{t-1} x_{t-1} + B_t u_t, A_{t-1} \Sigma_{t|t-1} A_{t-1}^T + \sigma_{w,t-1}^2) \\ = N(A_{t-1} x_{t-1} + B_t u_t + K_t v_t, (I - K_t H_t) \Sigma_{t|t-1}) \]</span> for now I have no idea how this can be derived rigorously, but <a href="https://pdfs.semanticscholar.org/6414/4a7b0b8dd5389463a6886b9dc3304203a7e4.pdf" target="_blank" rel="noopener">P.A. Bromiley, Products and Convolutions of Gaussian Probability Density Functions</a> introduces deriving distribution of the product of <span class="math inline">\(n\)</span> univariate Gaussian variables.</p><h3 id="applications">3. Applications</h3><p>Intuitively, I would say <em>Bayesian</em> formula establishes the relationship among posterior distributions, likelihood functions and priori distributions and therefore gives the posterior distribution estimate in a recursive way. This process of stepwise estimating and making corrections is how probability is interpreted and inferred over time in a <em>Bayesian</em> way. Based on this, <strong>Kalman Filter</strong> makes assumptions about the 1. dynamics of states and observations to adapt to the <em>Bayesian</em> framework and therefore get 2. the way of estimating states and prediction error matrices. At the same time, <strong>Kalman Filter</strong> seeks to estimate the states' posterior distributions in a way that the states' prediction errors are minimized. In short, this can be seen as using normal distributions assumptions on Bayesian Filters to get the best estimates of states. Let's talk about some examples</p><p><strong>Temperature Measurement</strong></p><p>We use simple dynamics to model the average temperature in a certain region in a period of time <span class="math display">\[ x_t = x_{t-1} + w_t, \space \space \space w_t \sim N(0, \sigma_e^2) \\ z_t = x_{t} + \epsilon_t, \space \space \space \epsilon_t \sim N(0, \sigma_{\epsilon}^2) \]</span></p><p>The initial value <span class="math inline">\(x_1\)</span> is either given or follows a known distribution, and it is independent of <span class="math inline">\(\{w_t\}\)</span> and <span class="math inline">\(\{\epsilon_t\}\)</span> for <span class="math inline">\(t &gt;0\)</span>.</p><p>It has another name which is the <strong>Local Trend Model</strong>. In the model, <span class="math inline">\(x_t\)</span> is simply a random walk. <span class="math inline">\(x_t\)</span> can be referred to as the trend of the temperature which is not directly observable, and <span class="math inline">\(z_t\)</span> is the temperature detected by some sensors with observational noise <span class="math inline">\(\epsilon_t\)</span>. The dynamic dependence of <span class="math inline">\(z_t\)</span> is governed by <span class="math inline">\(x_t\)</span>'s dynamics because <span class="math inline">\(\epsilon_t\)</span> is not serially correlated.</p><blockquote><p>The model can also be used to analyze realized volatility of an asset price. Here <span class="math inline">\(x_t\)</span> represents the underlying log volatility of the asset price and <span class="math inline">\(x_t\)</span> is the logarithm of realized volatility. The true log volatility is not directly observed but evolves over time according to a random-walk model. On the other hand, <span class="math inline">\(x_t\)</span> is constructed from high-frequency transactions data and subjected to the influence of market microstructure noises. The standard deviation of <span class="math inline">\(\epsilon_t\)</span> denotes the scale used to measure the impact of market microstructure noises.</p></blockquote><p><strong>Pair Trading</strong></p><p>We use simple dynamics to model the dynamic relationship among different securities over time <span class="math display">\[ \beta_t = \beta_{t-1} + \epsilon_t \\ p^A_t = \beta_t p^B_t + w_t \]</span></p><p>where $p^A_t $ and <span class="math inline">\(p^B_t\)</span> are the price series of the pair, <span class="math inline">\(\beta_t\)</span> is the time-varying hedge ratio, <span class="math inline">\(w_t\)</span> and <span class="math inline">\(\epsilon_t\)</span> are independent normal error terms.</p><blockquote><p>The second equation represents the observation equation and the first equation is the state transition equation. This model suggests that the hidden state, which is the hedge ratio, follows a random walk. In order to make an estimation of the hidden state with Kalman filter, the observation error terms and transition error terms need to be estimated and specified.</p></blockquote><p><strong>Macroeconomic Nowcasting</strong></p><p>There is a <a href="https://towardsdatascience.com/macroeconomic-nowcasting-with-kalman-filtering-557926dbc737" target="_blank" rel="noopener">blog</a> demonstrating how Kalman filter helps in macroeconomic indicators nowcasting.</p><p><strong>Reference</strong>:</p><ol type="1"><li><a href="https://www.cnblogs.com/ycwang16/p/5999034.html" target="_blank" rel="noopener">细说Kalman滤波：The Kalman Filter</a></li><li><a href="https://users.aalto.fi/~ssarkka/course_k2016/handout3.pdf" target="_blank" rel="noopener">Simo Särkkä, Bayesian Filtering Equations and Kalman Filter</a></li><li><a href="http://www.cs.cmu.edu/~16831-f14/notes/F14/16831_lecture02_prayana_tdecker_humphreh.pdf" target="_blank" rel="noopener">Statistical Techniques in Robotics (16-831, F10) Lecture #02 (Thursday, August 28)Bayes Filtering</a></li><li><a href="http://people.ciirc.cvut.cz/~hlavac/TeachPresEn/55AutonomRobotics/2015-05-04ReinsteinBayes-ekf.pdf" target="_blank" rel="noopener">Michal Reinštein, From Bayes to Extended Kalman Filter</a></li><li><a href="https://uwspace.uwaterloo.ca/handle/10012/12793" target="_blank" rel="noopener">High Frequency Statistical Arbitrage with Kalman Filter and Markov Chain Monte Carlo</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;kalman-filter&quot;&gt;Kalman Filter&lt;/h2&gt;&lt;h3 id=&quot;state-space-models&quot;&gt;0. State Space Models&lt;/h3&gt;&lt;p&gt;In &lt;a href=&quot;https://en.wikipedia.org/wiki/
      
    
    </summary>
    
      <category term="Math" scheme="http://yoursite.com/categories/Math/"/>
    
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
      <category term="Bayesian Inference" scheme="http://yoursite.com/tags/Bayesian-Inference/"/>
    
  </entry>
  
  <entry>
    <title>Canonical Component Analysis</title>
    <link href="http://yoursite.com/2019/10/20/CCA/"/>
    <id>http://yoursite.com/2019/10/20/CCA/</id>
    <published>2019-10-20T22:22:44.000Z</published>
    <updated>2020-08-01T16:14:23.871Z</updated>
    
    <content type="html"><![CDATA[<h2 id="a-quick-question">0. A Quick Question</h2><p>Let's first look at a question:</p><blockquote><p>Here is a portfolio risk report for 2 underlying yield sensitivity (like DV01)<br>- 10-year treasury yield sensitivity: - 100k / bp<br>- 30-year treasury yield sensitivity: + 100k / bp</p><p>How do we hedge this portfolio against these 2 yield risks?</p></blockquote><p>The most common thing one can think of is selling 30-year treasury bonds and buying 30-year treasury bonds with a hedge ratio derived from historical data regression to hedge the above risk exposure. This kind of regression or static hedging will easily break because of yield curve reshaping. In this case, CCA (and PCA) are to be introduced in risk exposures hedging to cope with this yield curve reshaping issue.</p><h2 id="an-introduction-to-cca">1. An Introduction to CCA</h2><p>Say there is a pair of CMT rates - (<span class="math inline">\(Y_t^1\)</span>, <span class="math inline">\(Y_t^2\)</span>). What CCA is trying to do is to find a cointegration vector <span class="math inline">\([1, -\gamma]^T\)</span> to let the following equation stand</p><p><span class="math display">\[ Y_t^1 - \gamma \cdot Y_t^2 = \mu + \epsilon_t \]</span> where <span class="math inline">\(\epsilon_t\)</span> is a stationary white noise process, and therefore also is a <strong>mean-reverting</strong> process.</p><p><strong>This seems like doing OLS regression. The assumptions about the error terms are quite the same. However, the difference lies in their optimization objectives. CCA tries to find a vector to yield the most mean-reverting time series using self-predictability measure while OLS seeks to find a vector to minimize the prediction error. We will talk more about the difference later.</strong></p><p>What if we extend a pair to a portfolio consisting of more than n CMT rates <span class="math inline">\(Y_t = [Y_t^1, Y_t^2, ..., Y_t^n]^T\)</span>? The problem is also focusing on find a cointegration vector <span class="math inline">\(\gamma = [\gamma_1, \gamma_2, ..., \gamma_n]^T\)</span> to let the following equation stand</p><p><span class="math display">\[ \gamma^T Y_t = \mu + \epsilon_t \]</span> Theoretically, we can find a cointegration vector to build a mean-reverting time series among as many assets as we want. But in practice, it's hard to execute them all with proper prices at one time, and the execution costs are also very high.</p><p>So how do we find the cointegration vector <span class="math inline">\(\gamma\)</span>? Actually, <span class="math inline">\(\gamma\)</span> is such a vector yielded by CCA that makes <span class="math inline">\(\gamma^T Y_t\)</span> a canonical variable, even it's the least predictable variable.</p><h2 id="derivation-of-cca">2. Derivation of CCA</h2><p>Let's look at how CCA is rigorously derived. There are 2 similar ways of performing CCA - one is introduced in <a href="http://pages.stern.nyu.edu/~dbackus/BCZ/HS/BoxTiao_canonical_Bio_77.pdf" target="_blank" rel="noopener">Box-Tiao(1977)</a> and another is introduced in <a href="https://www.elibrary.imf.org/doc/IMF001/01258-9781451950700/01258-9781451950700/Other_formats/Source_PDF/01258-9781451999181.pdf" target="_blank" rel="noopener">Chou-Ng(1994)</a>. Here we consider the former paper.</p><h3 id="self-predictability-measure">2.1 Self-predictability Measure</h3><p>Consider a $1 k $ vector process <span class="math inline">\(\{\mathbb{Z_t}\}\)</span> and let <span class="math inline">\(z_t = \mathbb{Z_t} - \mu\)</span>, where <span class="math inline">\(\mu\)</span> is a convenient <span class="math inline">\(1 \times k\)</span> vector of origin which is the mean if the process is stationary. Suppose <span class="math inline">\(z_t\)</span> follows the <em>p</em>th order multiple autoregressive model</p><p><span class="math display">\[z_t = \hat z_{t-1}(1) + a_t\]</span></p><p>where</p><p><span class="math display">\[\hat z_{t-1}(1) = E(z_t|z_{t-1}, z_{t-2},..) = \sum_{l=1}^{p}z_{t-l} \pi_l\]</span></p><p>is the expectation of <span class="math inline">\(z_t\)</span> conditional on past history up to time <span class="math inline">\(t-1\)</span>, the <span class="math inline">\(\pi_l\)</span> are <span class="math inline">\(k \times k\)</span> matrices, <span class="math inline">\(\{ a_t\}\)</span> is a sequence of independently and normally distributed <span class="math inline">\(1 \times k\)</span> vector random shocks with mean zero and covariance matrix <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(a_t\)</span> is independent of <span class="math inline">\(\hat z_{t-1}(1)\)</span> - like the assumptions in OLS. And the <span class="math inline">\(AR(p)\)</span> model can be then represented as</p><p><span class="math display">\[z_t (I - \sum_{l=1}^{p}\pi_l B^l) = a_t\]</span></p><p>where <span class="math inline">\(I\)</span> is the identity matrix and <span class="math inline">\(B\)</span> is the backshift operator such that <span class="math inline">\(B z_t = z_{t-1}\)</span>.</p><p>The process <span class="math inline">\(\{z_t\}\)</span> is stationary if the determinantal polynomial in <span class="math inline">\(B\)</span>, <span class="math inline">\(det(I - \sum_{l=1}^{p}\pi_l B^l)\)</span> has its zeros lying outside the unit circle, and otherwise the process will be called non-stationary.</p><p>Now, let's make the problem simpler by setting <span class="math inline">\(k=1\)</span> to narrow down to only 1 time series. Then, if the process is stationary, due to <span class="math inline">\(a_t\)</span> being independent of <span class="math inline">\(\hat z_{t-1}(1)\)</span>.</p><p><span class="math display">\[E(z_t^2) = E(\{\hat z_{t-1}(1)\}^2) + E(a_t^2)\]</span></p><p>which can be also written as</p><p><span class="math display">\[ \sigma_z^2 = \sigma_{\hat z}^2 + \sigma_a^2\]</span></p><p>We can then define a quantity <span class="math inline">\(\lambda\)</span> to measure the predictability of a stationary series from its past as <span class="math inline">\(\lambda = \frac{\sigma_{\hat z}^2}{\sigma_z^2} = 1 - \frac{\sigma_a^2}{\sigma_z^2}\)</span>.</p><p><strong>Note</strong>: the derivation above only applies to 1 time series. And now <span class="math inline">\(z_t\)</span> is assumed to be stationary.</p><h3 id="intuition-of-cca-decomposition">2.2 Intuition of CCA Decomposition</h3><p>Now let's consider <span class="math inline">\(k\)</span> processes <span class="math inline">\(z_t\)</span> which represent <span class="math inline">\(k\)</span> different stock market indexes such as <em>Dow Jones Average</em>, <em>Standard and Poors</em> and <em>Russell Index</em>, etc., all of which exhibit dynamic growth.</p><p>It is natural to conjecture that <strong>each</strong> might be represented as some aggregate of one or more common inputs which may be nearly nonstationary (<strong>momentum</strong>), together with other stationary (<strong>mean-reverting</strong>) or white noise components.</p><p>In other words. This leads us to contemplate <strong>linear aggregates of the form <span class="math inline">\(u_t = z_t m\)</span></strong>, where <span class="math inline">\(m\)</span> is such a <span class="math inline">\(k \times 1\)</span> vector that <strong>make <span class="math inline">\(u_t\)</span> a momentum or mean-reverting time series.</strong></p><blockquote><p>Note: <span class="math inline">\(z_t\)</span> is a vector consisting of k time series. And different <span class="math inline">\(m\)</span> will yield different <span class="math inline">\(u_t\)</span>. These different time series <span class="math inline">\(u_t\)</span> (whether mean-reverting or momentum) are <strong>aggregates</strong>. And these aggregates are <strong>derived</strong> time series from <span class="math inline">\(z_t\)</span>. The process of getting <span class="math inline">\(u_t\)</span> from <span class="math inline">\(z_t\)</span> is called <strong>CCA decompostion</strong> even if these 2 processes are not in the same level (<span class="math inline">\(u_t\)</span> is a transformed or derived process.)</p></blockquote><p><strong>(Can <span class="math inline">\(z_t\)</span> be represented as a linear combination of <span class="math inline">\(u_t\)</span>?)</strong></p><p>The aggregates <span class="math inline">\(u_t\)</span> which depend most heavily on the past, namely having large <span class="math inline">\(\lambda\)</span> (refers to <span class="math inline">\(u_t\)</span>'s <span class="math inline">\(\lambda\)</span>), may serve as useful composite indicators of the overall growth of the stock market (<strong>momentum</strong>). By contrast, the aggregates with <span class="math inline">\(\lambda\)</span> nearly zero may reflect stable contemporaneous relationships (<strong>mean-reverting</strong>) among the original indicators.</p><p>The analysis given in this paper yields <span class="math inline">\(k\)</span> 'canonical' components <span class="math inline">\(u_t\)</span> from least to most predictable. Thus we may usefully decompose the k-dimensional space of the observation <span class="math inline">\(z_t\)</span> into stationary and non-stationary subspaces.</p><h3 id="derive-canonical-variables">2.3 Derive Canonical Variables</h3><p>Let <span class="math inline">\(\Gamma_j(z) = E(z_t^T z_{t-j})\)</span> be the lag <span class="math inline">\(j\)</span> autocovariance matrix of <span class="math inline">\(z_t\)</span>. In the variance form, we have</p><p><span class="math display">\[ \Gamma_0(z) = \sum_{l=1}^{p} \Gamma_l(z)\pi_l + \Sigma= \Gamma_0(\hat z) + \Sigma \]</span></p><p>say, where <span class="math inline">\(\Gamma_0(\hat z)\)</span> is the covariance matrix of <span class="math inline">\(\hat z_{t-1}(1)\)</span>. <strong>Until further notice, we shall assume that <span class="math inline">\(\Sigma\)</span> and therefore <span class="math inline">\(\Gamma_0(z)\)</span> are <a href>postive-defnite</a>.</strong></p><p>Now, consider the linear combination <span class="math inline">\(u_t = z_t m\)</span>. For <span class="math inline">\(u_t\)</span>, we have that <span class="math inline">\(u_t = \hat u_{t-1}(1) + v_t\)</span>, where <span class="math inline">\(\hat u_{t-1}(1) = \hat z_{t-1}(1) m\)</span> and <span class="math inline">\(v_t=a_t m\)</span>. The predictability of <span class="math inline">\(u_t\)</span> from its past is therefore measured by</p><p><span class="math display">\[ \lambda = \sigma_{\hat u}^2 \sigma_{u}^{-2} = \{ m \Gamma_{0}(\hat z) m^T \} \{m \Gamma_{0}(z) m^T \}^{-1}\]</span></p><p>which can be represented in matrix form as</p><p><span class="math display">\[ \Lambda = M \Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z) M^{-1}\]</span></p><blockquote><p>Note: <span class="math inline">\(M\)</span> is what we are looking for. The logic is we want to find a transformed process <span class="math inline">\(\{u_t\}\)</span> which is generated by <span class="math inline">\(M\)</span> and the original <span class="math inline">\(z_t\)</span>. And we derive that <span class="math inline">\(M\)</span> can be found using eigendecomposition of <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span>.</p></blockquote><p>This is what we call <a href>eigendecomposition</a>, and therefore we can conclude that for the maximum predictability, <span class="math inline">\(\lambda\)</span> must be the maximum eigenvalue of <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span> and <span class="math inline">\(m\)</span> the corresponding eigenvector that makes <span class="math inline">\(u_t\)</span> a momentum time series. Similarly, the eigenvector that corresponds to the smallest eigenvalue will yield the least predictable combination of <span class="math inline">\(z_t\)</span>. <strong>This vector is referred to as cointegration vector</strong> that is mainly used in the first question (risk hedging) mentioned at the very beginning.</p><h3 id="canonical-transformation">Canonical Transformation</h3><p>Let <span class="math inline">\(\lambda_1, ..., \lambda_k\)</span> be the k real eigenvalues of matrix <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span>. Suppose <span class="math inline">\(\lambda_j\)</span> are ordered with <span class="math inline">\(\lambda_1\)</span> the smallest, and that the k corresponding <a href><strong>linearly independent eigenvectors</strong></a>, <span class="math inline">\(m_1, .., m_k\)</span> from the <span class="math inline">\(k\)</span> columns of a matrix <span class="math inline">\(M\)</span>. Then, we can construct a transformed process <span class="math inline">\(\{ y_t\}\)</span>, where</p><p><span class="math display">\[ y_t = \hat y_{t-1} (1) + b_t \]</span></p><p>with</p><p><span class="math display">\[ y_t = z_t M, b_t = a_t M, \hat y_{t-1}(1)=\sum_{l=1}^{p} y_{t-l}\pi^1_l \]</span> where <span class="math inline">\(\pi^1_l=M^{-1} \pi_l M\)</span></p><p>We now also have</p><p><span class="math display">\[ \Gamma_0(y) = \Gamma_0(\hat y) + \Sigma^1 \]</span></p><p>where <span class="math inline">\(\Gamma_0(y)=M \Gamma_0(z)M^T, \Gamma_0(\hat y)=M \Gamma_0(\hat z)M^T, \Sigma^1=M \Sigma M^T\)</span></p><p>Note: - $ M <em>{0}(z) </em>{0}^{-1}(z) M^{-1} = , M _{0}^{-1}(z) M^{-1} = I - $ where <span class="math inline">\(\Lambda\)</span> is a <span class="math inline">\(k \times k\)</span> matrix with elements <span class="math inline">\((\lambda_1, .., \lambda_k)\)</span> - <span class="math inline">\(0 \leq \lambda_j &lt; 1 \space (j=1,..,k)\)</span> - for <span class="math inline">\(i \neq j, m_i \Gamma_0(z) m_j^T = m_i \Sigma m_j^T = 0\)</span>. This makes <span class="math inline">\(\Gamma_0(y), \Gamma_0(\hat y), \Sigma^1\)</span> all diagonal (Otherwise, <span class="math inline">\(\Lambda\)</span> would not be diagonal). <strong>(And this can be proved using <span class="math inline">\(m_i \Gamma_{0}(\hat z) m_j^T = \lambda_{ij} m_i \Gamma_{0}(z) m_j^T\)</span>, where <span class="math inline">\(\lambda_{ij} = 0\)</span> when <span class="math inline">\(i \neq j\)</span>)</strong> - eigenvectors <span class="math inline">\(M\)</span> do not form an orthonormal basis because of the asymmetry of <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span></p><p>With this <strong>diagonal</strong> property, we can conclude that this transformation has produced <span class="math inline">\(k\)</span> new components series <span class="math inline">\(\{ y_{1t}, y_{2t}, .., y_{kt}\}\)</span> which are</p><ul><li>ordered from least predictable to most predictable (meaning self-predictability)</li><li>are contemporaneously independent</li><li>have predictable components <span class="math inline">\(\{\hat y_{1(t-1)}(1), \hat y_{2(t-1)}(1), .., \hat y_{k(t-1)}(1)\}\)</span> which are also contemporaneously independent</li><li>the same goes for <span class="math inline">\(\{ b_{1t}, b_{2t}, .., b_{kt}\}\)</span></li></ul><p><strong>Note</strong>: The content above goes for general time series, and the content below goes for <span class="math inline">\(AR(1)\)</span> time series.(Also <span class="math inline">\(M\)</span> above can be computed in another way.)</p><h3 id="example">Example</h3><p>Say we have <a href="https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=yield" target="_blank" rel="noopener">Constant Maturity Treasury</a> rates (CMT rates) data <span class="math inline">\(\{z_t\}\)</span> from <span class="math inline">\(02/01/2012\)</span> to <span class="math inline">\(06/30/2015\)</span>, a part of which is given below.</p><table><thead><tr class="header"><th>Date</th><th>6 Mo</th><th>1 Yr</th><th>2 Yr</th><th>3 Yr</th><th>5 Yr</th><th>...</th><th>30 Yr</th></tr></thead><tbody><tr class="odd"><td>2015-06-30</td><td>0.11</td><td>0.28</td><td>0.64</td><td>1.01</td><td>1.63</td><td>...</td><td>3.11</td></tr><tr class="even"><td>2015-06-29</td><td>0.11</td><td>0.27</td><td>0.64</td><td>1.00</td><td>1.62</td><td>...</td><td>3.09</td></tr><tr class="odd"><td>2015-06-26</td><td>0.08</td><td>0.29</td><td>0.72</td><td>1.09</td><td>1.75</td><td>...</td><td>3.25</td></tr><tr class="even"><td>2016-06-25</td><td>0.07</td><td>0.29</td><td>0.68</td><td>1.06</td><td>1.70</td><td>...</td><td>3.16</td></tr><tr class="odd"><td>....</td><td>....</td><td>....</td><td>....</td><td>....</td><td>....</td><td>...</td><td>....</td></tr><tr class="even"><td>2012-02-01</td><td>0.09</td><td>0.13</td><td>0.23</td><td>0.31</td><td>0.72</td><td>...</td><td>3.01</td></tr></tbody></table><p>In this period of time, these CMT rates time series <span class="math inline">\(\{z_t\}\)</span> are both momentum time series. When we try to fit <span class="math inline">\(AR(1)\)</span> with these series with different maturities separately, the <span class="math inline">\(AR(1)\)</span> decaying parameters are around <span class="math inline">\(0.95\)</span> - <span class="math inline">\(0.99\)</span> (<span class="math inline">\(2.5\)</span> years is not a short term and both of these rates are contemporaneously under the same influence. So in the long term, they are both presenting similar trends).</p><p>But after we do canonical transformation to construct new time series (just as we discussed above) <span class="math inline">\(\{y_t\}\)</span>, the most mean-reverting series has a <span class="math inline">\(0.51\)</span> decaying parameter in <span class="math inline">\(AR(1)\)</span> fitting.</p><figure><img src="CCA.png" alt="CCA"><figcaption>CCA</figcaption></figure><p>Note the constructed series <span class="math inline">\(\{y_t\}\)</span> are not corresponding to the original CMT rates series <span class="math inline">\(\{z_t\}\)</span>. This is similar to what is given by PCA - the first principle component is not corresponding to the first column of the original panel data.</p><h3 id="application">Application</h3><ol type="1"><li>Spot small mean-reverting portfolios.</li><li>Do CCA reconstruction to generate detrended data.</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;a-quick-question&quot;&gt;0. A Quick Question&lt;/h2&gt;&lt;p&gt;Let&#39;s first look at a question:&lt;/p&gt;&lt;blockquote&gt;&lt;p&gt;Here is a portfolio risk report for 2
      
    
    </summary>
    
      <category term="Math" scheme="http://yoursite.com/categories/Math/"/>
    
    
      <category term="Math" scheme="http://yoursite.com/tags/Math/"/>
    
      <category term="Linear Algebra" scheme="http://yoursite.com/tags/Linear-Algebra/"/>
    
      <category term="Data Engineering" scheme="http://yoursite.com/tags/Data-Engineering/"/>
    
  </entry>
  
  <entry>
    <title>Fixed Income Quant Trading</title>
    <link href="http://yoursite.com/2019/08/19/FIQT/"/>
    <id>http://yoursite.com/2019/08/19/FIQT/</id>
    <published>2019-08-19T15:25:41.000Z</published>
    <updated>2020-08-01T16:15:15.784Z</updated>
    
    <content type="html"><![CDATA[<h2 id="intro-to-fiqt---eurodollar-futures">1. Intro to FIQT - Eurodollar Futures</h2><h4 id="convexity-adjustment">Convexity Adjustment</h4><p>(A time scope graph is needed here...)</p><p>There exists a convexity adjustment between LIBOR forward contracts and Eurodollar futures. Hos is this derived is shown below</p><p>Assume <span class="math inline">\(\delta(t, T)\)</span> is the forward price at time <span class="math inline">\(t\)</span> which expires at time <span class="math inline">\(T\)</span>, and <span class="math inline">\(L(T)\)</span> is the LIBOR rate at time <span class="math inline">\(T\)</span>, <span class="math inline">\(r(t)\)</span> is the risk-free rate process. We can easily know the rational price of zero coupon bond at time <span class="math inline">\(t\)</span> which expires at time <span class="math inline">\(T\)</span>, or <span class="math inline">\(T\)</span>-forward numeraire is <span class="math display">\[ p(t, T) = E_t^Q[e^{-\int_{t}^{T}r(u)du}] \]</span> For a LIBOR forward <strong>contract</strong> that was entered at time <span class="math inline">\(t\)</span>, its contact value is 0, which is <span class="math display">\[ \begin{aligned} 0 &amp;= E_t^Q \left[ e^{-\int_{t}^{T}r(u)du} [\delta(t, T) - L(T)] \right] \\ &amp;= E_t^{Q}\left[ e^{-\int_{t}^{T}r(u)du} \delta(t, T) - e^{-\int_{t}^{T}r(u)du} L(T)\right]\\ &amp;= E_t^{Q}\left[ e^{-\int_{t}^{T}r(u)du} \delta(t, T) \right] - E_t^{Q}\left[ e^{-\int_{t}^{T}r(u)du} \right] E_t^Q [L(T)] - cov \left( e^{-\int_{t}^{T}r(u)du}, L(T) \right) \\ &amp;= \delta(t, T) \cdot p(t, T) - p(t, T) \cdot E_t^Q [L(T)] - cov \left( e^{-\int_{t}^{T}r(u)du}, L(T) \right) \\ \end{aligned} \]</span> where we apply <span class="math inline">\(cov(X, Y) = E[X Y] - E[X] E[Y]\)</span>, so we have <span class="math display">\[ \delta(t, T) = E_t^Q [L(T)] + \frac{1}{p(t,T)} cov \left( e^{-\int_{t}^{T}r(u)du}, L(T) \right) \]</span></p><h4 id="npv-effect">NPV Effect</h4><p>Consider the value of a forward contract at <span class="math inline">\(t&#39; &gt; t\)</span> under CSA, a contract that was entered at time <span class="math inline">\(t\)</span>, so the difference in contract values on <span class="math inline">\(t&#39;\)</span> and <span class="math inline">\(t\)</span> that exchanges hands at <span class="math inline">\(t&#39;\)</span> is equal to <span class="math display">\[ V(t&#39;) - V(t) = E_{t&#39;} \left( e^{-\int_{t&#39;}^{T}r_c(u)du} \right) (F_{CSA}(t&#39;, T) - F_{CSA}(t, T)) \]</span> while the difference of futures contract will not be discounted, which is <span class="math display">\[ F(t&#39;) - F(t) = F_{CSA}(t&#39;, T) - F_{CSA}(t, T) \]</span></p><p>Let's take a look at another example in <a href="http://janroman.dhis.org/finance/OIS/Piterbarg/Piterbarg_Cuttingedge.pdf" target="_blank" rel="noopener">Piterbarg (2010)</a>.</p><h2 id="historical-factor-model">2. Historical Factor Model</h2><h4 id="canonical-component-analysis">Canonical Component Analysis</h4><h2 id="term-structure-model">3. Term Structure Model</h2><h2 id="tsm-fitting">4. TSM fitting</h2><h2 id="signal-research-framework">5. Signal Research Framework</h2><h2 id="live-eurodollar-futures-trading">6. Live Eurodollar Futures Trading</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;intro-to-fiqt---eurodollar-futures&quot;&gt;1. Intro to FIQT - Eurodollar Futures&lt;/h2&gt;&lt;h4 id=&quot;convexity-adjustment&quot;&gt;Convexity Adjustment&lt;/h4
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Class Magic Methods in Python</title>
    <link href="http://yoursite.com/2019/07/13/magic-methods/"/>
    <id>http://yoursite.com/2019/07/13/magic-methods/</id>
    <published>2019-07-13T20:25:16.000Z</published>
    <updated>2020-08-01T14:30:32.169Z</updated>
    
    <content type="html"><![CDATA[<h3 id="getattribute__-method">__getattribute__ method</h3><p>This method usually implements a class's getter to <strong>get</strong> whatever you request (<strong>attributes and methods</strong>) from an object/a class.<br>I don't know for sure the difference with <code>__getattr__</code>.</p><p>A common way of implementing this method is as follows:</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">class</span> test:</a><a class="sourceLine" id="cb1-3" data-line-number="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, lo<span class="op">=</span><span class="dv">1</span>, hi<span class="op">=</span><span class="dv">20</span>, size<span class="op">=</span><span class="dv">10</span>):</a><a class="sourceLine" id="cb1-4" data-line-number="4">        <span class="va">self</span>.array <span class="op">=</span> np.random.randint(low<span class="op">=</span>lo, high<span class="op">=</span>hi, size<span class="op">=</span>size)</a><a class="sourceLine" id="cb1-5" data-line-number="5"></a><a class="sourceLine" id="cb1-6" data-line-number="6">    <span class="kw">def</span> <span class="fu">__getattribute__</span>(<span class="va">self</span>, name):</a><a class="sourceLine" id="cb1-7" data-line-number="7">        builtin_members <span class="op">=</span> [<span class="st">&#39;count&#39;</span>]</a><a class="sourceLine" id="cb1-8" data-line-number="8">        <span class="cf">if</span> name <span class="kw">in</span> builtin_members: </a><a class="sourceLine" id="cb1-9" data-line-number="9">            <span class="cf">return</span> <span class="bu">super</span>().<span class="fu">__getattribute__</span>(name)</a><a class="sourceLine" id="cb1-10" data-line-number="10">        </a><a class="sourceLine" id="cb1-11" data-line-number="11">    <span class="kw">def</span> count(<span class="va">self</span>):</a><a class="sourceLine" id="cb1-12" data-line-number="12">        <span class="cf">return</span> np.size(<span class="va">self</span>.array)</a></code></pre></div><p><strong>Question</strong>: What is the following codes' output?</p><div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1">t <span class="op">=</span> test()</a><a class="sourceLine" id="cb2-2" data-line-number="2"><span class="bu">print</span>(t.count())</a></code></pre></div><p>The answer is 1.</p><p>And what about the results when we change the test's structure to this:</p><div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a><a class="sourceLine" id="cb3-2" data-line-number="2"><span class="kw">class</span> test:</a><a class="sourceLine" id="cb3-3" data-line-number="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, lo<span class="op">=</span><span class="dv">1</span>, hi<span class="op">=</span><span class="dv">20</span>, size<span class="op">=</span><span class="dv">10</span>):</a><a class="sourceLine" id="cb3-4" data-line-number="4">        <span class="va">self</span>.array <span class="op">=</span> np.random.randint(low<span class="op">=</span>lo, high<span class="op">=</span>hi, size<span class="op">=</span>size)</a><a class="sourceLine" id="cb3-5" data-line-number="5"></a><a class="sourceLine" id="cb3-6" data-line-number="6">    <span class="kw">def</span> <span class="fu">__getattribute__</span>(<span class="va">self</span>, name):</a><a class="sourceLine" id="cb3-7" data-line-number="7">        builtin_members <span class="op">=</span> [<span class="st">&#39;count&#39;</span>]</a><a class="sourceLine" id="cb3-8" data-line-number="8">        <span class="cf">if</span> name <span class="kw">in</span> builtin_members: </a><a class="sourceLine" id="cb3-9" data-line-number="9">            <span class="cf">return</span> <span class="bu">super</span>().<span class="fu">__getattribute__</span>(name)</a><a class="sourceLine" id="cb3-10" data-line-number="10">        <span class="cf">else</span>:</a><a class="sourceLine" id="cb3-11" data-line-number="11">            <span class="cf">return</span> <span class="bu">object</span>.<span class="fu">__getattribute__</span>(<span class="va">self</span>, name)</a><a class="sourceLine" id="cb3-12" data-line-number="12">        </a><a class="sourceLine" id="cb3-13" data-line-number="13">    <span class="kw">def</span> count(<span class="va">self</span>):</a><a class="sourceLine" id="cb3-14" data-line-number="14">        <span class="cf">return</span> np.size(<span class="va">self</span>.array)</a></code></pre></div><p>The answer is 10.</p><p>And the reason is 1. <code>test</code> inherits from <code>object</code>, so <code>super().__getattribute__(name)</code> is equivalent to <code>object.__getattribute__(self, name)</code>. 2. the first class doesn't implement the case when <strong>non-built-in members</strong> (which in this case is <code>array</code>) are retrieved, so it automatically returns <code>None</code> whose size is 1.</p><p>Sometimes, there are many other complex implementation of an attribute getter.</p><h3 id="iadd__-and-__add__-method">__iadd__ and __add__ method</h3><p>Methods like <code>__iadd__</code>are called to implement <a href="https://docs.python.org/3/reference/datamodel.html#object.__iadd__" target="_blank" rel="noopener">augmented arithmetic assignments</a>. Specifically, these two methods are the implementations of the operators <code>+=</code> and <code>+</code>. To overload these 2 operators, we must implement and override <code>__iadd__</code> and <code>__add__</code> methods of a class. If <code>__iadd__</code> is not implemented, the augmented assignment <code>+=</code> falls back to the normal methods. Originally, <code>x += y</code> is equivalent to <code>x = x.__iadd__(y)</code>. When <code>__iadd__</code> is not implemented, <code>__add__</code> or <code>__radd__</code> will be the alternative method to overload <code>+=</code>. We can look at some examples below</p><div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" data-line-number="1"><span class="kw">class</span> A:</a><a class="sourceLine" id="cb4-2" data-line-number="2">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, val):</a><a class="sourceLine" id="cb4-3" data-line-number="3">        <span class="va">self</span>.val <span class="op">=</span> val</a><a class="sourceLine" id="cb4-4" data-line-number="4">    <span class="kw">def</span> <span class="fu">__str__</span>(<span class="va">self</span>):</a><a class="sourceLine" id="cb4-5" data-line-number="5">        <span class="cf">return</span> <span class="bu">str</span>(<span class="va">self</span>.val)</a><a class="sourceLine" id="cb4-6" data-line-number="6">    <span class="kw">def</span> <span class="fu">__add__</span>(<span class="va">self</span>, other):</a><a class="sourceLine" id="cb4-7" data-line-number="7">        <span class="cf">return</span> A(<span class="va">self</span>.val <span class="op">+</span> other.val)</a><a class="sourceLine" id="cb4-8" data-line-number="8">    <span class="kw">def</span> <span class="fu">__iadd__</span>(<span class="va">self</span>, other):</a><a class="sourceLine" id="cb4-9" data-line-number="9">        <span class="va">self</span>.val <span class="op">+=</span> other.val</a><a class="sourceLine" id="cb4-10" data-line-number="10">        <span class="cf">return</span> <span class="va">self</span></a><a class="sourceLine" id="cb4-11" data-line-number="11">a1, a2 <span class="op">=</span> A(<span class="dv">1</span>), A(<span class="dv">2</span>)</a><a class="sourceLine" id="cb4-12" data-line-number="12">a3 <span class="op">=</span> a1 <span class="op">+</span> a2</a><a class="sourceLine" id="cb4-13" data-line-number="13"><span class="bu">print</span>(a1, a2, a3) <span class="co">#1, 2, 3</span></a><a class="sourceLine" id="cb4-14" data-line-number="14">a3 <span class="op">+=</span> a1</a><a class="sourceLine" id="cb4-15" data-line-number="15"><span class="bu">print</span>(a1, a2, a3) <span class="co">#1, 2, 4</span></a></code></pre></div><p>If <code>__iadd__</code> is to be implemented, then there are a few things to watch out for. Typically, <code>__iadd__</code> should attempt to do the operation in-place (modifying self) and return the result (which could be, but does not have to be, self). And consequently, <code>__iadd__</code> is implemented for mutable objects like <code>list</code>and <code>dict</code>. For immutable objects like <code>int</code> and <code>string</code>, <code>__iadd__</code> simply uses <code>__add__</code> or <code>__radd__</code> as alternatives to maintain the immutability. Let's look at a few examples to get familiar with this idea.</p><div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" data-line-number="1">a <span class="op">=</span> b <span class="op">=</span> [<span class="dv">1</span>]</a><a class="sourceLine" id="cb5-2" data-line-number="2">a <span class="op">+=</span> [<span class="dv">2</span>]</a><a class="sourceLine" id="cb5-3" data-line-number="3"><span class="bu">print</span>(b) <span class="co"># [1, 2]</span></a><a class="sourceLine" id="cb5-4" data-line-number="4">a <span class="op">=</span> a <span class="op">+</span> [<span class="dv">3</span>]</a><a class="sourceLine" id="cb5-5" data-line-number="5"><span class="bu">print</span>(b) <span class="co"># [1, 2]</span></a></code></pre></div><p>The above example clearly displays the difference between <code>__add__</code> and <code>__iadd__</code> on mutable lists. <code>a += [2]</code> changes what <code>a</code> is referencing but <code>a + [3]</code> simply returns a new list without changing <code>a</code> or <code>[3]</code>. The assignment <code>=</code> only makes <code>a</code> reference the new list but doesn't change what <code>b</code> references. Also, for lists, <code>a += [3]</code> is just like <code>a.append(ele)</code> because they both mutate the list in-place and maintain <code>a</code>'s reference while <code>a = a + [3]</code> returns a new list and performs an assignment and finally changing <code>a</code>'s reference.</p><div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" data-line-number="1">a <span class="op">=</span> b <span class="op">=</span> <span class="dv">32</span></a><a class="sourceLine" id="cb6-2" data-line-number="2">a <span class="op">+=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb6-3" data-line-number="3">b <span class="op">+=</span> <span class="dv">1</span></a><a class="sourceLine" id="cb6-4" data-line-number="4"><span class="bu">print</span>(a, b) <span class="co"># 33, 33</span></a><a class="sourceLine" id="cb6-5" data-line-number="5">a <span class="op">=</span> a <span class="op">+</span> <span class="dv">1</span></a><a class="sourceLine" id="cb6-6" data-line-number="6">b <span class="op">=</span> b <span class="op">+</span> <span class="dv">1</span></a><a class="sourceLine" id="cb6-7" data-line-number="7"><span class="bu">print</span>(a, b) <span class="co"># 34, 34</span></a></code></pre></div><p>For data of <code>int</code> types, <code>__iadd__</code> is the same as <code>__add__</code>. Then let's take a look at an <a href="https://docs.python.org/3/faq/programming.html#faq-augmented-assignment-tuple-error" target="_blank" rel="noopener">example</a> with unexpected errors concerning <code>__iadd__</code>.</p><div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" data-line-number="1">a_tuple <span class="op">=</span> ([<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>])</a><a class="sourceLine" id="cb7-2" data-line-number="2">a_tuple[<span class="dv">0</span>] <span class="op">+=</span> [<span class="dv">3</span>]</a><a class="sourceLine" id="cb7-3" data-line-number="3">Traceback (most recent call last):</a><a class="sourceLine" id="cb7-4" data-line-number="4">  ...</a><a class="sourceLine" id="cb7-5" data-line-number="5"><span class="pp">TypeError</span>: <span class="st">&#39;tuple&#39;</span> <span class="bu">object</span> does <span class="kw">not</span> support item assignment</a></code></pre></div><p>As we can see, <code>+=</code> operations to a mutable list fails because it is at the same time an element of a immutable tuple. But if we take a look at <code>a_tuple[0]</code> and we will find <code>a_tuple[0]</code> has become <code>[1, 2, 3]</code> successfully. Why? If we recreate this process with intermediate results, we will get</p><div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb8-1" data-line-number="1">result <span class="op">=</span> a_tuple[<span class="dv">0</span>].<span class="fu">__iadd__</span>([<span class="dv">1</span>])</a><a class="sourceLine" id="cb8-2" data-line-number="2">a_tuple[<span class="dv">0</span>] <span class="op">=</span> result</a><a class="sourceLine" id="cb8-3" data-line-number="3">Traceback (most recent call last):</a><a class="sourceLine" id="cb8-4" data-line-number="4">  ...</a><a class="sourceLine" id="cb8-5" data-line-number="5"><span class="pp">TypeError</span>: <span class="st">&#39;tuple&#39;</span> <span class="bu">object</span> does <span class="kw">not</span> support item assignment</a></code></pre></div><p>Simply speaking, the object <code>a_tuple[0]</code> references (which is <code>[1, 2]</code>) has been smoothly mutated to <code>[1, 2, 3]</code> because of <code>__iadd__</code>. However, <code>a_tuple[0] = result</code> serves to mutate the reference of <code>a_tuple</code>'s first element while it is prohibited by Python to maintain <code>tuple</code>'s immutability. However, we can make some minor changes to make this happen</p><div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb9-1" data-line-number="1">a_tuple <span class="op">=</span> ([<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>])</a><a class="sourceLine" id="cb9-2" data-line-number="2">a_tuple.append(<span class="dv">3</span>)</a><a class="sourceLine" id="cb9-3" data-line-number="3"><span class="bu">print</span>(a_tuple) <span class="co"># ([1, 2, 3], [3])</span></a></code></pre></div><p>This actually doesn't change any reference within that tuple because the only different thing is the list instead of the reference. And this can be proved as below</p><div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb10-1" data-line-number="1">a_tuple <span class="op">=</span> ([<span class="dv">1</span>, <span class="dv">2</span>], [<span class="dv">3</span>])</a><a class="sourceLine" id="cb10-2" data-line-number="2"><span class="bu">print</span>([<span class="bu">id</span>(x) <span class="cf">for</span> x <span class="kw">in</span> a_tuple]) <span class="co"># [1341325857224, 1341325857736]</span></a><a class="sourceLine" id="cb10-3" data-line-number="3">a_tuple[<span class="dv">0</span>].append(<span class="dv">3</span>)</a><a class="sourceLine" id="cb10-4" data-line-number="4"><span class="bu">print</span>([<span class="bu">id</span>(x) <span class="cf">for</span> x <span class="kw">in</span> a_tuple]) <span class="co"># [1341325857224, 1341325857736]</span></a></code></pre></div><p>In a summary, you cannot make the tuple reference different things but you can change what is inside the list as you leave the reference alone. For more info, please refer to the official docs <a href="https://docs.python.org/3/reference/simple_stmts.html#assignment-statements" target="_blank" rel="noopener">Python Assignment Statements</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;getattribute__-method&quot;&gt;__getattribute__ method&lt;/h3&gt;&lt;p&gt;This method usually implements a class&#39;s getter to &lt;strong&gt;get&lt;/strong&gt; whatev
      
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="Magic Methods" scheme="http://yoursite.com/tags/Magic-Methods/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2019/06/24/hello-world/"/>
    <id>http://yoursite.com/2019/06/24/hello-world/</id>
    <published>2019-06-25T01:51:47.291Z</published>
    <updated>2019-06-25T01:51:47.291Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb1-1" data-line-number="1">$ <span class="ex">hexo</span> new <span class="st">&quot;My New Post&quot;</span></a></code></pre></div><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="run-server">Run server</h3><div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb2-1" data-line-number="1">$ <span class="ex">hexo</span> server</a></code></pre></div><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="generate-static-files">Generate static files</h3><div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb3-1" data-line-number="1">$ <span class="ex">hexo</span> generate</a></code></pre></div><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb4-1" data-line-number="1">$ <span class="ex">hexo</span> deploy</a></code></pre></div><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
