<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Avalon</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-24T01:39:20.651Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>John Reese</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>A Mysterious Algorithm used in Apollo 11 Guidance</title>
    <link href="http://yoursite.com/2020/04/23/kalman-filter/"/>
    <id>http://yoursite.com/2020/04/23/kalman-filter/</id>
    <published>2020-04-24T01:05:54.000Z</published>
    <updated>2020-04-24T01:39:20.651Z</updated>
    
    <content type="html"><![CDATA[<h2 id="kalman-filter">Kalman Filter</h2><h3 id="state-space-models">0. State Space Models</h3><p>In <a href="https://en.wikipedia.org/wiki/State-space_representation#Example:_continuous-time_LTI_case" target="_blank" rel="noopener">Wikipedia</a>, a state space model is described as a set of input, output and state variables mixed with some ODE systems. I have no idea what that is with this abstract definition. State variables</p><p>can also be known as hidden states evolving through time in a way that we don't know how but we would like to use some <strong>dynamics</strong> and <strong>observations</strong> to fit the evolution path.</p><p><strong>An Example</strong></p><p>Let's look at a random walk example below,</p><p>Say we model some hidden states with two parts, which are its last state and some random noise. Also, we give the mapping relationship from <span class="math inline">\(x_k\)</span> to <span class="math inline">\(z_k\)</span> as below. <span class="math display">\[ x_k = x_{k - 1} + w_{k - 1}, w_{k} \sim N(0, Q), \]</span></p><p><span class="math display">\[ z_k = x_k + \epsilon_{k}, \epsilon_{k} \sim N(0, R), \]</span></p><p>where <span class="math inline">\(x_k\)</span> is the state variable and <span class="math inline">\(z_k\)</span> is the observation value, and <span class="math inline">\(w_{k}\)</span>, <span class="math inline">\(\epsilon_{k}\)</span> are independent normal random noise.</p><figure><img src="random_walk.png" alt="Random walk"><figcaption>Random walk</figcaption></figure><p>Basically, we have a state space model about <span class="math inline">\(x_k\)</span>, and we would like to know how exactly <span class="math inline">\(x_k\)</span> will evolve in future steps. From the equations above, we can also estimate <span class="math inline">\(\hat x_k\)</span> with current <span class="math inline">\(\hat x_{k - 1}\)</span> and <span class="math inline">\(z_{k-1}\)</span>.</p><p><strong>Probabilistic State Space Models</strong></p><p>But usually, a point estimate is less significant than an interval estimate. So here comes Probabilistic State Space Models.</p><p>With the same example above, we can know the distribution of <span class="math inline">\(x_k\)</span> given <span class="math inline">\(x_{k-1}\)</span>and the distribution of observations <span class="math inline">\(z_k\)</span> given <span class="math inline">\(x_{k}\)</span>. <span class="math display">\[ p(x_k | x_{k - 1}) = \frac{1}{\sqrt{2 \pi Q}}\exp \left( -\frac{1}{2Q}(x_k - x_{k - 1})^2 \right) \]</span></p><p><span class="math display">\[ p(z_k | x_{k}) = \frac{1}{\sqrt{2 \pi R}}\exp \left( -\frac{1}{2R}(z_k - x_{k})^2 \right) \]</span></p><p>where <span class="math inline">\(p(\cdot)\)</span> represents a probability density function.</p><p><strong>Assumptions</strong></p><p>Typically, we use <span class="math inline">\(p(x_k | x_{k - 1})\)</span> instead of <span class="math inline">\(p(x_k | x_{1},x_{2},...,x_{k - 1})\)</span> to describe the conditional distribution is due to the Markovian assumption, which is <strong>future states are independent from the past states given the present.</strong> <span class="math display">\[ p(x_k | x_{k - 1}) = p(x_k | x_{1},x_{2},...,x_{k - 1}, z_{1},z_{2},...,z_{k - 1}) \]</span> Also, we can also naturally think of the relationship between states and observations. Obviously, <strong>observations only depend on its current states</strong> instead of past observations or past states. <span class="math display">\[ p(z_k | x_{k}) = p(z_k | x_{1},x_{2},...,x_{k}, z_{1},z_{2},...,z_{k - 1}) \]</span> Simpler forms of these assumptions are as below <span class="math display">\[ p(x_k | x_{1:k-1}, z_{1:k}) = p(x_k | x_{k - 1}) \]</span></p><p><span class="math display">\[ p(z_k | x_{1:k}, z_{1:k - 1}) = p(z_k | x_{k}) \]</span></p><h3 id="bayesian-filter">1. Bayesian Filter</h3><p>From <a href="https://en.wikipedia.org/wiki/Bayesian_inference" target="_blank" rel="noopener">Bayesian inference</a>, the probability for an event is updated as more evidence or information becomes available, so we would like to use Bayes rules to <strong>simulate</strong> the probability distribution of hidden states <span class="math inline">\(x_t\)</span> with incoming observations. <span class="math display">\[ \begin{align*} p(x_k |z_{1:k}) &amp;= \frac{p(x_k, z_{1:k})}{p( z_{1:k})}\\ &amp;= \frac{p(z_k | z_{1:k-1}, x_k) p(x_k |z_{1:k-1}) p(z_{1:k-1})}{p(z_{1:k})}\\ &amp;= \eta p(z_k | x_k) p(x_k | z_{1:k-1}), \space where \space\space \eta = p(z_k | z_{1:k-1})\\ &amp;= \eta p(z_k | x_k) \int{p(x_k | x_{k-1}, z_{1:k-1})p(x_{k-1} | z_{1:k-1})dx_{k-1}}\\ &amp;= \eta p(z_k | x_k) \int{p(x_k | x_{k-1})p(x_{k-1} | z_{1:k-1})dx_{k-1}}\\ \end{align*} \]</span> Here comes a recursion formula if we replace <span class="math inline">\(p(x_k |u_{1:k})\)</span> with <span class="math inline">\(Bel(x_k)\)</span> meaning the posterior pdf of <span class="math inline">\(x_k\)</span> at time <span class="math inline">\(k\)</span>, so we have <span class="math display">\[ Bel(x_k) = \eta p(z_k|x_k) \int p(x_k|x_{k-1})Bel(x_{k-1})dx_{k-1} \]</span> where <span class="math inline">\(p(z_k|x_k)\)</span> and <span class="math inline">\(p(x_k|x_{k-1})\)</span> correspond exactly to the 2 hypothetical equations discussed in the random walk example, which are <strong>Observations-States Mapping</strong> and <strong>State Dynamics</strong>. Intuitively, we need state dynamics to make apriori estimates (<span class="math inline">\(\int{p(x_k | x_{k-1})p(x_{k-1} | z_{1:k-1})dx_{k-1}}\)</span>) and observations to make likelihood adjustments, thus making the Bayesian rules complete. <a href="https://users.aalto.fi/~ssarkka/course_k2016/handout3.pdf#page=12" target="_blank" rel="noopener">Here</a> shows how the distribution of <span class="math inline">\(x_{k}\)</span> evolves under Bayesian rules.</p><p><strong>A New Input</strong></p><p>Now we introduce another control variable <span class="math inline">\({u_t}\)</span> which may serve as an additional impact to <span class="math inline">\(x_k\)</span>, and we can now imagine the state model has the new form like below: <span class="math display">\[ x_k = x_{k - 1} + u_{k} + w_{k - 1}, w_{k} \sim N(0, Q), \]</span></p><p><span class="math display">\[ z_k = x_k + \epsilon_{k}, \epsilon_{k} \sim N(0, R), \]</span></p><p>Also, we cast those 2 assumptions to <span class="math inline">\(u_k\)</span>, then we have <span class="math display">\[ p(x_k | x_{1:k-1}, u_{1:k}, z_{1:k}) = p(x_k | x_{k - 1}, u_k) \]</span></p><p><span class="math display">\[ p(z_k | x_{1:k}, u_{1:k}, z_{:k - 1}) = p(z_k | x_{k}) \]</span></p><p>The target conditional probability distribution has now become posterior distribution <span class="math inline">\(p(x_k | u_{1:k}, z_{1:k})\)</span>, we would like to know how the current state behaves given the most recent series of observations and control variables. <span class="math display">\[ \begin{align*} p(x_k |u_{1:k}, z_{1:k}) &amp;= \frac{p(x_k, u_{1:k}, z_{1:k})}{p(u_{1:k}, z_{1:k})}\\ &amp;= \frac{p(z_k | u_{1:k}, z_{1:k-1}, x_k) p(x_k | u_{1:k}, z_{1:k-1}) p(u_{1:k}, z_{1:k-1})}{p(u_{1:k}, z_{1:k})}\\ &amp;= \eta p(z_k | x_k) p(x_k | u_{1:k}, z_{1:k-1}), \space where \space\space \eta = p(z_k | u_{1:k}, z_{1:k-1})\\ &amp;= \eta p(z_k | x_k) \int{p(x_k | x_{k-1}, u_{1:k}, z_{1:k-1})p(x_{k-1} | u_{1:k}, z_{1:k-1})dx_{k-1}}\\ &amp;= \eta p(z_k | x_k) \int{p(x_k | x_{k-1}, u_k)p(x_{k-1} | u_{1:k-1}, z_{1:k-1})dx_{k-1}}\\ \end{align*} \]</span> Here comes a recursion formula if we replace <span class="math inline">\(p(x_k |u_{1:k}, z_{1:k})\)</span> with <span class="math inline">\(Bel(x_k)\)</span> meaning the posterior pdf of <span class="math inline">\(x_k\)</span> at time <span class="math inline">\(k\)</span>, so we have <span class="math display">\[ Bel(x_k) = \eta p(z_k|x_k) \int p(x_k|x_{k-1},u_k)Bel(x_{k-1})dx_{k-1} \]</span> which is all the same as the previous formula except <span class="math inline">\(u_k\)</span>. Now we can separate this recursion formula into two parts <span class="math display">\[ \overline {Bel}{(x_k)} = \int p(x_k|x_{k-1},u_k)Bel(x_{k-1})dx_{k-1} \]</span></p><p><span class="math display">\[ Bel(x_k) = \eta p(z_k|x_k)\overline {Bel}{(x_k)} \]</span></p><p>state prediction and state update respectively. We can say <span class="math inline">\(x_k\)</span> is predicted with new inputs of control variables <span class="math inline">\(u_k\)</span> and updated with corresponding observations <span class="math inline">\(z_k\)</span>.</p><p>Why we need a new input <span class="math inline">\(u_k\)</span>? Simply speaking, we need some other factors rather than pure past states to explain the evolution of <span class="math inline">\(x_k\)</span>. To be more specific, Bayesian filter is widely used in probabilistic robotics where there is always robotics control (say robots are controlled to move 2 meters ahead) and the control itself influences a system's state (the state is the position of the robot).</p><h3 id="kalman-filter-1">2. Kalman Filter</h3><p>The recursion equation above indicates how the distribution of <span class="math inline">\(x_k\)</span> evolves through time. However we do not know what is the exact distribution of <span class="math inline">\(x_{k-1}\)</span> given a random initial distribution <span class="math inline">\(x_0\)</span>. If an initial random distribution is given, then we must follow the recursion formula to integrate over all possible <span class="math inline">\(x_{k-1}\)</span> to get an estimate.</p><p>Here comes the trick of Kalman Filter-using <strong>Gaussian</strong> distributions to describe <span class="math inline">\(x_k\)</span>. Because linear combinations of Gaussian distributions are still Gaussian, the states distribution evolution is easier to follow under Gaussian assumptions.</p><p><strong>Assumptions</strong></p><ol type="1"><li>State Dynamics and Observation-State Mapping are both linear models:</li></ol><p><span class="math display">\[ x_k = F_k x_{k-1} + B_ku_k + w_k \]</span></p><p><span class="math display">\[ z_k = H_k x_{k} + \epsilon_k \]</span></p><ol start="2" type="1"><li>State noise <span class="math inline">\(w_t\)</span> and observation noise <span class="math inline">\(\epsilon_t\)</span> both follow normal distribution:</li></ol><p><span class="math display">\[ w_{k} \sim N(0, Q_k) \]</span></p><p><span class="math display">\[ \epsilon_{k} \sim N(0, R_k) \]</span></p><p>Also, we introduce <span class="math inline">\(P_k = Var(x_k - \hat x_{k|k - 1})\)</span> to describe the variance of estimate error (which is the difference between actual state <span class="math inline">\(x_k\)</span> and estimate state <span class="math inline">\(\hat x_{k|k-1}\)</span>).</p><p><strong>Kalman Filter Derivation</strong></p><p>We follow the <strong>state prediction</strong> and <strong>state update</strong> procedure as above to derive <span class="math inline">\(x_k\)</span> and <span class="math inline">\(P_k\)</span></p><ol type="1"><li>State Prediction (Arrive at Apriori Estimate):</li></ol><p><span class="math display">\[ \hat x_{k|k-1} = F_{k} \hat x_{k-1|k-1} + B_k u_k + w_k \]</span></p><p><span class="math display">\[ \begin {align*} P_{k|k-1} &amp;= Var(x_{k} - \hat x_{k|k-1}) \\ &amp;= Var(F_k(x_{k} - \hat x_{k|k-1}) + w_k) \\ &amp;= F_k P_k F_k^T + Q_k \end {align*} \]</span></p><ol start="2" type="1"><li><p>State Update (Obtain the Posteriori Estimate): <span class="math display">\[ K_k = P_{k|k-1}H_k^T(H_k P_{k|k-1} H_k^T + R_k)^{-1} \]</span></p><p><span class="math display">\[ \hat x_{k|k} = \hat x_{k|k-1} + K_k (z_k - H_k \hat x_{k|k-1}) \]</span></p><p><span class="math display">\[ P_{k|k} = [I - K H_k] P_{k|k-1} \]</span></p><p><span class="math inline">\(K\)</span> for <strong>Kalman Gain</strong> is how Kalman Filters extends from Bayesian Filter. Let's first look at the state update which can be interpreted as <strong>apriori estimate</strong> plus a portion of <strong>predicted observation error</strong>.</p><p>Let's then understand the structure of <span class="math inline">\(K\)</span>, where <span class="math display">\[ Var(z_k - \hat z_{k|k-1}) = Var(H_k(x_k - \hat x_{k|k-1}) + \epsilon_k) = H_k P_{k|k-1} H_k^T + R_k \]</span> So we can express the portion of predicted observation error as <span class="math display">\[ K_k (z_k - \hat z_{k|k-1}) = Var[x_k - x_{k|k-1}] H^T_k Var^{-1}[z_k - \hat z_{k|k-1}] (z_k - \hat z_{k|k-1}) \]</span> We can interpret <span class="math inline">\(K\)</span> as a compensation term for model prediction uncertainty.</p><p>A large <span class="math inline">\(K\)</span> means there is much more noise in state prediction (a relatively larger <span class="math inline">\(P_{k|k-1}\)</span>) than in observation prediction (a relatively smaller <span class="math inline">\(Var(z_k - \hat z_{k|k-1})\)</span>), then we apply a large correction term to apriori estimate <span class="math inline">\(\hat x_{k|k-1}\)</span> to approximate a more accurate posteriori <span class="math inline">\(x_{k|k}\)</span>. Otherwise, a small <span class="math inline">\(K\)</span> means the current prediction <span class="math inline">\(\hat x_{k|k-1}\)</span> makes some sense so that a small correction is needed.</p><p>How is <span class="math inline">\(K\)</span> derived exactly? Follow the interpretation of <span class="math inline">\(K\)</span>, we can think of <span class="math inline">\(K\)</span> must be the result of minimizing the state prediction error <span class="math inline">\(P_{k|k}\)</span>. So let's express <span class="math inline">\(P_{k|k}\)</span> as below <span class="math display">\[ \begin {align*} P_{k|k} &amp;= Var(x_k - \hat x_{k|k}) = Var(x_k - [\hat x_{k|k-1} + K_k (z_k - H_k \hat x_{k|k-1})]) \\ &amp;= Var(x_k - [\hat x_{k|k-1} + K_k (H_k x_{k} + \epsilon_k - H_k \hat x_{k|k-1})]) \\ &amp;= Var(x_k - \hat x_{k|k-1} - K_k H_k(x_{k} - \hat x_{k|k-1}) - K_k\epsilon_k ) \\ &amp;= Var((I - K_k H_k)(x_{k} - \hat x_{k|k-1})) + Var(K_k\epsilon_k ) \\ &amp;= (I - K_k H_k)P_{k|k-1}(I - K_k H_k)^T + K_k R_k K_k^T\\ \end {align*} \]</span> Now we introduce <span class="math inline">\(S_k = Var(z_k - \hat z_{k|k-1}) = H_k P_{k|k-1} H_k^T + R_k\)</span>, then we have <span class="math display">\[ P_{k|k} = P_{k|k-1} - K_k H_k P_{k|k-1} - P_{k|k-1}H_k^T K_k^T + K_k S_k K_k^T \]</span> Now minimize the trace of <span class="math inline">\(P_{k|k}\)</span> to minimize the squared error, so we have <span class="math display">\[ \frac{\partial tr(P_{k|k})}{\partial K_k} = -2P_{k|k-1} H_k^T + 2K_k S_k = 0 \]</span> then we get <span class="math inline">\(K_k = P_{k|k-1} H_k^T S_k^{-1}\)</span></p></li></ol><p><strong>Kalman Filter Bayesian Derivation</strong></p><p>We stick to the recursion formula derived by Bayesian rules: <span class="math display">\[ Bel(x_k) = \eta p(z_k|x_k) \int p(x_k|x_{k-1},u_k) Bel(x_{k-1})dx_{k-1} \]</span> Let's give an initial condition <span class="math inline">\(Bel(x_0) = N(x_0, P_0)\)</span>, then for <span class="math inline">\(k=1\)</span> with state dynamics <span class="math inline">\(x_k = F_k x_{k-1} + B_k u_k + w_k\)</span>, we know <span class="math inline">\(p(x_1|x_{0},u_1) = N(F_1 x_0 + B_1 u_1, Q_1)\)</span>.</p><p>According to <a href="https://users.aalto.fi/~ssarkka/course_k2016/handout3.pdf#page=14" target="_blank" rel="noopener">convolution rules</a> in Gaussian distribution, we can get <span class="math display">\[ \overline {Bel}(x_1) = N(F_1 x_0 + B_1 u_1, F_1 P_0 F_1^T + Q_1) \]</span> The same goes for any other <span class="math inline">\(k\geq 1\)</span> as the recursion holds true for growing <span class="math inline">\(k\)</span>, which is <span class="math display">\[ \overline {Bel}(x_k) = N(F_k x_{k-1} + B_k u_k, F_k P_{k-1} F_k^T + Q_k) \]</span> For the observation-state mapping part, we know <span class="math inline">\(p(z_k|x_k) = N(H_k x_k, R_t)\)</span>, so the problem now becomes computing the distribution of 2 Gaussian variables. This will definitely lead to the same results above like <span class="math display">\[ \eta N(H_k x_k, R_t) N(F_k x_{k-1} + B_k u_k, F_k P_{k-1} F_k^T + Q_k) \\ = N(F_k x_{k-1} + B_k u_k + K_k (z_k - H_k (F_k x_{k-1} + B_k u_k)), [I - K H_k] P_{k|k-1}) \]</span> for now I have no idea how this can be derived rigorously, but <a href="http://www.tina-vision.net/docs/memos/2003-003.pdf" target="_blank" rel="noopener">P.A. Bromiley, Products and Convolutions of Gaussian Probability Density Functions</a> introduces deriving distribution of the product of n univariate Gaussian variables.</p><p><strong>Reference</strong>:</p><p>[1] <a href="https://www.cnblogs.com/ycwang16/p/5999034.html" target="_blank" rel="noopener">细说Kalman滤波：The Kalman Filter</a></p><p>[2] <a href="https://users.aalto.fi/~ssarkka/course_k2016/handout3.pdf" target="_blank" rel="noopener">Simo Särkkä, Bayesian Filtering Equations and Kalman Filter</a></p><p>[3] <a href="http://www.cs.cmu.edu/~16831-f14/notes/F14/16831_lecture02_prayana_tdecker_humphreh.pdf" target="_blank" rel="noopener">Statistical Techniques in Robotics (16-831, F10) Lecture #02 (Thursday, August 28)Bayes Filtering</a></p><p>[4] <a href="http://people.ciirc.cvut.cz/~hlavac/TeachPresEn/55AutonomRobotics/2015-05-04ReinsteinBayes-ekf.pdf" target="_blank" rel="noopener">Michal Reinštein, From Bayes to Extended Kalman Filter</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;kalman-filter&quot;&gt;Kalman Filter&lt;/h2&gt;&lt;h3 id=&quot;state-space-models&quot;&gt;0. State Space Models&lt;/h3&gt;&lt;p&gt;In &lt;a href=&quot;https://en.wikipedia.org/wiki/
      
    
    </summary>
    
      <category term="Quant" scheme="http://yoursite.com/categories/Quant/"/>
    
    
  </entry>
  
  <entry>
    <title>Canonical Component Analysis</title>
    <link href="http://yoursite.com/2019/10/20/CCA/"/>
    <id>http://yoursite.com/2019/10/20/CCA/</id>
    <published>2019-10-20T22:22:44.000Z</published>
    <updated>2020-04-24T01:39:13.476Z</updated>
    
    <content type="html"><![CDATA[<h2 id="a-quick-question">0. A Quick Question</h2><p>Let's first look at a question: Here is a risk report for 2 underlying yield sensitivity (Position sensitivity) { 10yr yield sensitivity: - 100k / bp { 30yr yield sensitivity: + 100k / bp So How do we hedge this risk?</p><p>The most common thing one can think of is doing oridinal leaset regression. The problem with this method is that the in-sample regression relationship will easily break out of sample. <strong>There should be an example snippet.</strong> However we have more pratical method to find the hedge ratio. And this is where CCA should be introduced.</p><h2 id="an-introduction-to-cca">1. An Introduction to CCA</h2><p>Say there is a pair of cmt rates - (<span class="math inline">\(Y_t^1\)</span>, <span class="math inline">\(Y_t^2\)</span>). What CCA is trying to do is to find a coitegration vector <span class="math inline">\([1, -\gamma]^T\)</span> to let the following equation stand</p><p><span class="math display">\[ Y_t^1 - \gamma \cdot Y_t^2 = \mu + \epsilon_t\]</span></p><p>where <span class="math inline">\(\epsilon_t\)</span> is a stationary white noise process, and therefore also is a <strong>mean-reverting</strong> process.</p><p><strong>This seems like doing OLS regression. But what is the difference?</strong></p><p>What if we extend a pair to a portfolio consisting of more than n cmt rates <span class="math inline">\(Y_t = [Y_t^1, Y_t^2, ..., Y_t^n]^T\)</span>? The problem also focusing on find a coitegration vector <span class="math inline">\(\gamma = [\gamma_1, \gamma_2, ..., \gamma_n]^T\)</span> to let the following equation stand</p><p><span class="math display">\[ \gamma^T Y_t = \mu + \epsilon_t \]</span> Theoretically, we can find a cointegration vector to build a mean-reverting time series among as many assets as we want. But in practice, it's hard to execute them all with proper prices at one time, and the execution costs are also very high.</p><p>So how do we find the cointegration vector <span class="math inline">\(\gamma\)</span>? Actually, <span class="math inline">\(\gamma\)</span> is such a vector yielded by CCA that makes <span class="math inline">\(\gamma^T Y_t\)</span> a cononical variable, even it's the least predictable variable.</p><p>There is also one thing to know - Will regression with different order yield different sets of cointegration vectors? <strong>Seems like the cointegration vectors are the same.</strong> (<strong>Formula and derivation needed.</strong>)</p><h2 id="derivation-of-cca">2. Derivation of CCA</h2><p>Let's look at how CCA is rigorously derived. There are 2 similar ways of performing CCA - one is introduced in <a href="http://pages.stern.nyu.edu/~dbackus/BCZ/HS/BoxTiao_canonical_Bio_77.pdf" target="_blank" rel="noopener">Box-Tiao(1977)</a> and another is introduced in <a href="https://www.elibrary.imf.org/doc/IMF001/01258-9781451950700/01258-9781451950700/Other_formats/Source_PDF/01258-9781451999181.pdf" target="_blank" rel="noopener">Chou-Ng(1994)</a>. Here we consider the former paper.</p><h3 id="self-predictability-measure">2.1 Self-predictability Measure</h3><p>Consider a $1 k $ vector process <span class="math inline">\(\{\mathbb{Z_t}\}\)</span> and let <span class="math inline">\(z_t = \mathbb{Z_t} - \mu\)</span>, where <span class="math inline">\(\mu\)</span> is a convenient <span class="math inline">\(1 \times k\)</span> vector of origin which is the mean if the process is stationary. Suppose <span class="math inline">\(z_t\)</span> follows the <em>p</em>th order multiple autoregressive model</p><p><span class="math display">\[z_t = \hat z_{t-1}(1) + a_t\]</span></p><p>where</p><p><span class="math display">\[\hat z_{t-1}(1) = E(z_t|z_{t-1}, z_{t-2},..) = \sum_{l=1}^{p}z_{t-l} \pi_l\]</span></p><p>is the expectation of <span class="math inline">\(z_t\)</span> conditional on past history up to time <span class="math inline">\(t-1\)</span>, the <span class="math inline">\(\pi_l\)</span> are <span class="math inline">\(k \times k\)</span> matrices, <span class="math inline">\(\{ a_t\}\)</span> is a sequence of independtly and normally distributed <span class="math inline">\(1 \times k\)</span> vector random shocks with mean zero and covariance matrix <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(a_t\)</span> is independent of <span class="math inline">\(\hat z_{t-1}(1)\)</span> - like the assumptions in OLS. And the <span class="math inline">\(AR(p)\)</span> model can be then represented as</p><p><span class="math display">\[z_t (I - \sum_{l=1}^{p}\pi_l B^l) = a_t\]</span></p><p>where <span class="math inline">\(I\)</span> is the identity matrix and <span class="math inline">\(B\)</span> is the backshift operator such that <span class="math inline">\(B z_t = z_{t-1}\)</span>.</p><p>The process <span class="math inline">\(\{z_t\}\)</span> is stationary if the determinantal polynomial in <span class="math inline">\(B\)</span>, <span class="math inline">\(det(I - \sum_{l=1}^{p}\pi_l B^l)\)</span> has its zeros lying outside the unit circle(<strong>?? recall AR(1) has its coef &lt; 1</strong>), and otherwise the process will be called non-stationary.</p><p>Now, let's make the problem simpler by setting <span class="math inline">\(k=1\)</span> to narrow down to only 1 time series. Then, if the process is stationary (<strong>if not, can we still derive this?</strong> - look back on stationary's condtions), due to <span class="math inline">\(a_t\)</span> being independent of <span class="math inline">\(\hat z_{t-1}(1)\)</span>.</p><p><span class="math display">\[E(z_t^2) = E(\{\hat z_{t-1}(1)\}^2) + E(a_t^2)\]</span></p><p>which can be also written as</p><p><span class="math display">\[ \sigma_z^2 = \sigma_{\hat z}^2 + \sigma_a^2\]</span></p><p>We can then define a quantity <span class="math inline">\(\lambda\)</span> to measure the predictability of a stationary series from its past as <span class="math inline">\(\lambda = \frac{\sigma_{\hat z}^2}{\sigma_z^2} = 1 - \frac{\sigma_a^2}{\sigma_z^2}\)</span>.</p><p><strong>Note</strong>: the derivation above only applies to 1 time series. And now <span class="math inline">\(z_t\)</span> is assumed to be stationary.</p><h3 id="intuition-of-cca-decomposition">2.2 Intuition of CCA Decomposition</h3><p>Now let's consider <span class="math inline">\(k\)</span> processes <span class="math inline">\(z_t\)</span> which represent <span class="math inline">\(k\)</span> diffrent stock market indexes such as <em>Dow Jones Average</em>, <em>Standard and Poors</em> and <em>Russell Index</em>, etc., all of which exhibit dynamic growth.</p><p>It is natural to conjecture that <strong>each</strong> might be represented as some aggregate of one or more common inputs which may be nearly nonstationary (<strong>momentum</strong>), together with other stationary(<strong>mean-reverting</strong>) or white noise components.</p><p>In other words. This leads us to contemplate <strong>linear aggregates of the form <span class="math inline">\(u_t = z_t m\)</span></strong>, where <span class="math inline">\(m\)</span> is such a <span class="math inline">\(k \times 1\)</span> vector that <strong>make <span class="math inline">\(u_t\)</span> a momentum or mean-reverting time series.</strong></p><blockquote><p>Note: <span class="math inline">\(z_t\)</span> is a vector consisting of k time series. And different <span class="math inline">\(m\)</span> will yield different <span class="math inline">\(u_t\)</span>. These different time series <span class="math inline">\(u_t\)</span> (whether mean-reverting or momentum) are <strong>aggregates</strong>. And these aggregates are <strong>derived</strong> time series from <span class="math inline">\(z_t\)</span>. The process of getting <span class="math inline">\(u_t\)</span> from <span class="math inline">\(z_t\)</span> is called <strong>CCA decompositon</strong> even if these 2 processes are not in the same level (<span class="math inline">\(u_t\)</span> is a transformed or derived process.)</p></blockquote><p><strong>(Can <span class="math inline">\(z_t\)</span> be represented as a linear combination of <span class="math inline">\(u_t\)</span>?)</strong></p><p>The aggregates <span class="math inline">\(u_t\)</span> which depend most heavily on the past, namely having large <span class="math inline">\(\lambda\)</span> (refers to <span class="math inline">\(u_t\)</span>'s <span class="math inline">\(\lambda\)</span>), may serve as useful composite indicators of the overall growth of the stock market (<strong>momentum</strong>). By contrast, the aggregates with <span class="math inline">\(\lambda\)</span> nearly zero may reflect stable contemporaneous relationships (<strong>mean-reverting</strong>) among the orignal indicators.</p><p>The analysis given in this paper yields <span class="math inline">\(k\)</span> 'conancial' components <span class="math inline">\(u_t\)</span> from laest to most predictable. Thus we may usefully decompose the k-dimensional space of the observation <span class="math inline">\(z_t\)</span> into stationary and non-stationary subspaces.</p><h3 id="derive-canonical-variables">2.3 Derive Canonical Variables</h3><p>Let <span class="math inline">\(\Gamma_j(z) = E(z_t^T z_{t-j})\)</span> be the lag <span class="math inline">\(j\)</span> autocovariance matrix of <span class="math inline">\(z_t\)</span>. In the variance form, we have (<strong>the first '=' needs to be proved further.</strong>)</p><p><span class="math display">\[ \Gamma_0(z) = \sum_{l=1}^{p} \Gamma_l(z)\pi_l + \Sigma= \Gamma_0(\hat z) + \Sigma \]</span></p><p>say, where <span class="math inline">\(\Gamma_0(\hat z)\)</span> is the covariance matrix of <span class="math inline">\(\hat z_{t-1}(1)\)</span>. <strong>Until further notice, we shall assume that <span class="math inline">\(\Sigma\)</span> and therefore <span class="math inline">\(\Gamma_0(z)\)</span> are <a href>postive-defnite</a>.</strong></p><p>Now, consider the linear combination <span class="math inline">\(u_t = z_t m\)</span>. For <span class="math inline">\(u_t\)</span>, we have that <span class="math inline">\(u_t = \hat u_{t-1}(1) + v_t\)</span>, where <span class="math inline">\(\hat u_{t-1}(1) = \hat z_{t-1}(1) m\)</span> and <span class="math inline">\(v_t=a_t m\)</span>. The predictability of <span class="math inline">\(u_t\)</span> from its past is therefore measured by</p><p><span class="math display">\[ \lambda = \sigma_{\hat u}^2 \sigma_{u}^{-2} = \{ m \Gamma_{0}(\hat z) m^T \} \{m \Gamma_{0}(z) m^T \}^{-1}\]</span></p><p>which can be represented in matrix form as</p><p><span class="math display">\[ \Lambda = M \Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z) M^{-1}\]</span></p><blockquote><p>Note: <span class="math inline">\(M\)</span> is what we are looking for. The logic is we want to find a transformed process <span class="math inline">\(\{u_t\}\)</span> which is generated by <span class="math inline">\(M\)</span> and the original <span class="math inline">\(z_t\)</span>. And we derive that <span class="math inline">\(M\)</span> can be found using eigendecomposition of <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span>.</p></blockquote><p>This is what we call <a href>eigendecomposition</a>, and therefore we can conclude that for the maximum predictability, <span class="math inline">\(\lambda\)</span> must be the maximum eigenvalue of <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span> and <span class="math inline">\(m\)</span> the corresponding eigenvector that makes <span class="math inline">\(u_t\)</span> a momentum time series. Similarly, the eigenvector that corresponds to the smallest eigenvalue will yield the least predictable combination of <span class="math inline">\(z_t\)</span>. <strong>This vector is referred to as cointegration vector</strong> that is mainly used in the first question (risk hedging) mentioned at the very beginning.</p><h3 id="conanical-transformation">Conanical Transformation</h3><p>This chapter is a bit like PCA transformation.</p><p>Let <span class="math inline">\(\lambda_1, ..., \lambda_k\)</span> be the k real eigenvalues of matrix <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span>. Suppose <span class="math inline">\(\lambda_j\)</span> are ordered with <span class="math inline">\(\lambda_1\)</span> the smallest, and that the k corresponding <a href><strong>linearly independent eigenvectors</strong></a>, <span class="math inline">\(m_1, .., m_k\)</span> from the <span class="math inline">\(k\)</span> columns of a matrix <span class="math inline">\(M\)</span>. Then, we can construct a transformed process <span class="math inline">\(\{ y_t\}\)</span>, where</p><p><span class="math display">\[y_t = \hat y_{t-1} (1) + b_t\]</span></p><p>with</p><p><span class="math display">\[y_t = z_t M, b_t = a_t M, \hat y_{t-1}(1)=\sum_{l=1}^{p} y_{t-l}\pi^1_l\]</span></p><p>where <span class="math inline">\(\pi^1_l=M^{-1} \pi_l M\)</span></p><p>We now also have</p><p><span class="math display">\[\Gamma_0(y) = \Gamma_0(\hat y) + \Sigma^1\]</span></p><p>where <span class="math inline">\(\Gamma_0(y)=M \Gamma_0(z)M^T, \Gamma_0(\hat y)=M \Gamma_0(\hat z)M^T, \Sigma^1=M \Sigma M^T\)</span></p><p>Note: - $ M <em>{0}(z) </em>{0}^{-1}(z) M^{-1} = , M _{0}^{-1}(z) M^{-1} = I - $ where <span class="math inline">\(\Lambda\)</span> is a <span class="math inline">\(k \times k\)</span> matrix with elements <span class="math inline">\((\lambda_1, .., \lambda_k)\)</span> - <span class="math inline">\(0 \leq \lambda_j &lt; 1 \space (j=1,..,k)\)</span> - for <span class="math inline">\(i \neq j, m_i \Gamma_0(z) m_j^T = m_i \Sigma m_j^T = 0\)</span>. This makes <span class="math inline">\(\Gamma_0(y), \Gamma_0(\hat y), \Sigma^1\)</span> all diagonal (Otherwise, <span class="math inline">\(\Lambda\)</span> would not be diagonal).</p><p><strong>(And this can be proved by constructing an equation of the previous <span class="math inline">\(\lambda\)</span> formula, which is inspired by my friend Nick.)</strong></p><p>With this <strong>diagonal</strong> propery, we can conclude that this transformation has produced <span class="math inline">\(k\)</span> components series <span class="math inline">\(\{ y_{1t}, y_{2t}, .., y_{kt}\}\)</span> which are</p><ul><li>ordered from least predictable to most predictable (meaning self-predictbility)</li><li>are contemporaneously independent</li><li>have predictable components <span class="math inline">\(\{\hat y_{1(t-1)}(1), y_{2(t-1)}(1), .., y_{k(t-1)}(1)\}\)</span> which are also contemporaneously independent</li><li>the same goes for <span class="math inline">\(\{ b_{1t}, b_{2t}, .., b_{kt}\}\)</span></li></ul><p><strong>Note</strong>: The content above goes for general time series, and the content below goes for AR(1) time series.(Also <span class="math inline">\(M\)</span> above can be computed in another way.)</p><h3 id="example">Example</h3><p>Say we have <a href="https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=yield" target="_blank" rel="noopener">Constant Maturity Treasury</a> rates (CMT rates) data <span class="math inline">\(\{z_t\}\)</span> from <span class="math inline">\(02/01/2012\)</span> to <span class="math inline">\(06/30/2015\)</span>, a part of which is given below.</p><table><thead><tr class="header"><th>Date</th><th>6 Mo</th><th>1 Yr</th><th>2 Yr</th><th>3 Yr</th><th>5 Yr</th><th>...</th><th>30 Yr</th></tr></thead><tbody><tr class="odd"><td>2015-06-30</td><td>0.11</td><td>0.28</td><td>0.64</td><td>1.01</td><td>1.63</td><td>...</td><td>3.11</td></tr><tr class="even"><td>2015-06-29</td><td>0.11</td><td>0.27</td><td>0.64</td><td>1.00</td><td>1.62</td><td>...</td><td>3.09</td></tr><tr class="odd"><td>2015-06-26</td><td>0.08</td><td>0.29</td><td>0.72</td><td>1.09</td><td>1.75</td><td>...</td><td>3.25</td></tr><tr class="even"><td>2016-06-25</td><td>0.07</td><td>0.29</td><td>0.68</td><td>1.06</td><td>1.70</td><td>...</td><td>3.16</td></tr><tr class="odd"><td>....</td><td>....</td><td>....</td><td>....</td><td>....</td><td>....</td><td>...</td><td>....</td></tr><tr class="even"><td>2012-02-01</td><td>0.09</td><td>0.13</td><td>0.23</td><td>0.31</td><td>0.72</td><td>...</td><td>3.01</td></tr></tbody></table><p>In this period of time, these CMT rates time series <span class="math inline">\(\{z_t\}\)</span> are both momentum time series. When we try to fit <span class="math inline">\(AR(1)\)</span> with these sereis with different maturities separately, the <span class="math inline">\(AR(1)\)</span> decaying parameters are around <span class="math inline">\(0.95\)</span> - <span class="math inline">\(0.99\)</span> (2.5 years is not a short term and both of these rates are contemporaneously under the same influence. So in the long term, they are both presenting similar trends).</p><p>But after we do conanical transformation to construct new time series (just as we discussed above) <span class="math inline">\(\{y_t\}\)</span>, the most mean-reverting series has a <span class="math inline">\(0.51\)</span> decaying parameter in <span class="math inline">\(AR(1)\)</span> fitting.</p><figure><img src="CCA/CCA.png" alt="CCA"><figcaption>CCA</figcaption></figure><p>Note the constructed seires <span class="math inline">\(\{y_t\}\)</span> are not corresponding to the orignal cmt rates series <span class="math inline">\(\{z_t\}\)</span>. This is similar to what is given by PCA - the first principle component is not corresponding to the first column of the orignal panel data.</p><p>Application 1. Spot small mean-reverting portfolios. 2. CCA decomposition to generate detrended data.</p><p>(<strong>Snippets Needed</strong>)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;a-quick-question&quot;&gt;0. A Quick Question&lt;/h2&gt;&lt;p&gt;Let&#39;s first look at a question: Here is a risk report for 2 underlying yield sensitivit
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>post</title>
    <link href="http://yoursite.com/2019/08/19/post-1/"/>
    <id>http://yoursite.com/2019/08/19/post-1/</id>
    <published>2019-08-19T15:25:41.000Z</published>
    <updated>2019-09-05T10:28:41.001Z</updated>
    
    <content type="html"><![CDATA[<h2 id="intro-to-fiqt---eurodollar-futures">1. Intro to FIQT - Eurodollar Futures</h2><h4 id="convexity-adjustment">Convexity Adjustment</h4><p>(A time scope grapsh is needed here...)</p><p>There exists a convexity adjustment between LIBOR forward contracts and Eurodollar futures. Hos is this derived is shown below</p><p>Assume <span class="math inline">\(\delta(t, T)\)</span> is the forward price at time <span class="math inline">\(t\)</span> which expires at time <span class="math inline">\(T\)</span>, and <span class="math inline">\(L(T)\)</span> is the LIBOR rate at time <span class="math inline">\(T\)</span>, <span class="math inline">\(r(t)\)</span> is the risk-free rate process. We can easily know the rational price of zero coupon bond at time <span class="math inline">\(t\)</span> which expires at time <span class="math inline">\(T\)</span>, or <span class="math inline">\(T\)</span>-forward numeraire is <span class="math display">\[ p(t, T) = E_t^Q[e^{-\int_{t}^{T}r(u)du}]\]</span></p><p>For a LIBOR forward <strong>contract</strong> that was entered at time <span class="math inline">\(t\)</span>, its contact value is 0, which is $$ <span class="math display">\[\begin{equation} \begin{aligned} 0 &amp; = E_t^Q \left[ e^{-\int_{t}^{T}r(u)du} [\delta(t, T) - L(T)] \right] \\ &amp; = E_t^{Q}\left[ e^{-\int_{t}^{T}r(u)du} \delta(t, T) - e^{-\int_{t}^{T}r(u)du} L(T)\right]\\ &amp; = E_t^{Q}\left[ e^{-\int_{t}^{T}r(u)du} \delta(t, T) \right] - E_t^{Q}\left[ e^{-\int_{t}^{T}r(u)du} \right] E_t^Q [L(T)] - cov \left( e^{-\int_{t}^{T}r(u)du}, L(T) \right) \\ &amp; = \delta(t, T) \cdot p(t, T) - p(t, T) \cdot E_t^Q [L(T)] - cov \left( e^{-\int_{t}^{T}r(u)du}, L(T) \right)\\ \end{aligned} \end{equation}\]</span> <span class="math display">\[ where we apply the covriance equation $cov(X, Y) = E[X Y] - E[X] E[Y]$, so we have \]</span>(t, T) = E_t^Q [L(T)] + cov ( e^{-_{t}^{T}r(u)du}, L(T) )$$</p><h4 id="npv-effect">NPV Effect</h4><p>Consider the value of a forward contract at <span class="math inline">\(t&#39; &gt; t\)</span> under CSA, a contract that was entered at time <span class="math inline">\(t\)</span>, so the difference in contract values on <span class="math inline">\(t&#39;\)</span> and <span class="math inline">\(t\)</span> that exchanges hands at <span class="math inline">\(t&#39;\)</span> is equal to <span class="math display">\[ V(t&#39;) - V(t) = E_{t&#39;} \left( e^{-\int_{t&#39;}^{T}r_c(u)du} \right) (F_{CSA}(t&#39;, T) - F_{CSA}(t, T)) \]</span> while the difference of futures contract will not be discounted, which is <span class="math display">\[ F(t&#39;) - F(t) = F_{CSA}(t&#39;, T) - F_{CSA}(t, T) \]</span></p><p>Let's take a look at another example in Pieterbarg(2010)</p><h2 id="historical-factor-model">2. Historical Factor Model</h2><h4 id="canonical-component-anaylsis">Canonical Component Anaylsis</h4><p>Let's first look at a question: Here is a risk report for 2 underlying yield sensitivity (Position sensitivity) { 10yr yield sensitivity: - 100k / bp { 30yr yield sensitivity: + 100k / bp So How do we hedge this risk?</p><p>The most common thing one can think of is oridinal leaset regression. The problem with this method is that the in-sample regression relationship will easily break out of sample. ** There should be an example snippet.** However we have more pratical method to find the hedge ratio. And this is where CCA should be introduced.</p><p>Say there is a pair of cmt rates - (<span class="math inline">\(Y_t^1\)</span>, <span class="math inline">\(Y_t^2\)</span>), what we are trying to do is find a coitegration vector to let the following equation stand</p><p><span class="math display">\[ Y_t^1 - \gamma \cdot Y_t^2 = \mu + \epsilon_t\]</span></p><p>where <span class="math inline">\(\epsilon_t\)</span> is a stationary white noise process.</p><p>What if we extend a pair to a portfolio consisting of more than n cmt rates <span class="math inline">\(Y_t = [Y_t^1, Y_t^2, ..., Y_t^n]^T\)</span>? The problem also focusing on find a coitegration vector <span class="math inline">\(\gamma = [\gamma_1, \gamma_2, ..., \gamma_n]^T\)</span> to let the following equation stand</p><p><span class="math display">\[ \gamma^T Y_t = \mu + \epsilon_t\]</span></p><p>Theoretically, we can find a cointegration vector to build a mean-reverting time series among as many assets as we want. But in practice, it's hard to execute them all with proper prices at one time, and the execution costs are also very high.</p><p>There is also one thing to know - Will regression with different order yield different sets of cointegration vectors? <strong>Seems like the cointegration vectors are the same.</strong> (<strong>Formula and derivation needed.</strong>)</p><p>Let's look at how CCA is rigorously derived. There are 2 similar ways of performing CCA - one is introduced in <a href="https://" target="_blank" rel="noopener">Box-Tiao(1977)</a> and another is introduced in <a href="https://" target="_blank" rel="noopener">Chou and Ng(19xx)</a>.</p><p>Consider a $1 k $ vector process <span class="math inline">\(\{\mathbb{Z_t}\}\)</span> and let <span class="math inline">\(z_t = \mathbb{Z_t} - \mu\)</span>, where <span class="math inline">\(\mu\)</span> is a convenient <span class="math inline">\(1 \times k\)</span> vector of origin which is the mean if the process is stationary. Suppose <span class="math inline">\(z_t\)</span> follows the <em>p</em>th order multiple autoregressive model</p><p><span class="math display">\[z_t = \hat z_{t-1}(1) + a_t\]</span></p><p>where</p><p><span class="math display">\[\hat z_{t-1}(1) = E(z_t|z_{t-1}, z_{t-2},..) = \sum_{l=1}^{p}z_{t-l} \pi_l\]</span></p><p>is the expectation of <span class="math inline">\(z_t\)</span> conditional on past history up to time <span class="math inline">\(t-1\)</span>, the <span class="math inline">\(\pi_l\)</span> are <span class="math inline">\(k \times k\)</span> matrices, <span class="math inline">\(\{ a_t\}\)</span> is a sequence of independtly and normally distributed <span class="math inline">\(1 \times k\)</span> vector random shocks with mean zero and covariance matrix <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(a_t\)</span> is independent of <span class="math inline">\(\hat z_{t-1}(1)\)</span> - like the assumptions in OLS. And the <span class="math inline">\(AR(p)\)</span> model can be then represented as</p><p><span class="math display">\[z_t (I - \sum_{l=1}^{p}\pi_l B^l) = a_t\]</span></p><p>where <span class="math inline">\(I\)</span> is the identity matrix and <span class="math inline">\(B\)</span> is the backshift operator such that <span class="math inline">\(B z_t = z_{t-1}\)</span>.</p><p>The process <span class="math inline">\(\{z_t\}\)</span> is stationary if the determinantal polynomial in <span class="math inline">\(B\)</span>, <span class="math inline">\(det(I - \sum_{l=1}^{p}\pi_l B^l)\)</span> has its zeros lying outside the unit circle(<strong>?? recall AR(1) has its coef &lt; 1</strong>), and otherwise the process will be called non-stationary.</p><p>Now, let's make the problem simpler by setting <span class="math inline">\(k=1\)</span> to narrow down to only 1 time series. Then, if the process is stationary (<strong>if not, can we still derive this?</strong> - look back on stationary's condtions), due to <span class="math inline">\(a_t\)</span> being independent of <span class="math inline">\(\hat z_{t-1}(1)\)</span>.</p><p><span class="math display">\[E(z_t^2) = E(\{\hat z_{t-1}(1)\}^2) + E(a_t^2)\]</span></p><p>which can be also written as</p><p><span class="math display">\[ \sigma_z^2 = \sigma_{\hat z}^2 + \sigma_a^2\]</span></p><p>We can then define a quantity <span class="math inline">\(\lambda\)</span> to measure the predictability of a stationary series from its past as <span class="math inline">\(\lambda = \frac{\sigma_{\hat z}^2}{\sigma_z^2} = 1 - \frac{\sigma_a^2}{\sigma_z^2}\)</span>. <strong>Note</strong>: the derivation above only applies to 1 time series.</p><h4 id="intuition">Intuition</h4><p>Now let's go back to the <span class="math inline">\(k \times 1\)</span> vector, and we can think of these <span class="math inline">\(k\)</span> processes represent <span class="math inline">\(k\)</span> diffrent stock market index such as <em>Dow Jones Average</em>, <em>Standard and Poors</em> and <em>Russell Index</em>, etc., all of which exhibit dynamic growth. It is natural to conjecture that <strong>each</strong> might be represented as some aggregate of one or more common inputs which may be nearly nonstationary (<strong>momentum</strong>), together with other stationary(<strong>mean-reverting</strong>) or white noise components.</p><p><strong>Here is the trick.</strong> It leads to a natrual contemplate linear aggregates of the form $u_t = z_t m $, where <span class="math inline">\(m\)</span> is the cononical matrix whose column are such vectors that <strong>make <span class="math inline">\(u_t\)</span> a multi time series consiting of momentum and mean-reverting time series.</strong></p><p>And we call these time series <strong>aggregates</strong>. The aggregates which depend most heavily on the past, namely having large <span class="math inline">\(\lambda\)</span> (<span class="math inline">\(\lambda\)</span> here refers to <span class="math inline">\(u_t\)</span>'s <span class="math inline">\(\lambda\)</span>), may serve as useful composite indicators of the overall growth of the stock market (<strong>momentum</strong>). By contrast, the aggregates with <span class="math inline">\(\lambda\)</span> nearly zero may reflect stable contemporaneous relationships (<strong>mean-reverting</strong>) among the orignal indicators.</p><p>The analysis given in this paper yields <span class="math inline">\(k\)</span> 'conancial' components from laest to most predictable. Thus we may usefully decompose the k-dimensional space of the observation <span class="math inline">\(z_t\)</span> into stationary and non-stationary subspaces.</p><h4 id="derive-canonical-variables">Derive Canonical Variables</h4><p>Let <span class="math inline">\(\Gamma_j(z) = E(z_t^T z_{t-j})\)</span> be the lag <span class="math inline">\(j\)</span> autocovariance matrix of <span class="math inline">\(z_t\)</span>. In the variance form, we have (<strong>the first '=' needs to be proved further.</strong>)</p><p><span class="math display">\[ \Gamma_0(z) = \sum_{l=1}^{p} \Gamma_l(z)\pi_l + \Sigma= \Gamma_0(\hat z) + \Sigma \]</span></p><p>say, where <span class="math inline">\(\Gamma_0(\hat z)\)</span> is the covariance matrix of <span class="math inline">\(\hat z_{t-1}(1)\)</span>. <strong>Until further notice, we shall assume that <span class="math inline">\(\Sigma\)</span> and therefore <span class="math inline">\(\Gamma_0(z)\)</span> are postive-defnite.</strong></p><p>Now, consider the linear combination <span class="math inline">\(u_t = z_t m\)</span>. For <span class="math inline">\(u_t\)</span>, we have that <span class="math inline">\(u_t = \hat u_{t-1}(1) + v_t\)</span>, where <span class="math inline">\(\hat u_{t-1}(1) = \hat z_{t-1}(1) m\)</span> and <span class="math inline">\(v_t=a_t m\)</span>. The predictability of <span class="math inline">\(u_t\)</span> from its past is therefore measured by</p><p><span class="math display">\[ \lambda = \sigma_{\hat u}^2 \sigma_{u}^{-2} = \{ m \Gamma_{0}(\hat z) m^T \} \{m \Gamma_{0}(z) m^T \}^{-1}\]</span></p><p>which can be represented in matrix form as</p><p><span class="math display">\[ \Lambda = M \Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z) M^{-1}\]</span></p><p>(Note: M is what we are looking for. The logic is we want to find a transformed process <span class="math inline">\(\{u_t\}\)</span> which is generated by <span class="math inline">\(M\)</span> and the original <span class="math inline">\(z_t\)</span>. And we derive that M can be found using eigenvector decmoposition of <span class="math inline">\(???\)</span>.)</p><p>This is what we call eigen vector decomposition, and therefore we can conclude that for the maximum predictability, <span class="math inline">\(\lambda\)</span> must be the eigenvalue of <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span> and <span class="math inline">\(m\)</span> the corresponding eigenvector that makes <span class="math inline">\(u_t\)</span> a momentum time series. Similarly, the eigenvector that corresponds to the smallest eigenvalue will yield the least predictable combination of <span class="math inline">\(z_t\)</span>. <strong>This vector is referred to as coitegration vector</strong> that is mainly used in the first question (risk hedging) mentioned at the very beginning.</p><h3 id="conanical-transformation">Conanical Transformation</h3><p>This chapter is a bit like PCA transformation.</p><p>Let <span class="math inline">\(\lambda_1, ..., \lambda_k\)</span> be the k real eigenvalues of matrix <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span>. Supposed <span class="math inline">\(\lambda_j\)</span> are ordered with <span class="math inline">\(\lambda_1\)</span> the smallest, and that the k corresponding linearly independent eigenvectors, <span class="math inline">\(m_1, .., m_k\)</span> from the k columns of a matrix <span class="math inline">\(M\)</span>. Then, we can construct a transformed process <span class="math inline">\(\{ y_t\}\)</span>, where</p><p><span class="math display">\[y_t = \hat y_{t-1} (1) + b_t\]</span></p><p>with</p><p><span class="math display">\[y_t = z_t M, b_t = a_t M, \hat y_{t-1}(1)=\sum_{l=1}^{p} y_{t-l}\pi^1_l, \pi^1_l=M^{-1} \pi_l M ???\]</span></p><p>We now also have</p><p><span class="math display">\[\Gamma_0(y) = \Gamma_0(\hat y) + \Sigma^1\]</span></p><p>where <span class="math inline">\(\Gamma_0(y)=M \Gamma_0(z)M^T, \Gamma_0(\hat y)=M \Gamma_0(\hat z)M^T, \Sigma^1=M \Sigma M^T\)</span></p><p>Note: - $ M <em>{0}(z) </em>{0}^{-1}(z) M^{-1} = , M _{0}^{-1}(z) M^{-1} = I - $ where <span class="math inline">\(\Lambda\)</span> is a <span class="math inline">\(k \times k\)</span> matrix with elements <span class="math inline">\((\lambda_1, .., \lambda_k)\)</span> - <span class="math inline">\(0 \leq \lambda_j &lt; 1 \space (j=1,..,k)\)</span> - This makes <span class="math inline">\(M \Sigma M^T, M \Gamma_0(\hat z) M^T, M \Gamma_0(z) M^T\)</span> are all diagnoal (easy to prove).</p><p>With this diagonal propery, we can conclude that this transformation has produced <span class="math inline">\(k\)</span> components series <span class="math inline">\(\{ y_{1t}, y_{2t}, .., y_{kt}\}\)</span> which are</p><ul><li>ordered from least predictable to most predictable</li><li>are contemporaneously independent</li><li>have predictable components <span class="math inline">\(\{\hat y_{1(t-1)}(1), y_{2(t-1)}(1), .., y_{k(t-1)}(1)\}\)</span> which are also contemporaneously independent</li><li>the same goes for <span class="math inline">\(\{ b_{1t}, b_{2t}, .., b_{kt}\}\)</span></li></ul><p><strong>Note</strong>: The content above goes for general time series, and the content below goes for AR(1) time series.(Also <span class="math inline">\(M\)</span> above can be computed in another way.)</p><p>Application 1. Spot small mean-reverting portfolios. 2. CCA decomposition to generate detrended data.</p><p>(<strong>Snippets Needed</strong>)</p><h2 id="term-structure-model">3. Term Structure Model</h2><h2 id="tsm-fitting">4. TSM fitting</h2><h2 id="signal-research-framework">5. Signal Research Framework</h2><h2 id="livie-eurodollar-futures-trading">6. Livie Eurodollar Futures Trading</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;intro-to-fiqt---eurodollar-futures&quot;&gt;1. Intro to FIQT - Eurodollar Futures&lt;/h2&gt;&lt;h4 id=&quot;convexity-adjustment&quot;&gt;Convexity Adjustment&lt;/h4
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>电影的尽头是电影共产主义</title>
    <link href="http://yoursite.com/2019/07/30/post/"/>
    <id>http://yoursite.com/2019/07/30/post/</id>
    <published>2019-07-30T14:36:49.000Z</published>
    <updated>2019-08-05T19:52:18.003Z</updated>
    
    <content type="html"><![CDATA[<p>标题和副标题都是我瞎起的，只有摘要是认真写的。</p><p>起因是听观影风向标一五年戛纳那期节目时，被波米科普了戛纳颁奖的前前后后，其中谈论的马伯庸对《刺客聂隐娘》的批评言论及其引发的争论（马在未看过电影的前提下<a href="https://www.zhihu.com/question/30567872" target="_blank" rel="noopener">喷《聂》是连故事都讲不清的电影云云</a>，引发了包括但不限于[观众与导演表达权利是否对等]/[电影的故事中心论]等一系列争论），结合我后来了解到的波米，使我联想到了许多曾在我心中杂乱扎根但又从未厘清的一些树木。</p><h2 id="电影史话里的鸡和蛋">电影史话里的鸡和蛋</h2><p>关于电影本质的讨论无休无止，且从不会达成一致，如果沿着电影的原教旨轨迹出发，从卢米埃尔兄弟发明电影之初，到卡努杜等人为电影发表《第七艺术的诞生》，尚在探索阶段的电影并没有什么本质可言——拍电影几乎就是固定场景一镜到底——其艺术属性相比于其他艺术形式相当薄弱单调，但其间并不是没有先驱探索——梅里爱摒弃一镜到底拍出一众奇观电影，埃德温·鲍特最早使用镜头变换拍出《火车大劫案》——电影也才有了剪辑和镜头语言的概念，电影的叙事使命也应运而生，再到后来，格里菲斯将前人技法发扬光大发明了蒙太奇，至此，逐渐丰富的电影表现手法使得电影的叙事技法不断翻出花样，也使得电影的艺术属性得到初步完善。</p><p>你可以说，叙事需求是电影在形式和技法探索途中无可避免的产物，可以说叙事就是一种新探索出的电影技法，也可以说正是因为电影人为了更好地讲故事才催生了电影里那些先锋形式和革新技法，但不管怎么样，电影叙事作为从属于电影艺术形式的一个单位，都不能脱离电影媒介而独立存在——无论我们在谈论哪部电影故事讲的如何如何，它都不只是那个故事的文本本身——还天然包含了电影独有的艺术属性。</p><p>而被冠之以“不会讲故事”名号的《聂隐娘》正是通过一种全新的探索性的电影语言对一段时间/历史做出呈现，它打破的不仅是常见的电影叙事手法，还有传统武侠电影的类型技法（比如镜头调度/场景设置）。</p><h2 id="为什么要探索宇宙">为什么要探索宇宙？</h2><p>我始终觉得，看电影不是批大字报，不是党同伐异，是有来有回，是愿者上钩，关注它作为电影在整体和个体上的不同与相同，理解电影和电影以外，达成对作品和作者的理解，而退一步讲，如果我不喜欢不明白，能不能抑制住想要玩命否定立刻站队的冲动，而换一种更有效的办法来行使作为观众的权利去完成自我表达呢？在这里我想到了<a href="http://www.lettersofnote.com/2012/08/why-explore-space.html" target="_blank" rel="noopener">NASA和赞比亚修女Mary Jucunda之间的通信</a>，修女看到美国宇航局每年的巨额花费时，她说的是“当世界上还有那么多小孩吃不饱饭时你们为什么花这么多钱在一个探索火星的项目上”，而不是“当世界上还有那么多小孩吃不饱饭时你们花这么多钱探索火星真是<del><strong>残忍无情傻逼</strong></del>”，NASA方面给出的回信很长，最后一段是这样写的</p><blockquote><p>Very fortunately though, the space age not only holds out a mirror in which we can see ourselves, it also provides us with the technologies, the challenge, the motivation, and even with the optimism to attack these tasks with confidence. What we learn in our space program, I believe, is fully supporting what Albert Schweitzer had in mind when he said: &quot;I am looking at the future with concern, but with good hope.&quot;</p></blockquote><p>从某种意义上，修女对于NASA的疑惑也就像观众我辈对于《聂》的不解，就像是自此处向远方投去的眺望，和想要努力看清远方的热切，这样想的话侯导又何尝不是在眺望他自己的远方呢，用另一种方式去看目不能及足的世界，靠时间的堆积到达自己眺望过的地方，用福柯的话来讲就是<strong>“对知识的热情，如果仅仅导致某种程度的学识的增长，而不是以这样或那样的方式尽可能使求知者偏离自我的话，那这种热情还有什么价值可言？在人生中：如果人们进一步观察和思考，有些时候就绝对需要提出这样的问题：了解人能否采取与自己原有的思维方式不同的方式思考，能否采取与自己原有的观察方式不同的方式感知。”</strong>，这些不同本身就是意义所在，是特瑞吉列姆和詹姆斯卡梅隆向克利斯马克横跨三十年的致敬，是诺兰在雷乃将死之前完成的《盗梦空间》（虽然诺兰说他<a href="https://artsbeat.blogs.nytimes.com/2010/06/30/a-man-and-his-dream-christopher-nolan-and-inception/?mtrref=undefined&amp;gwh=F68694E1AE10624206DE88A261025F03&amp;gwt=pay" target="_blank" rel="noopener">没看过《去年在马里昂巴德》</a>，这又可以延申到其他话题——有关电影与评论的创作关系，以及历史的撞车），那些看上去空无一物的东西被时间赋予了丰富的普适意义，并将继续繁茂。</p><h2 id="马克思会梦见克里奥佩特拉的鼻子吗">马克思会梦见克里奥佩特拉的鼻子吗？</h2><p>以上讨论的语境是十足电影主义的，提起电影主义自然会想起网络迷踪那期节目里的桌面电影大盘点，以及随之而来的问题——桌面电影能代表电影吗？波米的担忧在于桌面电影失掉了基本的电影语言（剪辑）会使得电影界限变得模糊（<del>吗？</del>），此类视频化电影的兴起与其迅速增长的受众群体会降低了电影制作的门槛（<del>吗？</del>），电影会逐渐被视频化（<del>吗？</del>）</p><p>有一种从历史唯物主义出发的观点是这样的——即使电影变成那样，那也不过是历史所做出的客观扬弃，是历史使得电影从最初的卢米埃尔演变成如今的桌面电影，让电影艺术产生时间上不可逆的融合，你能阻挡历史进程的客观行进吗？我不能，但我可以选择我参与历史的方式，历史它对电影完成怎样的时代融合不取决于个人，但取决于每个个体的加总，是观众和作者的加总（我实在不想从什么历史唯物的角度讨论电影的走向，那种口吻实在像拿着某版标准答案狂喷自己学生无能的中学老师一样，未免太恃己太执果索因了些）。</p><h2 id="我自己画极坐标">我自己画极坐标</h2><p>说到这里，又可以延申出，观众和作者在电影融合途中的角色，我想这里应该有一个作品意识（观看意识/创作意识）的坐标轴，观众的最左是被投喂什么就吃什么（院线电影观众），最右则是不甘心做他人的茧（艺术馆观众），而导演的最左是唯观众与市场论者（商业片导演），最右则是绝对意义上的电影艺术探索者（艺术片导演）（我能立刻想到的就是阿巴斯，也大概有些理解为什么戈达尔说电影始于格里菲斯止于阿巴斯，因为无论是前者还是后者，他们对电影艺术可能性的探索都极具开创性，而阿巴斯又更纯粹一些，以至于让人相信无论是他的作品还是他对电影技作可能性的挖掘都饱满丰富，生生长流，当然这也与戈达尔本人对电影的实践做法相似），毫无疑问诺兰应该处于这个坐标的中间——似乎没有导演比他更懂大多数观众了，也似乎没有人比他在商业片类型化的创新上更娴熟了，但他能算作一个纯粹的作者导演吗？写到这里，也发现这个坐标轴只是狭隘一隅，除了时间维度的扩展（这里又可以延申到时间轴上的大起大落型选手），更应该加入新的维度。</p><h4 id="这简直是一个无穷无尽的dfs啊">这简直是一个无穷无尽的DFS啊！</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;标题和副标题都是我瞎起的，只有摘要是认真写的。&lt;/p&gt;&lt;p&gt;起因是听观影风向标一五年戛纳那期节目时，被波米科普了戛纳颁奖的前前后后，其中谈论的马伯庸对《刺客聂隐娘》的批评言论及其引发的争论（马在未看过电影的前提下&lt;a href=&quot;https://www.zhihu.com/
      
    
    </summary>
    
      <category term="物质现实复原" scheme="http://yoursite.com/categories/%E7%89%A9%E8%B4%A8%E7%8E%B0%E5%AE%9E%E5%A4%8D%E5%8E%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>Class bulti-in methods</title>
    <link href="http://yoursite.com/2019/07/13/test/"/>
    <id>http://yoursite.com/2019/07/13/test/</id>
    <published>2019-07-13T20:25:16.000Z</published>
    <updated>2019-07-30T14:32:48.296Z</updated>
    
    <content type="html"><![CDATA[<h3 id="getattribute__-method">1. __getattribute__ method</h3><p>This method usually implements a class's getter to <strong>get</strong> whatever you request (<strong>attributes and methods</strong>) from an object/a class.<br>I don't know for sure the difference with __getattr__</p><p>A common way of implementing this method is as follows:</p><div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a><a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw">class</span> test:</a><a class="sourceLine" id="cb1-3" data-line-number="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, lo<span class="op">=</span><span class="dv">1</span>, hi<span class="op">=</span><span class="dv">20</span>, size<span class="op">=</span><span class="dv">10</span>):</a><a class="sourceLine" id="cb1-4" data-line-number="4">        <span class="va">self</span>.array <span class="op">=</span> np.random.randint(low<span class="op">=</span>lo, high<span class="op">=</span>hi, size<span class="op">=</span>size)</a><a class="sourceLine" id="cb1-5" data-line-number="5"></a><a class="sourceLine" id="cb1-6" data-line-number="6">    <span class="kw">def</span> <span class="fu">__getattribute__</span>(<span class="va">self</span>, name):</a><a class="sourceLine" id="cb1-7" data-line-number="7">        builtin_members <span class="op">=</span> [<span class="st">&#39;count&#39;</span>]</a><a class="sourceLine" id="cb1-8" data-line-number="8">        <span class="cf">if</span> name <span class="kw">in</span> builtin_members: </a><a class="sourceLine" id="cb1-9" data-line-number="9">            <span class="cf">return</span> <span class="bu">super</span>().<span class="fu">__getattribute__</span>(name)</a><a class="sourceLine" id="cb1-10" data-line-number="10">        </a><a class="sourceLine" id="cb1-11" data-line-number="11">    <span class="kw">def</span> count(<span class="va">self</span>):</a><a class="sourceLine" id="cb1-12" data-line-number="12">        <span class="cf">return</span> np.size(<span class="va">self</span>.array)</a></code></pre></div><p><strong>Question</strong>: What is the following codes' output?</p><div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" data-line-number="1">t <span class="op">=</span> test()</a><a class="sourceLine" id="cb2-2" data-line-number="2"><span class="bu">print</span>(t.count())</a></code></pre></div><p>The answer is 1.</p><p>And what about the results when we change the test's structure to this:</p><div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" data-line-number="1"><span class="im">import</span> numpy <span class="im">as</span> np</a><a class="sourceLine" id="cb3-2" data-line-number="2"><span class="kw">class</span> test:</a><a class="sourceLine" id="cb3-3" data-line-number="3">    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, lo<span class="op">=</span><span class="dv">1</span>, hi<span class="op">=</span><span class="dv">20</span>, size<span class="op">=</span><span class="dv">10</span>):</a><a class="sourceLine" id="cb3-4" data-line-number="4">        <span class="va">self</span>.array <span class="op">=</span> np.random.randint(low<span class="op">=</span>lo, high<span class="op">=</span>hi, size<span class="op">=</span>size)</a><a class="sourceLine" id="cb3-5" data-line-number="5"></a><a class="sourceLine" id="cb3-6" data-line-number="6">    <span class="kw">def</span> <span class="fu">__getattribute__</span>(<span class="va">self</span>, name):</a><a class="sourceLine" id="cb3-7" data-line-number="7">        builtin_members <span class="op">=</span> [<span class="st">&#39;count&#39;</span>]</a><a class="sourceLine" id="cb3-8" data-line-number="8">        <span class="cf">if</span> name <span class="kw">in</span> builtin_members: </a><a class="sourceLine" id="cb3-9" data-line-number="9">            <span class="cf">return</span> <span class="bu">super</span>().<span class="fu">__getattribute__</span>(name)</a><a class="sourceLine" id="cb3-10" data-line-number="10">        <span class="cf">else</span>:</a><a class="sourceLine" id="cb3-11" data-line-number="11">            <span class="cf">return</span> <span class="bu">object</span>.<span class="fu">__getattribute__</span>(<span class="va">self</span>, name)</a><a class="sourceLine" id="cb3-12" data-line-number="12">        </a><a class="sourceLine" id="cb3-13" data-line-number="13">    <span class="kw">def</span> count(<span class="va">self</span>):</a><a class="sourceLine" id="cb3-14" data-line-number="14">        <span class="cf">return</span> np.size(<span class="va">self</span>.array)</a></code></pre></div><p>The answer is 10.</p><p>And the reason is 1. <code>test</code> inherits from <code>object</code>, so <code>super().__getattribute__(name)</code> = <code>object.__getattribute__(self, name)</code> 2. the first class doesn't implement the case when <strong>non-builtin members</strong> are retrived, so it automatically returns <code>None</code> whose size is 1.</p><p>Sometimes, there are many other complex implementation of an attribute getter.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;getattribute__-method&quot;&gt;1. __getattribute__ method&lt;/h3&gt;&lt;p&gt;This method usually implements a class&#39;s getter to &lt;strong&gt;get&lt;/strong&gt; wha
      
    
    </summary>
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
      <category term="Magic Methods" scheme="http://yoursite.com/categories/Python/Magic-Methods/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2019/06/24/hello-world/"/>
    <id>http://yoursite.com/2019/06/24/hello-world/</id>
    <published>2019-06-25T01:51:47.291Z</published>
    <updated>2019-06-25T01:51:47.291Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb1-1" data-line-number="1">$ <span class="ex">hexo</span> new <span class="st">&quot;My New Post&quot;</span></a></code></pre></div><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="run-server">Run server</h3><div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb2-1" data-line-number="1">$ <span class="ex">hexo</span> server</a></code></pre></div><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="generate-static-files">Generate static files</h3><div class="sourceCode" id="cb3"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb3-1" data-line-number="1">$ <span class="ex">hexo</span> generate</a></code></pre></div><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><div class="sourceCode" id="cb4"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb4-1" data-line-number="1">$ <span class="ex">hexo</span> deploy</a></code></pre></div><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
