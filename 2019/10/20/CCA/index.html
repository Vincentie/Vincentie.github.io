<!DOCTYPE html><html xmlns:v-bind="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="Hexo 3.9.0"><title>Canonical Component Analysis - Avalon</title><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="John Reese"><meta name="description" content="0. A Quick QuestionLet’s first look at a question:Here is..."><meta name="keywords" content><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous"><link rel="stylesheet" href="/css/journal.css?44469790"><script src="/js/loadCSS.js"></script><script>loadCSS("https://fonts.googleapis.com/css?family=Lora|Montserrat|Fira+Mono|Material+Icons"),function(e){var t,a={kitId:"dwg1tuc",scriptTimeout:3e3,async:!0},o=e.documentElement,s=setTimeout(function(){o.className=o.className.replace(/\bwf-loading\b/g,"")+" wf-inactive"},a.scriptTimeout),c=e.createElement("script"),i=!1,n=e.getElementsByTagName("script")[0];o.className+=" wf-loading",c.src="https://use.typekit.net/"+a.kitId+".js",c.async=!0,c.onload=c.onreadystatechange=function(){if(t=this.readyState,!(i||t&&"complete"!=t&&"loaded"!=t)){i=!0,clearTimeout(s);try{Typekit.load(a)}catch(e){}}},n.parentNode.insertBefore(c,n)}(document)</script><noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora|Montserrat|Anonymous+Pro:400|Material+Icons"></noscript><link rel="stylesheet" href="/css/prism.css" type="text/css"></head><body><div id="top"></div><div id="app"><div class="single-column-drawer-container" ref="drawer" v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }"><div class="drawer-content"><div class="drawer-menu"><a class="a-block drawer-menu-item false" href="http://yoursite.com">Home </a><a class="a-block drawer-menu-item false" href="/archives">記事一覽 </a><a class="a-block drawer-menu-item false" href="/about/index.html">關於我 </a><a class="a-block drawer-menu-item false" href="/categories/index.html">分類 </a><a class="a-block drawer-menu-item false" href="/tags/index.html">標簽</a></div></div></div><transition name="fade"><div v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if="isDrawerOpen" v-on:click="toggleDrawer"></div></transition><nav ref="navBar" class="navbar sticky-top navbar-light single-column-nav-container"><div ref="navBackground" class="nav-background"></div><div class="container container-narrow nav-content"><button id="nav_dropdown_btn" class="nav-dropdown-toggle" type="button" v-on:click="toggleDrawer"><i class="material-icons">menu</i></button> <a ref="navTitle" class="navbar-brand" href="/">Avalon</a></div></nav><div class="single-column-header-container" ref="pageHead" v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }"><a href="/"><div class="single-column-header-title">Avalon</div><div class="single-column-header-subtitle">Welcome!!</div></a></div><div ref="sideContainer" class="side-container"><a class="a-block nav-head false" href="/"><div class="nav-title">远い理想郷</div><div class="nav-subtitle">お帰りなさい</div></a><div class="nav-link-list"><a class="a-block no-tint nav-link-item false" href="/archives">記事一覽 </a><a class="a-block nav-link-item false" href="/about/index.html">關於我 </a><a class="a-block nav-link-item false" href="/categories/index.html">分類 </a><a class="a-block nav-link-item false" href="/tags/index.html">標簽</a></div><div class="nav-footer">Proudly published with Hexo<br>Theme <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> by <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a><br>&copy; 2020 <a href="http://yoursite.com">Avalon</a></div></div><div ref="extraContainer" class="extra-container"><div class="pagination"><a id="globalBackToTop" class="pagination-action animated-visibility" href="#top" :class="{ invisible: scrollY == 0 }"><i class="material-icons pagination-action-icon">keyboard_arrow_up</i></a></div></div><div ref="streamContainer" class="stream-container"><div class="post-list-container post-list-container-shadow"><div class="post"><div class="post-head-wrapper-text-only" style="background-image:url('')"><div class="post-title">Canonical Component Analysis<div class="post-meta"><time datetime="2019-10-20T22:22:44.000Z" itemprop="datePublished">2019-10-20 18:22 </time>&nbsp;</div></div></div><div class="post-body-wrapper"><div class="post-body"><h2 id="0-A-Quick-Question"><a href="#0-A-Quick-Question" class="headerlink" title="0. A Quick Question"></a>0. A Quick Question</h2><p>Let’s first look at a question:<br>Here is a risk report for 2 underlying yield sensitivity (Position sensitivity)<br>{ 10yr yield sensitivity: - 100k / bp<br>{ 30yr yield sensitivity: + 100k / bp<br>So How do we hedge this risk?</p><p>The most common thing one can think of is doing oridinal leaset regression. The problem with this method is that the in-sample regression relationship will easily break out of sample.<br><strong>There should be an example snippet.</strong><br>However we have more pratical method to find the hedge ratio. And this is where CCA should be introduced.</p><h2 id="1-An-Introduction-to-CCA"><a href="#1-An-Introduction-to-CCA" class="headerlink" title="1. An Introduction to CCA"></a>1. An Introduction to CCA</h2><p>Say there is a pair of cmt rates - ($Y_t^1$, $Y_t^2$). What CCA is trying to do is to find a coitegration vector $[1, -\gamma]^T$ to let the following equation stand</p><p>$$ Y_t^1 - \gamma \cdot Y_t^2 = \mu + \epsilon_t$$</p><p>where $\epsilon_t$ is a stationary white noise process, and therefore also is a <strong>mean-reverting</strong> process.</p><p><strong>This seems like doing OLS regression. But what is the difference?</strong></p><p>What if we extend a pair to a portfolio consisting of more than n cmt rates $Y_t = [Y_t^1, Y_t^2, …, Y_t^n]^T$? The problem also focusing on find a coitegration vector $\gamma = [\gamma_1, \gamma_2, …, \gamma_n]^T$ to let the following equation stand</p><p>$$<br>\gamma^T Y_t = \mu + \epsilon_t<br>$$<br>Theoretically, we can find a cointegration vector to build a mean-reverting time series among as many assets as we want. But in practice, it’s hard to execute them all with proper prices at one time, and the execution costs are also very high.</p><p>So how do we find the cointegration vector $\gamma$? Actually, $\gamma$ is such a vector yielded by CCA that makes $\gamma^T Y_t$ a cononical variable, even it’s the least predictable variable.</p><p>There is also one thing to know - Will regression with different order yield different sets of cointegration vectors? <strong>Seems like the cointegration vectors are the same.</strong> (<strong>Formula and derivation needed.</strong>)</p><h2 id="2-Derivation-of-CCA"><a href="#2-Derivation-of-CCA" class="headerlink" title="2. Derivation of CCA"></a>2. Derivation of CCA</h2><p>Let’s look at how CCA is rigorously derived. There are 2 similar ways of performing CCA - one is introduced in <a href="http://pages.stern.nyu.edu/~dbackus/BCZ/HS/BoxTiao_canonical_Bio_77.pdf" target="_blank" rel="noopener">Box-Tiao(1977)</a> and another is introduced in <a href="https://www.elibrary.imf.org/doc/IMF001/01258-9781451950700/01258-9781451950700/Other_formats/Source_PDF/01258-9781451999181.pdf" target="_blank" rel="noopener">Chou-Ng(1994)</a>. Here we consider the former paper.</p><h3 id="2-1-Self-predictability-Measure"><a href="#2-1-Self-predictability-Measure" class="headerlink" title="2.1 Self-predictability Measure"></a>2.1 Self-predictability Measure</h3><p>Consider a $1 \times k $ vector process ${\mathbb{Z_t}}$ and let $z_t = \mathbb{Z_t} - \mu$, where $\mu$ is a convenient $1 \times k$ vector of origin which is the mean if the process is stationary. Suppose $z_t$ follows the <em>p</em>th order multiple autoregressive model</p><p>$$z_t = \hat z_{t-1}(1) + a_t$$</p><p>where</p><p>$$\hat z_{t-1}(1) = E(z_t|z_{t-1}, z_{t-2},..) = \sum_{l=1}^{p}z_{t-l} \pi_l$$</p><p>is the expectation of $z_t$ conditional on past history up to time $t-1$, the $\pi_l$ are $k \times k$ matrices, ${ a_t}$ is a sequence of independtly and normally distributed $1 \times k$ vector random shocks with mean zero and covariance matrix $\Sigma$, and $a_t$ is independent of $\hat z_{t-1}(1)$ - like the assumptions in OLS. And the $AR(p)$ model can be then represented as</p><p>$$z_t (I - \sum_{l=1}^{p}\pi_l B^l) = a_t$$</p><p>where $I$ is the identity matrix and $B$ is the backshift operator such that $B z_t = z_{t-1}$.</p><p>The process ${z_t}$ is stationary if the determinantal polynomial in $B$, $det(I - \sum_{l=1}^{p}\pi_l B^l)$ has its zeros lying outside the unit circle(<strong>?? recall AR(1) has its coef &lt; 1</strong>), and otherwise the process will be called non-stationary.</p><p>Now, let’s make the problem simpler by setting $k=1$ to narrow down to only 1 time series. Then, if the process is stationary (<strong>if not, can we still derive this?</strong> - look back on stationary’s condtions), due to $a_t$ being independent of $\hat z_{t-1}(1)$.</p><p>$$E(z_t^2) = E({\hat z_{t-1}(1)}^2) + E(a_t^2)$$</p><p>which can be also written as</p><p>$$ \sigma_z^2 = \sigma_{\hat z}^2 + \sigma_a^2$$</p><p>We can then define a quantity $\lambda$ to measure the predictability of a stationary series from its past as $\lambda = \frac{\sigma_{\hat z}^2}{\sigma_z^2} = 1 - \frac{\sigma_a^2}{\sigma_z^2}$.</p><p><strong>Note</strong>: the derivation above only applies to 1 time series. And now $z_t$ is assumed to be stationary.</p><h3 id="2-2-Intuition-of-CCA-Decomposition"><a href="#2-2-Intuition-of-CCA-Decomposition" class="headerlink" title="2.2 Intuition of CCA Decomposition"></a>2.2 Intuition of CCA Decomposition</h3><p>Now let’s consider $k$ processes $z_t$ which represent $k$ diffrent stock market indexes such as <em>Dow Jones Average</em>, <em>Standard and Poors</em> and <em>Russell Index</em>, etc., all of which exhibit dynamic growth.</p><p>It is natural to conjecture that <strong>each</strong> might be represented as some aggregate of one or more common inputs which may be nearly nonstationary (<strong>momentum</strong>), together with other stationary(<strong>mean-reverting</strong>) or white noise components.</p><p>In other words. This leads us to contemplate <strong>linear aggregates of the form $u_t = z_t m$</strong>, where $m$ is such a $k \times 1$ vector that <strong>make $u_t$ a momentum or mean-reverting time series.</strong></p><blockquote><p>Note: $z_t$ is a vector consisting of k time series. And different $m$ will yield different $u_t$. These different time series $u_t$ (whether mean-reverting or momentum) are <strong>aggregates</strong>. And these aggregates are <strong>derived</strong> time series from $z_t$. The process of getting $u_t$ from $z_t$ is called <strong>CCA decompositon</strong> even if these 2 processes are not in the same level ($u_t$ is a transformed or derived process.)</p></blockquote><p><strong>(Can $z_t$ be represented as a linear combination of $u_t$?)</strong></p><p>The aggregates $u_t$ which depend most heavily on the past, namely having large $\lambda$ (refers to $u_t$’s $\lambda$), may serve as useful composite indicators of the overall growth of the stock market (<strong>momentum</strong>). By contrast, the aggregates with $\lambda$ nearly zero may reflect stable contemporaneous relationships (<strong>mean-reverting</strong>) among the orignal indicators.</p><p>The analysis given in this paper yields $k$ ‘conancial’ components $u_t$ from laest to most predictable. Thus we may usefully decompose the k-dimensional space of the observation $z_t$ into stationary and non-stationary subspaces.</p><h3 id="2-3-Derive-Canonical-Variables"><a href="#2-3-Derive-Canonical-Variables" class="headerlink" title="2.3 Derive Canonical Variables"></a>2.3 Derive Canonical Variables</h3><p>Let $\Gamma_j(z) = E(z_t^T z_{t-j})$ be the lag $j$ autocovariance matrix of $z_t$. In the variance form, we have (<strong>the first ‘=’ needs to be proved further.</strong>)</p><p>$$ \Gamma_0(z) = \sum_{l=1}^{p} \Gamma_l(z)\pi_l + \Sigma= \Gamma_0(\hat z) + \Sigma $$</p><p>say, where $\Gamma_0(\hat z)$ is the covariance matrix of $\hat z_{t-1}(1)$. <strong>Until further notice, we shall assume that $\Sigma$ and therefore $\Gamma_0(z)$ are <a href>postive-defnite</a>.</strong></p><p>Now, consider the linear combination $u_t = z_t m$. For $u_t$, we have that $u_t = \hat u_{t-1}(1) + v_t$, where $\hat u_{t-1}(1) = \hat z_{t-1}(1) m$ and $v_t=a_t m$. The predictability of $u_t$ from its past is therefore measured by</p><p>$$ \lambda = \sigma_{\hat u}^2 \sigma_{u}^{-2} = { m \Gamma_{0}(\hat z) m^T } {m \Gamma_{0}(z) m^T }^{-1}$$</p><p>which can be represented in matrix form as</p><p>$$ \Lambda = M \Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z) M^{-1}$$</p><blockquote><p>Note: $M$ is what we are looking for. The logic is we want to find a transformed process ${u_t}$ which is generated by $M$ and the original $z_t$. And we derive that $M$ can be found using eigendecomposition of $\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)$.</p></blockquote><p>This is what we call <a href>eigendecomposition</a>, and therefore we can conclude that for the maximum predictability, $\lambda$ must be the maximum eigenvalue of $\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)$ and $m$ the corresponding eigenvector that makes $u_t$ a momentum time series. Similarly, the eigenvector that corresponds to the smallest eigenvalue will yield the least predictable combination of $z_t$. <strong>This vector is referred to as cointegration vector</strong> that is mainly used in the first question (risk hedging) mentioned at the very beginning.</p><h3 id="Conanical-Transformation"><a href="#Conanical-Transformation" class="headerlink" title="Conanical Transformation"></a>Conanical Transformation</h3><p>This chapter is a bit like PCA transformation.</p><p>Let $\lambda_1, …, \lambda_k$ be the k real eigenvalues of matrix $\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)$. Suppose $\lambda_j$ are ordered with $\lambda_1$ the smallest, and that the k corresponding <a href><strong>linearly independent eigenvectors</strong></a>, $m_1, .., m_k$ from the $k$ columns of a matrix $M$. Then, we can construct a transformed process ${ y_t}$, where</p><p>$$y_t = \hat y_{t-1} (1) + b_t$$</p><p>with</p><p>$$y_t = z_t M, b_t = a_t M, \hat y_{t-1}(1)=\sum_{l=1}^{p} y_{t-l}\pi^1_l$$</p><p>where $\pi^1_l=M^{-1} \pi_l M$</p><p>We now also have</p><p>$$\Gamma_0(y) = \Gamma_0(\hat y) + \Sigma^1$$</p><p>where $\Gamma_0(y)=M \Gamma_0(z)M^T, \Gamma_0(\hat y)=M \Gamma_0(\hat z)M^T, \Sigma^1=M \Sigma M^T$</p><p>Note:</p><ul><li>$ M \Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z) M^{-1} = \Lambda, M \Sigma \Gamma_{0}^{-1}(z) M^{-1} = I - \Lambda$ where $\Lambda$ is a $k \times k$ matrix with elements $(\lambda_1, .., \lambda_k)$</li><li>$0 \leq \lambda_j &lt; 1 \space (j=1,..,k)$</li><li>for $i \neq j, m_i \Gamma_0(z) m_j^T = m_i \Sigma m_j^T = 0$. This makes $\Gamma_0(y), \Gamma_0(\hat y), \Sigma^1$ all diagonal (Otherwise, $\Lambda$ would not be diagonal).</li></ul><p><strong>(And this can be proved by constructing an equation of the previous $\lambda$ formula, which is inspired by my friend Nick.)</strong></p><p>With this <strong>diagonal</strong> propery, we can conclude that this transformation has produced $k$ components series ${ y_{1t}, y_{2t}, .., y_{kt}}$ which are</p><ul><li>ordered from least predictable to most predictable (meaning self-predictbility)</li><li>are contemporaneously independent</li><li>have predictable components ${\hat y_{1(t-1)}(1), y_{2(t-1)}(1), .., y_{k(t-1)}(1)}$ which are also contemporaneously independent</li><li>the same goes for ${ b_{1t}, b_{2t}, .., b_{kt}}$</li></ul><p><strong>Note</strong>: The content above goes for general time series, and the content below goes for AR(1) time series.(Also $M$ above can be computed in another way.)</p><h3 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h3><p>Say we have <a href="https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=yield" target="_blank" rel="noopener">Constant Maturity Treasury</a> rates (CMT rates) data ${z_t}$ from $02/01/2012$ to $06/30/2015$, a part of which is given below.</p><table><thead><tr><th>Date</th><th>6 Mo</th><th>1 Yr</th><th>2 Yr</th><th>3 Yr</th><th>5 Yr</th><th>…</th><th>30 Yr</th></tr></thead><tbody><tr><td>2015-06-30</td><td>0.11</td><td>0.28</td><td>0.64</td><td>1.01</td><td>1.63</td><td>…</td><td>3.11</td></tr><tr><td>2015-06-29</td><td>0.11</td><td>0.27</td><td>0.64</td><td>1.00</td><td>1.62</td><td>…</td><td>3.09</td></tr><tr><td>2015-06-26</td><td>0.08</td><td>0.29</td><td>0.72</td><td>1.09</td><td>1.75</td><td>…</td><td>3.25</td></tr><tr><td>2016-06-25</td><td>0.07</td><td>0.29</td><td>0.68</td><td>1.06</td><td>1.70</td><td>…</td><td>3.16</td></tr><tr><td>….</td><td>….</td><td>….</td><td>….</td><td>….</td><td>….</td><td>…</td><td>….</td></tr><tr><td>2012-02-01</td><td>0.09</td><td>0.13</td><td>0.23</td><td>0.31</td><td>0.72</td><td>…</td><td>3.01</td></tr></tbody></table><p>In this period of time, these CMT rates time series ${z_t}$ are both momentum time series. When we try to fit $AR(1)$ with these sereis with different maturities separately, the $AR(1)$ decaying parameters are around $0.95$ - $0.99$ (2.5 years is not a short term and both of these rates are contemporaneously under the same influence. So in the long term, they are both presenting similar trends).</p><p>But after we do conanical transformation to construct new time series (just as we discussed above) ${y_t}$, the most mean-reverting series has a $0.51$ decaying parameter in $AR(1)$ fitting.</p><p><img src="CCA/CCA.png" alt="CCA"></p><p>Note the constructed seires ${y_t}$ are not corresponding to the orignal cmt rates series ${z_t}$. This is similar to what is given by PCA - the first principle component is not corresponding to the first column of the orignal panel data.</p><p>Application</p><ol><li>Spot small mean-reverting portfolios.</li><li>CCA decomposition to generate detrended data.</li></ol><p>(<strong>Snippets Needed</strong>)</p></div></div><nav class="post-pagination"><a class="newer-posts" href="/2020/04/23/kalman-filter/">Previous post<br>A Mysterious Algorithm used in Apollo 11 Guidance </a><span class="page-number"></span> <a class="older-posts" href="/2019/08/19/post-1/">Next post<br>post</a></nav></div></div><div class="single-column-footer">Proudly published with Hexo<br>Theme <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> by <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a><br>&copy; 2020 <a href="http://yoursite.com">Avalon</a></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.4/dist/umd/popper.min.js" integrity="sha256-EGs9T1xMHdvM1geM8jPpoo8EZ1V1VRsmcJz8OByENLA=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.5.17/dist/vue.min.js" integrity="sha256-FtWfRI+thWlNz2sB3SJbwKx5PgMyKIVgwHCTwa3biXc=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/smooth-scroll@14.2.1/dist/smooth-scroll.polyfills.min.js" integrity="sha256-CI4Gq5E0io1Pv0xM3qPM+NUIOhbIBvC3GiN1Y4KhXpw=" crossorigin="anonymous"></script><script src="/js/journal.js?83544237"></script></body></html>