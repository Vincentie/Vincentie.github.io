<!DOCTYPE html><html xmlns:v-bind="http://www.w3.org/1999/xhtml"><head><meta name="generator" content="Hexo 3.9.0"><title>Canonical Component Analysis - Avalon</title><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="John Reese"><meta name="description" content="Construct a Mean-reverting Time Series"><meta name="keywords" content><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0" name="viewport"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><meta name="theme-color" content="#ffffff"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous"><link rel="stylesheet" href="/css/journal.css?98155174"><script src="/js/loadCSS.js"></script><script>loadCSS("https://fonts.googleapis.com/css?family=Lora|Montserrat|Fira+Mono|Material+Icons"),function(e){var t,a={kitId:"dwg1tuc",scriptTimeout:3e3,async:!0},o=e.documentElement,s=setTimeout(function(){o.className=o.className.replace(/\bwf-loading\b/g,"")+" wf-inactive"},a.scriptTimeout),c=e.createElement("script"),i=!1,n=e.getElementsByTagName("script")[0];o.className+=" wf-loading",c.src="https://use.typekit.net/"+a.kitId+".js",c.async=!0,c.onload=c.onreadystatechange=function(){if(t=this.readyState,!(i||t&&"complete"!=t&&"loaded"!=t)){i=!0,clearTimeout(s);try{Typekit.load(a)}catch(e){}}},n.parentNode.insertBefore(c,n)}(document)</script><noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lora|Montserrat|Anonymous+Pro:400|Material+Icons"></noscript><link rel="stylesheet" href="/css/prism.css" type="text/css"></head><body><div id="top"></div><div id="app"><div class="single-column-drawer-container" ref="drawer" v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }"><div class="drawer-content"><div class="drawer-menu"><a class="a-block drawer-menu-item false" href="http://yoursite.com">ホーム </a><a class="a-block drawer-menu-item false" href="/archives">記事一覽 </a><a class="a-block drawer-menu-item false" href="/about/index.html">關於我 </a><a class="a-block drawer-menu-item false" href="/categories/index.html">分類 </a><a class="a-block drawer-menu-item false" href="/tags/index.html">標簽</a></div></div></div><transition name="fade"><div v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if="isDrawerOpen" v-on:click="toggleDrawer"></div></transition><nav ref="navBar" class="navbar sticky-top navbar-light single-column-nav-container"><div ref="navBackground" class="nav-background"></div><div class="container container-narrow nav-content"><button id="nav_dropdown_btn" class="nav-dropdown-toggle" type="button" v-on:click="toggleDrawer"><i class="material-icons">menu</i></button> <a ref="navTitle" class="navbar-brand" href="/">Avalon</a></div></nav><div class="single-column-header-container" ref="pageHead" v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }"><a href="/"><div class="single-column-header-title">Avalon</div><div class="single-column-header-subtitle">Welcome!!</div></a></div><div ref="sideContainer" class="side-container"><a class="a-block nav-head false" href="/"><div class="nav-title">远い理想郷</div><div class="nav-subtitle">お帰りなさい</div></a><div class="nav-link-list"><a class="a-block no-tint nav-link-item false" href="/archives">記事一覽 </a><a class="a-block nav-link-item false" href="/about/index.html">關於我 </a><a class="a-block nav-link-item false" href="/categories/index.html">分類 </a><a class="a-block nav-link-item false" href="/tags/index.html">標簽</a></div><div class="nav-footer">Proudly published with Hexo<br>Theme <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> by <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a><br>&copy; 2020 <a href="http://yoursite.com">Avalon</a></div></div><div ref="extraContainer" class="extra-container"><div class="pagination"><a id="globalBackToTop" class="pagination-action animated-visibility" href="#top" :class="{ invisible: scrollY == 0 }"><i class="material-icons pagination-action-icon">keyboard_arrow_up</i></a></div></div><div ref="streamContainer" class="stream-container"><div class="post-list-container post-list-container-shadow"><div class="post"><div class="post-head-wrapper" style="background-image:url(/2019/10/20/CCA/White-Noise.jpg)"><div class="post-title">Canonical Component Analysis<div class="post-meta"><time datetime="2019-10-20T22:22:44.000Z" itemprop="datePublished">2019-10-20 18:22 </time>&nbsp; <i class="material-icons">folder</i> <a href="/categories/Math/">Math</a> <i class="material-icons">label</i> <a href="/tags/Math/">Math</a>, <a href="/tags/Linear-Algebra/">Linear Algebra</a>, <a href="/tags/Data-Engineering/">Data Engineering</a><script type="text/x-mathjax-config">MathJax.Hub.Config({
          tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            processEscapes: true
          }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            tex2jax: {
              skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
            }
          });</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
              var all = MathJax.Hub.getAllJax(), i;
              for(i=0; i < all.length; i += 1) {
                  all[i].SourceElement().parentNode.className += ' has-jax';
              }
          });</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></div></div></div><div class="post-body-wrapper"><div class="post-body"><h2 id="a-quick-question">0. A Quick Question</h2><p>Let's first look at a question:</p><blockquote><p>Here is a portfolio risk report for 2 underlying yield sensitivity (like DV01)<br>- 10-year treasury yield sensitivity: - 100k / bp<br>- 30-year treasury yield sensitivity: + 100k / bp</p><p>How do we hedge this portfolio against these 2 yield risks?</p></blockquote><p>The most common thing one can think of is selling 30-year treasury bonds and buying 30-year treasury bonds with a hedge ratio derived from historical data regression to hedge the above risk exposure. This kind of regression or static hedging will easily break because of yield curve reshaping. In this case, CCA (and PCA) are to be introduced in risk exposures hedging to cope with this yield curve reshaping issue.</p><h2 id="an-introduction-to-cca">1. An Introduction to CCA</h2><p>Say there is a pair of CMT rates - (<span class="math inline">\(Y_t^1\)</span>, <span class="math inline">\(Y_t^2\)</span>). What CCA is trying to do is to find a cointegration vector <span class="math inline">\([1, -\gamma]^T\)</span> to let the following equation stand</p><p><span class="math display">\[ Y_t^1 - \gamma \cdot Y_t^2 = \mu + \epsilon_t \]</span> where <span class="math inline">\(\epsilon_t\)</span> is a stationary white noise process, and therefore also is a <strong>mean-reverting</strong> process.</p><p><strong>This seems like doing OLS regression. The assumptions about the error terms are quite the same. However, the difference lies in their optimization objectives. CCA tries to find a vector to yield the most mean-reverting time series using self-predictability measure while OLS seeks to find a vector to minimize the prediction error. We will talk more about the difference later.</strong></p><p>What if we extend a pair to a portfolio consisting of more than n CMT rates <span class="math inline">\(Y_t = [Y_t^1, Y_t^2, ..., Y_t^n]^T\)</span>? The problem is also focusing on find a cointegration vector <span class="math inline">\(\gamma = [\gamma_1, \gamma_2, ..., \gamma_n]^T\)</span> to let the following equation stand</p><p><span class="math display">\[ \gamma^T Y_t = \mu + \epsilon_t \]</span> Theoretically, we can find a cointegration vector to build a mean-reverting time series among as many assets as we want. But in practice, it's hard to execute them all with proper prices at one time, and the execution costs are also very high.</p><p>So how do we find the cointegration vector <span class="math inline">\(\gamma\)</span>? Actually, <span class="math inline">\(\gamma\)</span> is such a vector yielded by CCA that makes <span class="math inline">\(\gamma^T Y_t\)</span> a canonical variable, even it's the least predictable variable.</p><h2 id="derivation-of-cca">2. Derivation of CCA</h2><p>Let's look at how CCA is rigorously derived. There are 2 similar ways of performing CCA - one is introduced in <a href="http://pages.stern.nyu.edu/~dbackus/BCZ/HS/BoxTiao_canonical_Bio_77.pdf" target="_blank" rel="noopener">Box-Tiao(1977)</a> and another is introduced in <a href="https://www.elibrary.imf.org/doc/IMF001/01258-9781451950700/01258-9781451950700/Other_formats/Source_PDF/01258-9781451999181.pdf" target="_blank" rel="noopener">Chou-Ng(1994)</a>. Here we consider the former paper.</p><h3 id="self-predictability-measure">2.1 Self-predictability Measure</h3><p>Consider a $1 k $ vector process <span class="math inline">\(\{\mathbb{Z_t}\}\)</span> and let <span class="math inline">\(z_t = \mathbb{Z_t} - \mu\)</span>, where <span class="math inline">\(\mu\)</span> is a convenient <span class="math inline">\(1 \times k\)</span> vector of origin which is the mean if the process is stationary. Suppose <span class="math inline">\(z_t\)</span> follows the <em>p</em>th order multiple autoregressive model</p><p><span class="math display">\[z_t = \hat z_{t-1}(1) + a_t\]</span></p><p>where</p><p><span class="math display">\[\hat z_{t-1}(1) = E(z_t|z_{t-1}, z_{t-2},..) = \sum_{l=1}^{p}z_{t-l} \pi_l\]</span></p><p>is the expectation of <span class="math inline">\(z_t\)</span> conditional on past history up to time <span class="math inline">\(t-1\)</span>, the <span class="math inline">\(\pi_l\)</span> are <span class="math inline">\(k \times k\)</span> matrices, <span class="math inline">\(\{ a_t\}\)</span> is a sequence of independently and normally distributed <span class="math inline">\(1 \times k\)</span> vector random shocks with mean zero and covariance matrix <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(a_t\)</span> is independent of <span class="math inline">\(\hat z_{t-1}(1)\)</span> - like the assumptions in OLS. And the <span class="math inline">\(AR(p)\)</span> model can be then represented as</p><p><span class="math display">\[z_t (I - \sum_{l=1}^{p}\pi_l B^l) = a_t\]</span></p><p>where <span class="math inline">\(I\)</span> is the identity matrix and <span class="math inline">\(B\)</span> is the backshift operator such that <span class="math inline">\(B z_t = z_{t-1}\)</span>.</p><p>The process <span class="math inline">\(\{z_t\}\)</span> is stationary if the determinantal polynomial in <span class="math inline">\(B\)</span>, <span class="math inline">\(det(I - \sum_{l=1}^{p}\pi_l B^l)\)</span> has its zeros lying outside the unit circle, and otherwise the process will be called non-stationary.</p><p>Now, let's make the problem simpler by setting <span class="math inline">\(k=1\)</span> to narrow down to only 1 time series. Then, if the process is stationary, due to <span class="math inline">\(a_t\)</span> being independent of <span class="math inline">\(\hat z_{t-1}(1)\)</span>.</p><p><span class="math display">\[E(z_t^2) = E(\{\hat z_{t-1}(1)\}^2) + E(a_t^2)\]</span></p><p>which can be also written as</p><p><span class="math display">\[ \sigma_z^2 = \sigma_{\hat z}^2 + \sigma_a^2\]</span></p><p>We can then define a quantity <span class="math inline">\(\lambda\)</span> to measure the predictability of a stationary series from its past as <span class="math inline">\(\lambda = \frac{\sigma_{\hat z}^2}{\sigma_z^2} = 1 - \frac{\sigma_a^2}{\sigma_z^2}\)</span>.</p><p><strong>Note</strong>: the derivation above only applies to 1 time series. And now <span class="math inline">\(z_t\)</span> is assumed to be stationary.</p><h3 id="intuition-of-cca-decomposition">2.2 Intuition of CCA Decomposition</h3><p>Now let's consider <span class="math inline">\(k\)</span> processes <span class="math inline">\(z_t\)</span> which represent <span class="math inline">\(k\)</span> different stock market indexes such as <em>Dow Jones Average</em>, <em>Standard and Poors</em> and <em>Russell Index</em>, etc., all of which exhibit dynamic growth.</p><p>It is natural to conjecture that <strong>each</strong> might be represented as some aggregate of one or more common inputs which may be nearly nonstationary (<strong>momentum</strong>), together with other stationary (<strong>mean-reverting</strong>) or white noise components.</p><p>In other words. This leads us to contemplate <strong>linear aggregates of the form <span class="math inline">\(u_t = z_t m\)</span></strong>, where <span class="math inline">\(m\)</span> is such a <span class="math inline">\(k \times 1\)</span> vector that <strong>make <span class="math inline">\(u_t\)</span> a momentum or mean-reverting time series.</strong></p><blockquote><p>Note: <span class="math inline">\(z_t\)</span> is a vector consisting of k time series. And different <span class="math inline">\(m\)</span> will yield different <span class="math inline">\(u_t\)</span>. These different time series <span class="math inline">\(u_t\)</span> (whether mean-reverting or momentum) are <strong>aggregates</strong>. And these aggregates are <strong>derived</strong> time series from <span class="math inline">\(z_t\)</span>. The process of getting <span class="math inline">\(u_t\)</span> from <span class="math inline">\(z_t\)</span> is called <strong>CCA decompostion</strong> even if these 2 processes are not in the same level (<span class="math inline">\(u_t\)</span> is a transformed or derived process.)</p></blockquote><p><strong>(Can <span class="math inline">\(z_t\)</span> be represented as a linear combination of <span class="math inline">\(u_t\)</span>?)</strong></p><p>The aggregates <span class="math inline">\(u_t\)</span> which depend most heavily on the past, namely having large <span class="math inline">\(\lambda\)</span> (refers to <span class="math inline">\(u_t\)</span>'s <span class="math inline">\(\lambda\)</span>), may serve as useful composite indicators of the overall growth of the stock market (<strong>momentum</strong>). By contrast, the aggregates with <span class="math inline">\(\lambda\)</span> nearly zero may reflect stable contemporaneous relationships (<strong>mean-reverting</strong>) among the original indicators.</p><p>The analysis given in this paper yields <span class="math inline">\(k\)</span> 'canonical' components <span class="math inline">\(u_t\)</span> from least to most predictable. Thus we may usefully decompose the k-dimensional space of the observation <span class="math inline">\(z_t\)</span> into stationary and non-stationary subspaces.</p><h3 id="derive-canonical-variables">2.3 Derive Canonical Variables</h3><p>Let <span class="math inline">\(\Gamma_j(z) = E(z_t^T z_{t-j})\)</span> be the lag <span class="math inline">\(j\)</span> autocovariance matrix of <span class="math inline">\(z_t\)</span>. In the variance form, we have</p><p><span class="math display">\[ \Gamma_0(z) = \sum_{l=1}^{p} \Gamma_l(z)\pi_l + \Sigma= \Gamma_0(\hat z) + \Sigma \]</span></p><p>say, where <span class="math inline">\(\Gamma_0(\hat z)\)</span> is the covariance matrix of <span class="math inline">\(\hat z_{t-1}(1)\)</span>. <strong>Until further notice, we shall assume that <span class="math inline">\(\Sigma\)</span> and therefore <span class="math inline">\(\Gamma_0(z)\)</span> are <a href>postive-defnite</a>.</strong></p><p>Now, consider the linear combination <span class="math inline">\(u_t = z_t m\)</span>. For <span class="math inline">\(u_t\)</span>, we have that <span class="math inline">\(u_t = \hat u_{t-1}(1) + v_t\)</span>, where <span class="math inline">\(\hat u_{t-1}(1) = \hat z_{t-1}(1) m\)</span> and <span class="math inline">\(v_t=a_t m\)</span>. The predictability of <span class="math inline">\(u_t\)</span> from its past is therefore measured by</p><p><span class="math display">\[ \lambda = \sigma_{\hat u}^2 \sigma_{u}^{-2} = \{ m \Gamma_{0}(\hat z) m^T \} \{m \Gamma_{0}(z) m^T \}^{-1}\]</span></p><p>which can be represented in matrix form as</p><p><span class="math display">\[ \Lambda = M \Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z) M^{-1}\]</span></p><blockquote><p>Note: <span class="math inline">\(M\)</span> is what we are looking for. The logic is we want to find a transformed process <span class="math inline">\(\{u_t\}\)</span> which is generated by <span class="math inline">\(M\)</span> and the original <span class="math inline">\(z_t\)</span>. And we derive that <span class="math inline">\(M\)</span> can be found using eigendecomposition of <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span>.</p></blockquote><p>This is what we call <a href>eigendecomposition</a>, and therefore we can conclude that for the maximum predictability, <span class="math inline">\(\lambda\)</span> must be the maximum eigenvalue of <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span> and <span class="math inline">\(m\)</span> the corresponding eigenvector that makes <span class="math inline">\(u_t\)</span> a momentum time series. Similarly, the eigenvector that corresponds to the smallest eigenvalue will yield the least predictable combination of <span class="math inline">\(z_t\)</span>. <strong>This vector is referred to as cointegration vector</strong> that is mainly used in the first question (risk hedging) mentioned at the very beginning.</p><h3 id="canonical-transformation">Canonical Transformation</h3><p>Let <span class="math inline">\(\lambda_1, ..., \lambda_k\)</span> be the k real eigenvalues of matrix <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span>. Suppose <span class="math inline">\(\lambda_j\)</span> are ordered with <span class="math inline">\(\lambda_1\)</span> the smallest, and that the k corresponding <a href><strong>linearly independent eigenvectors</strong></a>, <span class="math inline">\(m_1, .., m_k\)</span> from the <span class="math inline">\(k\)</span> columns of a matrix <span class="math inline">\(M\)</span>. Then, we can construct a transformed process <span class="math inline">\(\{ y_t\}\)</span>, where</p><p><span class="math display">\[ y_t = \hat y_{t-1} (1) + b_t \]</span></p><p>with</p><p><span class="math display">\[ y_t = z_t M, b_t = a_t M, \hat y_{t-1}(1)=\sum_{l=1}^{p} y_{t-l}\pi^1_l \]</span> where <span class="math inline">\(\pi^1_l=M^{-1} \pi_l M\)</span></p><p>We now also have</p><p><span class="math display">\[ \Gamma_0(y) = \Gamma_0(\hat y) + \Sigma^1 \]</span></p><p>where <span class="math inline">\(\Gamma_0(y)=M \Gamma_0(z)M^T, \Gamma_0(\hat y)=M \Gamma_0(\hat z)M^T, \Sigma^1=M \Sigma M^T\)</span></p><p>Note: - $ M <em>{0}(z) </em>{0}^{-1}(z) M^{-1} = , M _{0}^{-1}(z) M^{-1} = I - $ where <span class="math inline">\(\Lambda\)</span> is a <span class="math inline">\(k \times k\)</span> matrix with elements <span class="math inline">\((\lambda_1, .., \lambda_k)\)</span> - <span class="math inline">\(0 \leq \lambda_j &lt; 1 \space (j=1,..,k)\)</span> - for <span class="math inline">\(i \neq j, m_i \Gamma_0(z) m_j^T = m_i \Sigma m_j^T = 0\)</span>. This makes <span class="math inline">\(\Gamma_0(y), \Gamma_0(\hat y), \Sigma^1\)</span> all diagonal (Otherwise, <span class="math inline">\(\Lambda\)</span> would not be diagonal). <strong>(And this can be proved using <span class="math inline">\(m_i \Gamma_{0}(\hat z) m_j^T = \lambda_{ij} m_i \Gamma_{0}(z) m_j^T\)</span>, where <span class="math inline">\(\lambda_{ij} = 0\)</span> when <span class="math inline">\(i \neq j\)</span>)</strong> - eigenvectors <span class="math inline">\(M\)</span> do not form an orthonormal basis because of the asymmetry of <span class="math inline">\(\Gamma_{0}(\hat z) \Gamma_{0}^{-1}(z)\)</span></p><p>With this <strong>diagonal</strong> property, we can conclude that this transformation has produced <span class="math inline">\(k\)</span> new components series <span class="math inline">\(\{ y_{1t}, y_{2t}, .., y_{kt}\}\)</span> which are</p><ul><li>ordered from least predictable to most predictable (meaning self-predictability)</li><li>are contemporaneously independent</li><li>have predictable components <span class="math inline">\(\{\hat y_{1(t-1)}(1), \hat y_{2(t-1)}(1), .., \hat y_{k(t-1)}(1)\}\)</span> which are also contemporaneously independent</li><li>the same goes for <span class="math inline">\(\{ b_{1t}, b_{2t}, .., b_{kt}\}\)</span></li></ul><p><strong>Note</strong>: The content above goes for general time series, and the content below goes for <span class="math inline">\(AR(1)\)</span> time series.(Also <span class="math inline">\(M\)</span> above can be computed in another way.)</p><h3 id="example">Example</h3><p>Say we have <a href="https://www.treasury.gov/resource-center/data-chart-center/interest-rates/Pages/TextView.aspx?data=yield" target="_blank" rel="noopener">Constant Maturity Treasury</a> rates (CMT rates) data <span class="math inline">\(\{z_t\}\)</span> from <span class="math inline">\(02/01/2012\)</span> to <span class="math inline">\(06/30/2015\)</span>, a part of which is given below.</p><table><thead><tr class="header"><th>Date</th><th>6 Mo</th><th>1 Yr</th><th>2 Yr</th><th>3 Yr</th><th>5 Yr</th><th>...</th><th>30 Yr</th></tr></thead><tbody><tr class="odd"><td>2015-06-30</td><td>0.11</td><td>0.28</td><td>0.64</td><td>1.01</td><td>1.63</td><td>...</td><td>3.11</td></tr><tr class="even"><td>2015-06-29</td><td>0.11</td><td>0.27</td><td>0.64</td><td>1.00</td><td>1.62</td><td>...</td><td>3.09</td></tr><tr class="odd"><td>2015-06-26</td><td>0.08</td><td>0.29</td><td>0.72</td><td>1.09</td><td>1.75</td><td>...</td><td>3.25</td></tr><tr class="even"><td>2016-06-25</td><td>0.07</td><td>0.29</td><td>0.68</td><td>1.06</td><td>1.70</td><td>...</td><td>3.16</td></tr><tr class="odd"><td>....</td><td>....</td><td>....</td><td>....</td><td>....</td><td>....</td><td>...</td><td>....</td></tr><tr class="even"><td>2012-02-01</td><td>0.09</td><td>0.13</td><td>0.23</td><td>0.31</td><td>0.72</td><td>...</td><td>3.01</td></tr></tbody></table><p>In this period of time, these CMT rates time series <span class="math inline">\(\{z_t\}\)</span> are both momentum time series. When we try to fit <span class="math inline">\(AR(1)\)</span> with these series with different maturities separately, the <span class="math inline">\(AR(1)\)</span> decaying parameters are around <span class="math inline">\(0.95\)</span> - <span class="math inline">\(0.99\)</span> (<span class="math inline">\(2.5\)</span> years is not a short term and both of these rates are contemporaneously under the same influence. So in the long term, they are both presenting similar trends).</p><p>But after we do canonical transformation to construct new time series (just as we discussed above) <span class="math inline">\(\{y_t\}\)</span>, the most mean-reverting series has a <span class="math inline">\(0.51\)</span> decaying parameter in <span class="math inline">\(AR(1)\)</span> fitting.</p><figure><img src="CCA.png" alt="CCA"><figcaption>CCA</figcaption></figure><p>Note the constructed series <span class="math inline">\(\{y_t\}\)</span> are not corresponding to the original CMT rates series <span class="math inline">\(\{z_t\}\)</span>. This is similar to what is given by PCA - the first principle component is not corresponding to the first column of the original panel data.</p><h3 id="application">Application</h3><ol type="1"><li>Spot small mean-reverting portfolios.</li><li>Do CCA reconstruction to generate detrended data.</li></ol></div></div><nav class="post-pagination"><a class="newer-posts" href="/2020/04/23/kalman-filter/">前の記事<br>A Mysterious Algorithm used in Apollo 11 Guidance </a><span class="page-number"></span> <a class="older-posts" href="/2019/08/19/FIQT/">次の記事<br>Fixed Income Quant Trading</a></nav></div></div><div class="single-column-footer">Proudly published with Hexo<br>Theme <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> by <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a><br>&copy; 2020 <a href="http://yoursite.com">Avalon</a></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.4/dist/umd/popper.min.js" integrity="sha256-EGs9T1xMHdvM1geM8jPpoo8EZ1V1VRsmcJz8OByENLA=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.5.17/dist/vue.min.js" integrity="sha256-FtWfRI+thWlNz2sB3SJbwKx5PgMyKIVgwHCTwa3biXc=" crossorigin="anonymous"></script><script src="https://cdn.jsdelivr.net/npm/smooth-scroll@14.2.1/dist/smooth-scroll.polyfills.min.js" integrity="sha256-CI4Gq5E0io1Pv0xM3qPM+NUIOhbIBvC3GiN1Y4KhXpw=" crossorigin="anonymous"></script><script src="/js/journal.js?47494969"></script></body></html>